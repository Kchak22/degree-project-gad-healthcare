{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "8c6b1c47-5421-4a56-bb1e-81499851f135",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<torch._C.Generator at 0x7f8f17ffd390>"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import random\n",
    "import pickle\n",
    "\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.data import HeteroData\n",
    "from torch_geometric.nn import HeteroConv, Linear, SAGEConv\n",
    "import numpy as np\n",
    "import random\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "\n",
    "from torch_geometric.data import HeteroData\n",
    "\n",
    "np.random.seed(42)\n",
    "random.seed(42)\n",
    "torch.manual_seed(42)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db441fbf-07d5-456c-ad4d-bc4fda00b9a8",
   "metadata": {},
   "source": [
    "### 1. Get data "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "720dfad7-f70d-4e4a-bc82-c50590dd2b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load provider features : \n",
    "with open(\"final_df.pickle\", \"rb\") as pickle_file : \n",
    "    df_provider_features = pickle.load(pickle_file)\n",
    "\n",
    "providers_dataset = df_provider_features.index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "780e57dd-9ac6-47cc-a563-90ada60ff340",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load member features : \n",
    "with open(\"final_members_df.pickle\", \"rb\") as pickle_file : \n",
    "    df_member_features = pickle.load(pickle_file)\n",
    "\n",
    "members_dataset = df_member_features.index.to_list()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "87cde1e8-c22c-4959-99ff-947d6be09c84",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load claims data :\n",
    "with open(\"df_descriptions.pickle\", 'rb') as pickle_file : \n",
    "    df = pickle.load(pickle_file)\n",
    "\n",
    "df_edges = df[[\"providercode\", \"membercode\", \"claimcode\" ]]\n",
    "df_edges = df_edges.groupby([\"providercode\", \"membercode\"]).agg({\"claimcode\" : \"nunique\"}).reset_index()\n",
    "\n",
    "df_edges = df_edges.loc[((df_edges.membercode.isin(members_dataset))\n",
    "                         & (df_edges.providercode.isin(providers_dataset)))]\n",
    "df_edges.rename(columns={\"membercode\":\"member_id\", \n",
    "                \"providercode\" :\"provider_id\",\n",
    "                \"claimcode\" :\"nbr_claims\", \n",
    "                }, inplace=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6124e35-69f9-494a-845f-81c431509ad7",
   "metadata": {},
   "source": [
    "### 2. Build Graph "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "8f34ab64-5007-40cb-be28-a15a6284f0af",
   "metadata": {},
   "outputs": [],
   "source": [
    "providers = df_provider_features.index.tolist() \n",
    "members   = df_member_features.index.tolist() \n",
    "\n",
    "provider2idx = {p: i for i, p in enumerate(providers)}  \n",
    "member2idx   = {m: i for i, m in enumerate(members)}   \n",
    "\n",
    "data = HeteroData()\n",
    "\n",
    "# 1) Provider node features\n",
    "provider_feats = torch.tensor(df_provider_features.values, dtype=torch.float)\n",
    "data[\"provider\"].x = provider_feats  # shape: [num_providers, provider_feat_dim]\n",
    "\n",
    "# 2) Member node features\n",
    "member_feats = torch.tensor(df_member_features.values, dtype=torch.float)\n",
    "data[\"member\"].x = member_feats      # shape: [num_members, member_feat_dim]\n",
    "\n",
    "edge_list = []\n",
    "edge_weight_list = []\n",
    "\n",
    "for row in df_edges.itertuples(index=False):\n",
    "    # row.provider_id, row.member_id, row.nbr_claims\n",
    "    if row.provider_id in provider2idx and row.member_id in member2idx:\n",
    "        p_idx = provider2idx[row.provider_id]\n",
    "        m_idx = member2idx[row.member_id]\n",
    "        edge_list.append([p_idx, m_idx])\n",
    "        edge_list.append([m_idx, p_idx])\n",
    "        edge_weight_list.append(float(row.nbr_claims))\n",
    "        edge_weight_list.append(float(row.nbr_claims))\n",
    "\n",
    "# Convert to torch\n",
    "edge_index = torch.tensor(edge_list, dtype=torch.long).t().contiguous()  # shape [2, E]\n",
    "edge_attr  = torch.tensor(edge_weight_list, dtype=torch.float)\n",
    "\n",
    "# Assign to data\n",
    "data[\"provider\", \"links\", \"member\"].edge_index = edge_index\n",
    "data[\"provider\", \"links\", \"member\"].edge_attr  = edge_attr\n",
    "\n",
    "\n",
    "# Initialize anomaly labels for each node type (0=normal, 1=anomalous)\n",
    "data[\"provider\"].synthetic_labels = torch.zeros(data[\"provider\"].num_nodes, dtype=torch.long)\n",
    "data[\"member\"].synthetic_labels   = torch.zeros(data[\"member\"].num_nodes, dtype=torch.long)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "6e508f92-d986-4c98-9826-e3d3edfff82f",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([652, 38])"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"provider\"].x.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "33e0545d-de30-4802-aa59-909f795e1ead",
   "metadata": {},
   "source": [
    "### 3. Synthetic labels"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32dfb5eb-723f-44c7-80e9-033b3eae3e59",
   "metadata": {},
   "source": [
    "#### 3.1 Knowledge-based anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "c2757f17-74de-4a4b-910e-4f550518ba22",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_ghost_members(\n",
    "    data, \n",
    "    ratio=0.01, \n",
    "    connections_per_ghost=3\n",
    "):\n",
    "    \"\"\"\n",
    "    Create 'ghost' member nodes that do not exist originally,\n",
    "    connect them to random providers, mark them (and those providers) as anomalous.\n",
    "    \n",
    "    :param data: The HeteroData object\n",
    "    :param ratio: fraction of the *current* total members to add as new ghost members\n",
    "    :param connections_per_ghost: how many providers each ghost node connects to\n",
    "    \"\"\"\n",
    "    device = data[\"member\"].x.device\n",
    "    \n",
    "    old_num_members = data[\"member\"].num_nodes\n",
    "    num_ghost = max(1, int(ratio * old_num_members))  # at least 1 ghost\n",
    "    \n",
    "    feat_dim = data[\"member\"].x.size(1)\n",
    "    \n",
    "    # 1) Create random features for ghost members\n",
    "    ghost_feats = torch.randn(num_ghost, feat_dim, device=device) * 0.01\n",
    "    new_member_x = torch.cat([data[\"member\"].x, ghost_feats], dim=0)\n",
    "    data[\"member\"].x = new_member_x\n",
    "    \n",
    "    # Expand the synthetic_labels\n",
    "    old_labels = data[\"member\"].synthetic_labels\n",
    "    new_labels = torch.ones(num_ghost, dtype=torch.long, device=device)  # All ghost = anomaly\n",
    "    data[\"member\"].synthetic_labels = torch.cat([old_labels, new_labels], dim=0)\n",
    "    \n",
    "    # 2) Connect these ghost members to random providers\n",
    "    edge_list = data[\"provider\", \"links\", \"member\"].edge_index.clone().t().tolist()\n",
    "    edge_attr_list = data[\"provider\", \"links\", \"member\"].edge_attr.tolist()\n",
    "    \n",
    "    num_providers = data[\"provider\"].num_nodes\n",
    "    \n",
    "    for g in range(num_ghost):\n",
    "        ghost_member_idx = old_num_members + g\n",
    "        for _ in range(connections_per_ghost):\n",
    "            p_idx = random.randint(0, num_providers - 1)\n",
    "            edge_list.append([p_idx, ghost_member_idx])\n",
    "            # Use a random or default weight (nbr_claims)\n",
    "            w = float(random.randint(1, 5))\n",
    "            edge_attr_list.append(w)\n",
    "            \n",
    "            # Mark provider as anomaly\n",
    "            data[\"provider\"].synthetic_labels[p_idx] = 1\n",
    "\n",
    "    # Convert back to tensor\n",
    "    new_edge_index = torch.tensor(edge_list, dtype=torch.long, device=device).t().contiguous()\n",
    "    new_edge_attr  = torch.tensor(edge_attr_list, dtype=torch.float, device=device)\n",
    "\n",
    "    data[\"provider\", \"links\", \"member\"].edge_index = new_edge_index\n",
    "    data[\"provider\", \"links\", \"member\"].edge_attr  = new_edge_attr\n",
    "    \n",
    "    return data\n",
    "\n",
    "def inject_upcoding(\n",
    "    data,\n",
    "    ratio=0.01,\n",
    "    scale_factor=5.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Randomly select a fraction of edges, inflate their edge_attr (nbr_claims) by scale_factor,\n",
    "    mark involved nodes as anomaly.\n",
    "    \n",
    "    :param data: The HeteroData object\n",
    "    :param ratio: fraction of edges to inflate\n",
    "    :param scale_factor: how much to multiply the existing edge_attr\n",
    "    \"\"\"\n",
    "    edge_index = data[\"provider\", \"links\", \"member\"].edge_index\n",
    "    edge_attr  = data[\"provider\", \"links\", \"member\"].edge_attr\n",
    "    \n",
    "    E = edge_index.size(1)\n",
    "    num_inflate = max(1, int(ratio * E))\n",
    "    \n",
    "    chosen_edges = random.sample(range(E), k=num_inflate)\n",
    "    \n",
    "    for e_idx in chosen_edges:\n",
    "        edge_attr[e_idx] *= scale_factor\n",
    "        \n",
    "        # Mark provider & member as anomalies\n",
    "        p_idx = edge_index[0, e_idx]\n",
    "        m_idx = edge_index[1, e_idx]\n",
    "        \n",
    "        data[\"provider\"].synthetic_labels[p_idx] = 1\n",
    "        data[\"member\"].synthetic_labels[m_idx]   = 1\n",
    "    \n",
    "    return data\n",
    "\n",
    "def inject_collusion_ring(\n",
    "    data,\n",
    "    ratio=0.01,\n",
    "    partial_density=1.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Create a small 'collusion ring': \n",
    "    - Select a small subset of providers + members\n",
    "    - Connect them with a fully dense or partially dense bipartite block\n",
    "    - Mark them as anomalies\n",
    "    \n",
    "    :param data: HeteroData object\n",
    "    :param ratio: fraction of the existing node sets to pick for collusion ring\n",
    "    :param partial_density: fraction of the fully dense edges to actually add (0.2 to 1.0)\n",
    "    \"\"\"\n",
    "    device = data[\"member\"].x.device\n",
    "    \n",
    "    num_providers = data[\"provider\"].num_nodes\n",
    "    num_members   = data[\"member\"].num_nodes\n",
    "    \n",
    "    # Small subsets\n",
    "    sub_p = max(1, int(ratio * num_providers))\n",
    "    sub_m = max(1, int(ratio * num_members))\n",
    "    \n",
    "    # Randomly pick providers\n",
    "    provider_indices = random.sample(range(num_providers), k=sub_p)\n",
    "    # Randomly pick members\n",
    "    member_indices   = random.sample(range(num_members), k=sub_m)\n",
    "    \n",
    "    edge_list = data[\"provider\", \"links\", \"member\"].edge_index.clone().t().tolist()\n",
    "    edge_attr_list = data[\"provider\", \"links\", \"member\"].edge_attr.tolist()\n",
    "    \n",
    "    # Fully dense block means all provider_indices x member_indices\n",
    "    # partial_density means we only keep some fraction\n",
    "    possible_edges = []\n",
    "    for p in provider_indices:\n",
    "        for m in member_indices:\n",
    "            possible_edges.append((p, m))\n",
    "    \n",
    "    num_block_edges = int(partial_density * len(possible_edges))\n",
    "    block_edges = random.sample(possible_edges, k=num_block_edges)\n",
    "    \n",
    "    for (p_idx, m_idx) in block_edges:\n",
    "        edge_list.append([p_idx, m_idx])\n",
    "        w = float(random.randint(1, 10))  # random claims\n",
    "        edge_attr_list.append(w)\n",
    "        \n",
    "        # Mark them as anomaly\n",
    "        data[\"provider\"].synthetic_labels[p_idx] = 1\n",
    "        data[\"member\"].synthetic_labels[m_idx]   = 1\n",
    "\n",
    "    new_edge_index = torch.tensor(edge_list, dtype=torch.long, device=device).t().contiguous()\n",
    "    new_edge_attr  = torch.tensor(edge_attr_list, dtype=torch.float, device=device)\n",
    "    \n",
    "    data[\"provider\", \"links\", \"member\"].edge_index = new_edge_index\n",
    "    data[\"provider\", \"links\", \"member\"].edge_attr  = new_edge_attr\n",
    "\n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1fa0cbe7-1a49-4f64-a800-44c7eec9eba6",
   "metadata": {},
   "source": [
    "#### 3.2 Random anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "d9c55e92-a31d-47f4-8d22-44291f116c5c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_random_struct_anomaly(\n",
    "    data, \n",
    "    ratio=0.01,\n",
    "    min_block_size=2,\n",
    "    max_block_size=20,\n",
    "    partial=True\n",
    "):\n",
    "    \"\"\"\n",
    "    Random structure anomaly injection (Ding et al. [8] style):\n",
    "    - pick small subset of providers + members (2..20)\n",
    "    - create fully or partially dense bipartite edges\n",
    "    - mark them as anomalies\n",
    "    :param partial: if True, connect only some fraction of pairs (0.2..1.0)\n",
    "    \"\"\"\n",
    "    num_providers = data[\"provider\"].num_nodes\n",
    "    num_members   = data[\"member\"].num_nodes\n",
    "    device        = data[\"provider\"].x.device\n",
    "    \n",
    "    # Number of sub-anomalies to inject\n",
    "    # ratio is fraction of the total node sets => approximate how many anomalies we form\n",
    "    # We'll do 'num_anomalies' sub-block insertions\n",
    "    num_anomalies = max(1, int(ratio * (num_providers + num_members) // (max_block_size*2)))\n",
    "    \n",
    "    edge_list = data[\"provider\", \"links\", \"member\"].edge_index.clone().t().tolist()\n",
    "    edge_attr_list = data[\"provider\", \"links\", \"member\"].edge_attr.tolist()\n",
    "    \n",
    "    for _ in range(num_anomalies):\n",
    "        block_p_size = random.randint(min_block_size, max_block_size)\n",
    "        block_m_size = random.randint(min_block_size, max_block_size)\n",
    "        \n",
    "        # random sets\n",
    "        p_nodes = random.sample(range(num_providers), min(block_p_size, num_providers))\n",
    "        m_nodes = random.sample(range(num_members), min(block_m_size, num_members))\n",
    "        \n",
    "        fraction = random.uniform(0.2, 1.0) if partial else 1.0\n",
    "        \n",
    "        possible_edges = []\n",
    "        for p in p_nodes:\n",
    "            for m in m_nodes:\n",
    "                possible_edges.append((p, m))\n",
    "        \n",
    "        # partial or full density\n",
    "        chosen_edges_count = int(fraction * len(possible_edges))\n",
    "        chosen_edges = random.sample(possible_edges, k=chosen_edges_count)\n",
    "        \n",
    "        # Insert edges\n",
    "        for (p_idx, m_idx) in chosen_edges:\n",
    "            edge_list.append([p_idx, m_idx])\n",
    "            w = float(random.randint(1, 10))\n",
    "            edge_attr_list.append(w)\n",
    "            # Mark anomalies\n",
    "            data[\"provider\"].synthetic_labels[p_idx] = 1\n",
    "            data[\"member\"].synthetic_labels[m_idx]   = 1\n",
    "    \n",
    "    new_edge_index = torch.tensor(edge_list, dtype=torch.long, device=device).t().contiguous()\n",
    "    new_edge_attr  = torch.tensor(edge_attr_list, dtype=torch.float, device=device)\n",
    "    data[\"provider\", \"links\", \"member\"].edge_index = new_edge_index\n",
    "    data[\"provider\", \"links\", \"member\"].edge_attr  = new_edge_attr\n",
    "\n",
    "    return data\n",
    "\n",
    "\n",
    "def inject_random_attr_anomaly(\n",
    "    data,\n",
    "    node_type=\"provider\",\n",
    "    ratio=0.01,\n",
    "    method=\"outside_conf\",\n",
    "    c=3.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Inject attribute anomaly in node features following Ding et al. [8].\n",
    "    :param node_type: \"provider\" or \"member\"\n",
    "    :param ratio: fraction of nodes to become anomalies\n",
    "    :param method: \"outside_conf\" or \"scaled_gaussian\"\n",
    "    :param c: confidence/scale factor (2..4 recommended)\n",
    "    \"\"\"\n",
    "    x = data[node_type].x\n",
    "    n_nodes, feat_dim = x.shape\n",
    "    device = x.device\n",
    "    \n",
    "    # mean, std\n",
    "    mu = x.mean(dim=0)\n",
    "    sigma = x.std(dim=0) + 1e-6\n",
    "    \n",
    "    num_anomalies = max(1, int(ratio * n_nodes))\n",
    "    chosen_nodes = random.sample(range(n_nodes), k=num_anomalies)\n",
    "    \n",
    "    for nd in chosen_nodes:\n",
    "        data[node_type].synthetic_labels[nd] = 1\n",
    "        if method == \"outside_conf\":\n",
    "            # Replace a fraction of features randomly\n",
    "            # We'll pick half of the features to corrupt, for example\n",
    "            feat_indices = random.sample(range(feat_dim), k=feat_dim // 2)\n",
    "            for fi in feat_indices:\n",
    "                # Generate truncated gauss outside [mu - c*sigma, mu + c*sigma]\n",
    "                lower = mu[fi] - c * sigma[fi]\n",
    "                upper = mu[fi] + c * sigma[fi]\n",
    "                \n",
    "                # sample repeatedly until we are outside the interval\n",
    "                val = torch.randn(1, device=device) * sigma[fi] + mu[fi]\n",
    "                while lower <= val <= upper:\n",
    "                    val = torch.randn(1, device=device) * sigma[fi] + mu[fi]\n",
    "                \n",
    "                x[nd, fi] = val\n",
    "        else:  # \"scaled_gaussian\"\n",
    "            # add noise from N(0, c * sigma)\n",
    "            noise = torch.randn(feat_dim, device=device) * (c * sigma)\n",
    "            x[nd] = x[nd] + noise\n",
    "    \n",
    "    data[node_type].x = x\n",
    "    return data\n",
    "\n",
    "def inject_random_both_anomaly(\n",
    "    data,\n",
    "    ratio=0.01,\n",
    "    min_block_size=2,\n",
    "    max_block_size=20,\n",
    "    c=3.0\n",
    "):\n",
    "    \"\"\"\n",
    "    Combination: \n",
    "    1) Insert a random structural anomaly block\n",
    "    2) Then apply attribute anomaly to the newly involved nodes or edges\n",
    "    \"\"\"\n",
    "    # Step 1: structural anomaly\n",
    "    data = inject_random_struct_anomaly(\n",
    "        data,\n",
    "        ratio=ratio,\n",
    "        min_block_size=min_block_size,\n",
    "        max_block_size=max_block_size,\n",
    "        partial=True\n",
    "    )\n",
    "    \n",
    "    # Step 2: attribute anomaly on some providers or members\n",
    "    # (We can pick randomly whether to apply \"outside_conf\" or \"scaled_gaussian\")\n",
    "    method_choice = random.choice([\"outside_conf\", \"scaled_gaussian\"])\n",
    "    data = inject_random_attr_anomaly(\n",
    "        data,\n",
    "        node_type=\"provider\",\n",
    "        ratio=ratio,\n",
    "        method=method_choice,\n",
    "        c=c\n",
    "    )\n",
    "    data = inject_random_attr_anomaly(\n",
    "        data,\n",
    "        node_type=\"member\",\n",
    "        ratio=ratio,\n",
    "        method=method_choice,\n",
    "        c=c\n",
    "    )\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "6c2dddca-1b33-4fe6-9a02-79ff5a62aabc",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import random\n",
    "import itertools\n",
    "\n",
    "def inject_structural_anomalies(data, clique_size=15, num_cliques=10):\n",
    "    \"\"\"\n",
    "    Inject structural anomalies by adding fully connected cliques.\n",
    "    \n",
    "    The procedure is as follows:\n",
    "      1. Compute the total number of nodes to affect: total = clique_size * num_cliques.\n",
    "      2. Randomly sample total distinct nodes from the graph.\n",
    "      3. Partition these nodes into num_cliques groups of size clique_size.\n",
    "      4. For each group (clique), add edges between every pair of nodes (if not already present)\n",
    "         so that the group becomes a fully connected subgraph.\n",
    "      5. Mark all nodes in each injected clique as anomalous (synthetic_labels set to 1).\n",
    "    \n",
    "    This method injects m × n structural anomalies into the network.\n",
    "    \n",
    "    :param data: A PyTorch Geometric Data object with attributes x, edge_index, and synthetic_labels.\n",
    "    :param clique_size: The number of nodes in each clique (m).\n",
    "    :param num_cliques: The number of cliques to inject (n).\n",
    "    :return: The modified data object.\n",
    "    \"\"\"\n",
    "    total_anomalies = clique_size * num_cliques\n",
    "    num_nodes = data.x.size(0)\n",
    "    \n",
    "    if total_anomalies > num_nodes:\n",
    "        raise ValueError(\"Not enough nodes to inject the desired number of structural anomalies.\")\n",
    "    \n",
    "    # Randomly sample total_anomalies nodes without replacement.\n",
    "    all_nodes = list(range(num_nodes))\n",
    "    random.shuffle(all_nodes)\n",
    "    anomaly_nodes = all_nodes[:total_anomalies]\n",
    "    \n",
    "    # Partition the selected nodes into num_cliques groups of size clique_size.\n",
    "    cliques = [anomaly_nodes[i * clique_size : (i + 1) * clique_size] for i in range(num_cliques)]\n",
    "    \n",
    "    # Convert existing edge_index to a set of (src, dst) pairs for easy checking.\n",
    "    existing_edges = set(zip(data.edge_index[0].tolist(), data.edge_index[1].tolist()))\n",
    "    \n",
    "    new_edges = []\n",
    "    for clique in cliques:\n",
    "        # Mark each node in the clique as an anomaly.\n",
    "        for node in clique:\n",
    "            data.synthetic_labels[node] = 1\n",
    "        # For an undirected graph, add edges in both directions.\n",
    "        # Add an edge for each unordered pair (i, j) with i < j.\n",
    "        for i, j in itertools.combinations(clique, 2):\n",
    "            # (i, j) and (j, i) are both added.\n",
    "            new_edges.append((i, j))\n",
    "            new_edges.append((j, i))\n",
    "    \n",
    "    # Combine new edges with existing ones.\n",
    "    # (Removing duplicates is optional; here we form the union.)\n",
    "    combined_edges = set(existing_edges) | set(new_edges)\n",
    "    \n",
    "    # Convert back to a tensor of shape (2, num_edges).\n",
    "    edge_index_tensor = torch.tensor(list(combined_edges), dtype=torch.long).t().contiguous()\n",
    "    data.edge_index = edge_index_tensor\n",
    "    \n",
    "    return data\n",
    "\n",
    "def inject_attribute_anomalies(data, clique_size=15, num_cliques=10, k=10):\n",
    "    \"\"\"\n",
    "    Inject attribute anomalies by perturbing node features.\n",
    "    \n",
    "    The procedure is as follows:\n",
    "      1. Compute the total number of candidate nodes: total = clique_size * num_cliques.\n",
    "      2. Randomly sample total distinct nodes from the graph.\n",
    "      3. For each candidate node i:\n",
    "           a. Randomly select k other nodes from the graph (excluding i).\n",
    "           b. Compute the Euclidean distance between i's feature vector and each sampled node’s feature vector.\n",
    "           c. Find the node j with the maximum distance.\n",
    "           d. Replace node i’s features with node j’s features.\n",
    "           e. Mark node i as anomalous (synthetic_labels set to 1).\n",
    "    \n",
    "    This method injects an equal number (m × n) of attribute anomalies as are injected structurally.\n",
    "    \n",
    "    :param data: A PyTorch Geometric Data object with attributes x and synthetic_labels.\n",
    "    :param clique_size: Parameter m used to determine the total number of candidates.\n",
    "    :param num_cliques: Parameter n used to determine the total number of candidates.\n",
    "    :param k: The number of nodes to compare for each candidate node.\n",
    "    :return: The modified data object.\n",
    "    \"\"\"\n",
    "    total_candidates = clique_size * num_cliques\n",
    "    num_nodes = data.x.size(0)\n",
    "    \n",
    "    if total_candidates > num_nodes:\n",
    "        raise ValueError(\"Not enough nodes to inject the desired number of attribute anomalies.\")\n",
    "    \n",
    "    all_nodes = list(range(num_nodes))\n",
    "    random.shuffle(all_nodes)\n",
    "    candidate_nodes = all_nodes[:total_candidates]\n",
    "    \n",
    "    for i in candidate_nodes:\n",
    "        # Create a list of nodes to sample from (all except i).\n",
    "        possible_nodes = list(range(num_nodes))\n",
    "        possible_nodes.remove(i)\n",
    "        k_sel = min(k, len(possible_nodes))\n",
    "        sampled_nodes = random.sample(possible_nodes, k_sel)\n",
    "        \n",
    "        xi = data.x[i]\n",
    "        max_dist = -1\n",
    "        max_j = None\n",
    "        # Find the node among the k sampled nodes with the largest Euclidean distance from xi.\n",
    "        for j in sampled_nodes:\n",
    "            xj = data.x[j]\n",
    "            # Compute the Euclidean distance (using the 2-norm).\n",
    "            dist = torch.norm(xi - xj, p=2).item()\n",
    "            if dist > max_dist:\n",
    "                max_dist = dist\n",
    "                max_j = j\n",
    "        if max_j is not None:\n",
    "            # Replace node i's features with those of node max_j.\n",
    "            data.x[i] = data.x[max_j].clone()\n",
    "            # Mark node i as an attribute anomaly.\n",
    "            data.synthetic_labels[i] = 1\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "7e0e2b21-951b-420a-ba99-6d45d329699b",
   "metadata": {},
   "outputs": [
    {
     "ename": "AttributeError",
     "evalue": "'HeteroData' has no attribute 'x'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[16], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Inject 10 cliques of size 15 (i.e. 150 structural anomalies)\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m data \u001b[38;5;241m=\u001b[39m \u001b[43minject_structural_anomalies\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mclique_size\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m15\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mnum_cliques\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m10\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m      4\u001b[0m \u001b[38;5;66;03m# Inject 150 attribute anomalies using k=10 for comparison.\u001b[39;00m\n\u001b[1;32m      5\u001b[0m data \u001b[38;5;241m=\u001b[39m inject_attribute_anomalies(data, clique_size\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m15\u001b[39m, num_cliques\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m, k\u001b[38;5;241m=\u001b[39m\u001b[38;5;241m10\u001b[39m)\n",
      "Cell \u001b[0;32mIn[15], line 25\u001b[0m, in \u001b[0;36minject_structural_anomalies\u001b[0;34m(data, clique_size, num_cliques)\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;124;03mInject structural anomalies by adding fully connected cliques.\u001b[39;00m\n\u001b[1;32m      8\u001b[0m \u001b[38;5;124;03m\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     22\u001b[0m \u001b[38;5;124;03m:return: The modified data object.\u001b[39;00m\n\u001b[1;32m     23\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     24\u001b[0m total_anomalies \u001b[38;5;241m=\u001b[39m clique_size \u001b[38;5;241m*\u001b[39m num_cliques\n\u001b[0;32m---> 25\u001b[0m num_nodes \u001b[38;5;241m=\u001b[39m \u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mx\u001b[49m\u001b[38;5;241m.\u001b[39msize(\u001b[38;5;241m0\u001b[39m)\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m total_anomalies \u001b[38;5;241m>\u001b[39m num_nodes:\n\u001b[1;32m     28\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mValueError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNot enough nodes to inject the desired number of structural anomalies.\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "File \u001b[0;32m~/research_subject/karim_research/lib/python3.11/site-packages/torch_geometric/data/hetero_data.py:162\u001b[0m, in \u001b[0;36mHeteroData.__getattr__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m    160\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28mbool\u001b[39m(re\u001b[38;5;241m.\u001b[39msearch(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_dict$\u001b[39m\u001b[38;5;124m'\u001b[39m, key)):\n\u001b[1;32m    161\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcollect(key[:\u001b[38;5;241m-\u001b[39m\u001b[38;5;241m5\u001b[39m])\n\u001b[0;32m--> 162\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mAttributeError\u001b[39;00m(\u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__class__\u001b[39m\u001b[38;5;241m.\u001b[39m\u001b[38;5;18m__name__\u001b[39m\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m has no \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m    163\u001b[0m                      \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mattribute \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mkey\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'HeteroData' has no attribute 'x'"
     ]
    }
   ],
   "source": [
    "# Inject 10 cliques of size 15 (i.e. 150 structural anomalies)\n",
    "data = inject_structural_anomalies(data, clique_size=15, num_cliques=10)\n",
    "\n",
    "# Inject 150 attribute anomalies using k=10 for comparison.\n",
    "data = inject_attribute_anomalies(data, clique_size=15, num_cliques=10, k=10)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bf525909-53ec-4fb7-afcc-2a058c4167f5",
   "metadata": {},
   "source": [
    "#### 3.3 Injecting labels"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "ffe5fda6-f22a-4ca7-ac87-c231467a05a7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def inject_diverse_anomalies(data):\n",
    "    \"\"\"\n",
    "    Example pipeline injecting a mix of anomalies with small overall ratios.\n",
    "    Each pass uses a different injection function and parameters.\n",
    "    \"\"\"\n",
    "    # 1) Realistic Healthcare Fraud\n",
    "    \n",
    "    # (a) Ghost members: ~1% ratio, each connects to 2 providers\n",
    "    data = inject_ghost_members(data, ratio=0.01, connections_per_ghost=2)\n",
    "    \n",
    "    # (b) Upcoding: ~2% edges, scale factor between 2x and 5x\n",
    "    scale_f = random.uniform(2.0, 5.0)\n",
    "    data = inject_upcoding(data, ratio=0.02, scale_factor=scale_f)\n",
    "    \n",
    "    # (c) Collusion ring: ~1% ratio, partial density ~0.5\n",
    "    partial_dens = random.uniform(0.3, 0.7)\n",
    "    data = inject_collusion_ring(data, ratio=0.01, partial_density=partial_dens)\n",
    "    \n",
    "    # 2) Random Anomalies (Ding et al.)\n",
    "    \n",
    "    # (a) Random structural anomaly: ~1% ratio, block sizes up to 10\n",
    "    data = inject_random_struct_anomaly(\n",
    "        data,\n",
    "        ratio=0.01,\n",
    "        min_block_size=2, \n",
    "        max_block_size=10,\n",
    "        partial=True\n",
    "    )\n",
    "    \n",
    "    # (b) Random attribute anomaly on providers, 1% ratio, either outside_conf or scaled_gaussian\n",
    "    method_choice = random.choice([\"outside_conf\", \"scaled_gaussian\"])\n",
    "    c_val = random.uniform(2, 4)\n",
    "    data = inject_random_attr_anomaly(\n",
    "        data,\n",
    "        node_type=\"provider\",\n",
    "        ratio=0.01,\n",
    "        method=method_choice,\n",
    "        c=c_val\n",
    "    )\n",
    "    \n",
    "    # (c) Random attribute anomaly on members, 1% ratio\n",
    "    method_choice2 = random.choice([\"outside_conf\", \"scaled_gaussian\"])\n",
    "    c_val2 = random.uniform(2, 4)\n",
    "    data = inject_random_attr_anomaly(\n",
    "        data,\n",
    "        node_type=\"member\",\n",
    "        ratio=0.01,\n",
    "        method=method_choice2,\n",
    "        c=c_val2\n",
    "    )\n",
    "    \n",
    "    # (d) Combined structure + attribute anomaly, ~0.5% ratio\n",
    "    data = inject_random_both_anomaly(\n",
    "        data,\n",
    "        ratio=0.005,\n",
    "        min_block_size=2,\n",
    "        max_block_size=10,\n",
    "        c=random.uniform(2, 4)\n",
    "    )\n",
    "    \n",
    "    return data\n",
    "\n",
    "\n",
    "def inject_diverse_anomalies_second_version(data, max_anomaly_ratio=0.05):\n",
    "    num_providers = data[\"provider\"].num_nodes\n",
    "    num_members = data[\"member\"].num_nodes\n",
    "    total_nodes = num_providers + num_members\n",
    "    max_anomalies = int(max_anomaly_ratio * total_nodes)\n",
    "    \n",
    "    # Track anomalies\n",
    "    injected_anomalies = set()  # Store (node_type, idx)\n",
    "    \n",
    "    # 1) Ghost Members (max 10% of anomaly budget)\n",
    "    num_ghosts = max(1, int(0.1 * max_anomalies))\n",
    "    data, new_ghosts = inject_ghost_members(data, num_ghosts=num_ghosts)\n",
    "    injected_anomalies.update([(\"member\", idx) for idx in new_ghosts])\n",
    "    \n",
    "    # 2) Upcoding (max 20% of budget)\n",
    "    num_upcoding = max(1, int(0.2 * max_anomalies))\n",
    "    data, upcoded = inject_upcoding(data, num_edges=num_upcoding)\n",
    "    injected_anomalies.update([(\"provider\", p) for p in upcoded[\"providers\"]])\n",
    "    injected_anomalies.update([(\"member\", m) for m in upcoded[\"members\"]])\n",
    "    \n",
    "    # 3) Collusion Ring (use remaining budget)\n",
    "    remaining = max_anomalies - len(injected_anomalies)\n",
    "    if remaining > 0:\n",
    "        data = inject_collusion_ring(data, num_nodes=remaining)\n",
    "    \n",
    "    return data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da644f26-17fd-4be4-abfb-0ec518ef4d10",
   "metadata": {},
   "outputs": [],
   "source": [
    "data = inject_diverse_anomalies(data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "id": "20b6ca3a-1747-43af-96fc-2e2370b8e7ff",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "== Final Graph After Injection ==\n",
      "Number of provider nodes: 652\n",
      "Number of member nodes: 32885\n",
      "Provider anomaly count: 564\n",
      "Member anomaly count: 2385\n",
      "Edges shape: torch.Size([2, 60982])\n",
      "Edge attrs shape: torch.Size([60982])\n"
     ]
    }
   ],
   "source": [
    "print(\"== Final Graph After Injection ==\")\n",
    "print(\"Number of provider nodes:\", data[\"provider\"].num_nodes)\n",
    "print(\"Number of member nodes:\", data[\"member\"].num_nodes)\n",
    "print(\"Provider anomaly count:\", data[\"provider\"].synthetic_labels.sum().item())\n",
    "print(\"Member anomaly count:\", data[\"member\"].synthetic_labels.sum().item())\n",
    "print(\"Edges shape:\", data[\"provider\", \"links\", \"member\"].edge_index.shape)\n",
    "print(\"Edge attrs shape:\", data[\"provider\", \"links\", \"member\"].edge_attr.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "32572b65-6bad-49d2-9f92-8a7cb287603b",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "def split_heterodata_edges(data, val_ratio=0.1, test_ratio=0.1):\n",
    "    transform = RandomLinkSplit(\n",
    "        num_val=val_ratio,\n",
    "        num_test=test_ratio,\n",
    "        is_undirected=False,  # Set True if your graph is undirected\n",
    "        add_negative_train_samples=False,  # Set True if negative samples are needed\n",
    "        edge_types=(\"provider\", \"links\", \"member\"),\n",
    "        rev_edge_types=None  # Define if you have reverse edges\n",
    "    )\n",
    "    train_data, val_data, test_data = transform(data)\n",
    "    \n",
    "    return train_data, val_data, test_data\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "d259affa-9e1f-4313-8905-7d73273ca24c",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, val_data, test_data = split_heterodata_edges(data, val_ratio=0.2, test_ratio=0.2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5ad420eb-dc32-402a-a684-f8bfc03b682b",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "HeteroData(\n",
       "  provider={\n",
       "    x=[652, 38],\n",
       "    synthetic_labels=[652],\n",
       "  },\n",
       "  member={\n",
       "    x=[32560, 23],\n",
       "    synthetic_labels=[32560],\n",
       "  },\n",
       "  (provider, links, member)={\n",
       "    edge_index=[2, 70796],\n",
       "    edge_attr=[70796],\n",
       "    edge_label=[70796],\n",
       "    edge_label_index=[2, 70796],\n",
       "  }\n",
       ")"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "56abfc16-8528-4a96-836f-0f94360a6bfc",
   "metadata": {},
   "source": [
    "### 4. Main model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b5d7465-88c0-4f8b-8c98-82258fecfff4",
   "metadata": {},
   "source": [
    "#### 4.1 Model architecture "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "8c1903e4-5076-47e9-9c8f-bdf9af4ed7df",
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch_geometric.nn import HeteroConv, Linear\n",
    "\n",
    "class HeteroAutoEncoder(torch.nn.Module):\n",
    "    def __init__(self, metadata, hidden_channels=16, out_channels=8):\n",
    "        \"\"\"\n",
    "        :param metadata: data.metadata() -> (node_types, edge_types)\n",
    "        :param hidden_channels: dimension of hidden embeddings\n",
    "        :param out_channels: dimension of final latent embeddings\n",
    "        \"\"\"\n",
    "        super().__init__()\n",
    "        self.metadata = metadata\n",
    "        \n",
    "        # 1) HeteroConv for encoding\n",
    "        #    We'll define a GNN conv for each (source, rel, target) edge type\n",
    "        #    e.g., \"provider\"->\"member\" and optionally reverse if defined\n",
    "        self.convs = torch.nn.ModuleList()\n",
    "\n",
    "        # Example: single layer HeteroConv with SAGEConv (or GCNConv)\n",
    "        convs_dict = {}\n",
    "        for edge_type in metadata[1]:  # edge_types\n",
    "            # e.g. edge_type = (\"provider\", \"links\", \"member\")\n",
    "            convs_dict[edge_type] = SAGEConv(\n",
    "                (-1, -1),  # (in_channels for src, in_channels for dst)\n",
    "                hidden_channels\n",
    "            )\n",
    "        self.hetero_conv = HeteroConv(convs_dict, aggr='sum')\n",
    "        \n",
    "        # 2) MLP to go from hidden -> latent\n",
    "        #    We'll store a separate Linear for each node type to unify dimension\n",
    "        self.lin_dict = torch.nn.ModuleDict()\n",
    "        for node_type in metadata[0]:  # node_types\n",
    "            # Example: hidden_channels -> out_channels\n",
    "            self.lin_dict[node_type] = Linear(hidden_channels, out_channels)\n",
    "\n",
    "        # 3) Decoders for attribute reconstruction\n",
    "        #    We'll also store a separate MLP per node type to go from out_channels -> original dim\n",
    "        self.decoder_attr = torch.nn.ModuleDict()\n",
    "        # For adjacency, we'll do dot-product, no separate MLP needed\n",
    "        # But if you prefer a param-based decoder, you could add it here\n",
    "        \n",
    "\n",
    "    def forward(self, x_dict, edge_index_dict):\n",
    "        \"\"\"\n",
    "        Encoding step: produce out_channels embeddings for each node type\n",
    "        :param x_dict: {node_type: [num_nodes, in_dim]}\n",
    "        :param edge_index_dict: {edge_type: [2, E]}\n",
    "        :return: z_dict, a dict of {node_type: [num_nodes, out_channels]} embeddings\n",
    "        \"\"\"\n",
    "        # HeteroConv expects x_dict, edge_index_dict\n",
    "        h_dict = self.hetero_conv(x_dict, edge_index_dict)\n",
    "        \n",
    "        # Apply linear transform per node_type\n",
    "        z_dict = {}\n",
    "        for node_type, h in h_dict.items():\n",
    "            z_dict[node_type] = self.lin_dict[node_type](h)\n",
    "        \n",
    "        return z_dict\n",
    "    \n",
    "    def decode_attributes(self, z_dict, original_x_dict):\n",
    "        \"\"\"\n",
    "        Reconstruct node attributes from embeddings\n",
    "        :return: recon_x_dict with same shape as original_x_dict\n",
    "        \"\"\"\n",
    "        recon_x_dict = {}\n",
    "        for node_type, z in z_dict.items():\n",
    "            out_dim = original_x_dict[node_type].size(1)\n",
    "            # We'll define the attribute decoder on the fly if not exists\n",
    "            # or store them in self.decoder_attr\n",
    "            if node_type not in self.decoder_attr:\n",
    "                self.decoder_attr[node_type] = torch.nn.Sequential(\n",
    "                    Linear(z.size(1), z.size(1)),\n",
    "                    torch.nn.ReLU(),\n",
    "                    Linear(z.size(1), out_dim) \n",
    "                )\n",
    "            recon_x = self.decoder_attr[node_type](z)\n",
    "            recon_x_dict[node_type] = recon_x\n",
    "        return recon_x_dict\n",
    "    \n",
    "    def decode_adjacency(self, z_src, z_dst, edge_index):\n",
    "        \"\"\"\n",
    "        Dot-product decoder for adjacency or edge weight\n",
    "        :param z_src: [num_src_nodes, out_channels]\n",
    "        :param z_dst: [num_dst_nodes, out_channels]\n",
    "        :param edge_index: [2, E], specifying (src, dst) pairs\n",
    "        :return: predicted edge weights [E]\n",
    "        \"\"\"\n",
    "        # gather node embeddings\n",
    "        src = edge_index[0]\n",
    "        dst = edge_index[1]\n",
    "        z_s = z_src[src]\n",
    "        z_d = z_dst[dst]\n",
    "        # dot product\n",
    "        edge_pred = (z_s * z_d).sum(dim=-1)\n",
    "        # You might want a sigmoid or other transform if edges are in [0,1]\n",
    "        # or do direct MSE if edges are real-valued\n",
    "        return edge_pred\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40e36ca9-3bdc-4b44-9301-8ef79b22d277",
   "metadata": {},
   "source": [
    "#### 4.2 Training the model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "0c600957-fd42-42fc-b231-6442613ff07b",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from torch_geometric.transforms import RandomLinkSplit\n",
    "\n",
    "def train_autoencoder(data, hidden_channels=16, out_channels=8, lr=1e-3, epochs=50, alpha=0.5):\n",
    "    \"\"\"\n",
    "    Train a Heterogeneous Autoencoder with attribute and adjacency reconstruction losses.\n",
    "    \n",
    "    Fixes:\n",
    "    - Ensures correct edge indexing after `RandomLinkSplit`\n",
    "    - Prevents node mismatch by reindexing node features\n",
    "    - Verifies that providers and members are correctly ordered\n",
    "    \"\"\"\n",
    "    device = torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n",
    "    model = HeteroAutoEncoder(data.metadata(), hidden_channels, out_channels).to(device)\n",
    "    optimizer = torch.optim.Adam(model.parameters(), lr=lr)\n",
    "    \n",
    "    # Move data to device\n",
    "    data = data.to(device)\n",
    "\n",
    "    # Apply RandomLinkSplit to generate negative samples for link prediction\n",
    "    transform = RandomLinkSplit(\n",
    "        num_val=0.1, num_test=0.1, \n",
    "        add_negative_train_samples=True,\n",
    "        edge_types=(\"provider\", \"links\", \"member\")\n",
    "    )\n",
    "    train_data, val_data, test_data = transform(data)\n",
    "\n",
    "    loss_history = []\n",
    "\n",
    "    # Ensure correct x_dict mapping\n",
    "    x_dict = {node_type: train_data[node_type].x for node_type in train_data.node_types}\n",
    "\n",
    "    # Ensure edge index is within valid range\n",
    "    edge_index = train_data[\"provider\", \"links\", \"member\"].edge_index\n",
    "\n",
    "    # ✅ Fix swapped provider ↔ member issue\n",
    "    if edge_index[0].max() > x_dict[\"provider\"].size(0) - 1 or edge_index[1].max() > x_dict[\"member\"].size(0) - 1:\n",
    "        print(\"⚠️ Warning: Swapping providers and members in edge_index!\")\n",
    "        edge_index = edge_index[[1, 0], :]  # Swap rows\n",
    "\n",
    "    # Restrict edge index to valid nodes\n",
    "    max_provider_id = x_dict[\"provider\"].size(0) - 1\n",
    "    max_member_id = x_dict[\"member\"].size(0) - 1\n",
    "    valid_edges = (edge_index[0] <= max_provider_id) & (edge_index[1] <= max_member_id)\n",
    "    edge_index = edge_index[:, valid_edges]\n",
    "\n",
    "    edge_index_dict = {(\"provider\", \"links\", \"member\"): edge_index}\n",
    "\n",
    "    # Check for edge attributes (weights)\n",
    "    edge_attr = train_data[\"provider\", \"links\", \"member\"].edge_attr if hasattr(train_data[\"provider\", \"links\", \"member\"], 'edge_attr') else None\n",
    "\n",
    "    for epoch in range(1, epochs + 1):\n",
    "        model.train()\n",
    "        optimizer.zero_grad()\n",
    "        \n",
    "        # 1) Encode node features\n",
    "        z_dict = model(x_dict, edge_index_dict)\n",
    "\n",
    "        # 2) Decode node attributes\n",
    "        recon_x_dict = model.decode_attributes(z_dict, x_dict)\n",
    "\n",
    "        # 3) Compute attribute reconstruction loss (MSE)\n",
    "        loss_attr = sum(F.mse_loss(recon_x_dict[n], x_dict[n]) for n in x_dict)\n",
    "\n",
    "        # 4) Decode adjacency (link prediction)\n",
    "        z_provider = z_dict[\"provider\"]\n",
    "        z_member   = z_dict[\"member\"]\n",
    "        pred_edges = model.decode_adjacency(z_provider, z_member, edge_index_dict[(\"provider\", \"links\", \"member\")])\n",
    "\n",
    "        # 5) Compute adjacency reconstruction loss\n",
    "        if edge_attr is not None:\n",
    "            # Edge-weighted loss (if real-valued)\n",
    "            loss_adj = F.mse_loss(pred_edges, edge_attr, reduction=\"none\")\n",
    "            loss_adj = (loss_adj * edge_attr).mean()\n",
    "        else:\n",
    "            # Binary cross-entropy loss for link prediction\n",
    "            loss_adj = F.binary_cross_entropy_with_logits(pred_edges, torch.ones_like(pred_edges))\n",
    "\n",
    "        # 6) Dynamically adjust alpha for balanced loss scaling\n",
    "        alpha_tuned = alpha * (loss_attr.item() / (loss_adj.item() + 1e-8))  # Normalize alpha\n",
    "        loss = loss_attr + alpha_tuned * loss_adj\n",
    "\n",
    "        # Backpropagation & Optimization\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "\n",
    "        # Store loss history\n",
    "        loss_history.append(loss.item())\n",
    "\n",
    "        # Logging every 10 epochs\n",
    "        if epoch % 10 == 0:\n",
    "            print(f\"Epoch {epoch:02d}, Loss: {loss.item():.4f}, Attr: {loss_attr.item():.4f}, Adj: {loss_adj.item():.4f}\")\n",
    "\n",
    "    return model, loss_history\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "63fa578c-f738-418f-9c3e-d95134b076dc",
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "⚠️ Warning: Swapping providers and members in edge_index!\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'provider'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[21], line 2\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;66;03m# Pass the full dataset instead of an already split dataset\u001b[39;00m\n\u001b[0;32m----> 2\u001b[0m model, loss_history \u001b[38;5;241m=\u001b[39m \u001b[43mtrain_autoencoder\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdata\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mhidden_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m16\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout_channels\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m8\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m1e-3\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mepochs\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m50\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "Cell \u001b[0;32mIn[20], line 64\u001b[0m, in \u001b[0;36mtrain_autoencoder\u001b[0;34m(data, hidden_channels, out_channels, lr, epochs, alpha)\u001b[0m\n\u001b[1;32m     61\u001b[0m recon_x_dict \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecode_attributes(z_dict, x_dict)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# 3) Compute attribute reconstruction loss (MSE)\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m loss_attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43msum\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmse_loss\u001b[49m\u001b[43m(\u001b[49m\u001b[43mrecon_x_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mx_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m\u001b[43m)\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mn\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx_dict\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# 4) Decode adjacency (link prediction)\u001b[39;00m\n\u001b[1;32m     67\u001b[0m z_provider \u001b[38;5;241m=\u001b[39m z_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprovider\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "Cell \u001b[0;32mIn[20], line 64\u001b[0m, in \u001b[0;36m<genexpr>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     61\u001b[0m recon_x_dict \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mdecode_attributes(z_dict, x_dict)\n\u001b[1;32m     63\u001b[0m \u001b[38;5;66;03m# 3) Compute attribute reconstruction loss (MSE)\u001b[39;00m\n\u001b[0;32m---> 64\u001b[0m loss_attr \u001b[38;5;241m=\u001b[39m \u001b[38;5;28msum\u001b[39m(F\u001b[38;5;241m.\u001b[39mmse_loss(\u001b[43mrecon_x_dict\u001b[49m\u001b[43m[\u001b[49m\u001b[43mn\u001b[49m\u001b[43m]\u001b[49m, x_dict[n]) \u001b[38;5;28;01mfor\u001b[39;00m n \u001b[38;5;129;01min\u001b[39;00m x_dict)\n\u001b[1;32m     66\u001b[0m \u001b[38;5;66;03m# 4) Decode adjacency (link prediction)\u001b[39;00m\n\u001b[1;32m     67\u001b[0m z_provider \u001b[38;5;241m=\u001b[39m z_dict[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mprovider\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n",
      "\u001b[0;31mKeyError\u001b[0m: 'provider'"
     ]
    }
   ],
   "source": [
    "# Pass the full dataset instead of an already split dataset\n",
    "model, loss_history = train_autoencoder(data, hidden_channels=16, out_channels=8, lr=1e-3, epochs=50)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f69c65c-9cb7-4e75-a887-45f5bcb33be5",
   "metadata": {},
   "source": [
    "#### 4.3 Anomaly scoring"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3474db60-1539-43c1-bb32-419e5a8807a9",
   "metadata": {},
   "outputs": [],
   "source": [
    "def compute_anomaly_scores(model, data):\n",
    "    device = data[\"provider\"].x.device\n",
    "    model.eval()\n",
    "    \n",
    "    x_dict = {\n",
    "        \"provider\": data[\"provider\"].x,\n",
    "        \"member\":   data[\"member\"].x\n",
    "    }\n",
    "    edge_index_dict = {\n",
    "        (\"provider\", \"links\", \"member\"): data[\"provider\", \"links\", \"member\"].edge_index\n",
    "    }\n",
    "    edge_attr = data[\"provider\", \"links\", \"member\"].edge_attr\n",
    "    \n",
    "    with torch.no_grad():\n",
    "        # Encode\n",
    "        z_dict = model(x_dict, edge_index_dict)\n",
    "        # Decode attributes\n",
    "        recon_x_dict = model.decode_attributes(z_dict, x_dict)\n",
    "        # Compute node-level attribute error\n",
    "        # We'll do MSE across features\n",
    "        attr_error_provider = torch.sum((recon_x_dict[\"provider\"] - x_dict[\"provider\"])**2, dim=1)\n",
    "        attr_error_member   = torch.sum((recon_x_dict[\"member\"]   - x_dict[\"member\"])**2, dim=1)\n",
    "        \n",
    "        # Adjacency reconstruction\n",
    "        z_provider = z_dict[\"provider\"]\n",
    "        z_member   = z_dict[\"member\"]\n",
    "        edge_idx = edge_index_dict[(\"provider\", \"links\", \"member\")]\n",
    "        pred_edges = model.decode_adjacency(z_provider, z_member, edge_idx)\n",
    "        \n",
    "        # Edge error\n",
    "        if edge_attr is not None:\n",
    "            edge_error = (pred_edges - edge_attr)**2\n",
    "        else:\n",
    "            edge_error = (pred_edges - 1.0)**2\n",
    "        \n",
    "        # We'll accumulate adjacency errors for each node\n",
    "        node_adj_error_provider = torch.zeros(z_provider.size(0), device=device)\n",
    "        node_adj_error_member   = torch.zeros(z_member.size(0),   device=device)\n",
    "        \n",
    "        for e_idx in range(edge_error.size(0)):\n",
    "            p_idx = edge_idx[0, e_idx]\n",
    "            m_idx = edge_idx[1, e_idx]\n",
    "            node_adj_error_provider[p_idx] += edge_error[e_idx]\n",
    "            node_adj_error_member[m_idx]   += edge_error[e_idx]\n",
    "        \n",
    "        # Combine node-level anomaly scores\n",
    "        # e.g. anomaly_score = attr_error + alpha * adj_error\n",
    "        alpha = 0.5\n",
    "        provider_score = attr_error_provider + alpha * node_adj_error_provider\n",
    "        member_score   = attr_error_member   + alpha * node_adj_error_member\n",
    "        \n",
    "    return provider_score, member_score\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2d486c4f-7fde-46f1-9bc5-ed8ed87b387c",
   "metadata": {},
   "source": [
    "#### 4.4 Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02831064-d543-41b0-877f-fb1a465c2c3c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def evaluate_anomalies(provider_score, member_score, data):\n",
    "    # Convert to cpu numpy for sklearn\n",
    "    p_score_np = provider_score.detach().cpu().numpy()\n",
    "    m_score_np = member_score.detach().cpu().numpy()\n",
    "    \n",
    "    p_label_np = data[\"provider\"].synthetic_labels.cpu().numpy()\n",
    "    m_label_np = data[\"member\"].synthetic_labels.cpu().numpy()\n",
    "    \n",
    "    from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "    \n",
    "    # Providers\n",
    "    if len(np.unique(p_label_np)) > 1:  # At least 1 anomaly\n",
    "        auc_p = roc_auc_score(p_label_np, p_score_np)\n",
    "        ap_p  = average_precision_score(p_label_np, p_score_np)\n",
    "    else:\n",
    "        auc_p, ap_p = -1, -1\n",
    "    \n",
    "    # Members\n",
    "    if len(np.unique(m_label_np)) > 1:\n",
    "        auc_m = roc_auc_score(m_label_np, m_score_np)\n",
    "        ap_m  = average_precision_score(m_label_np, m_score_np)\n",
    "    else:\n",
    "        auc_m, ap_m = -1, -1\n",
    "    \n",
    "    print(f\"Provider AUC: {auc_p:.4f}, AP: {ap_p:.4f}\")\n",
    "    print(f\"Member   AUC: {auc_m:.4f}, AP: {ap_m:.4f}\")\n",
    "    \n",
    "    # If you want an overall:\n",
    "    all_scores = np.concatenate([p_score_np, m_score_np])\n",
    "    all_labels = np.concatenate([p_label_np, m_label_np])\n",
    "    if len(np.unique(all_labels)) > 1:\n",
    "        auc_all = roc_auc_score(all_labels, all_scores)\n",
    "        ap_all  = average_precision_score(all_labels, all_scores)\n",
    "    else:\n",
    "        auc_all, ap_all = -1, -1\n",
    "    print(f\"Overall  AUC: {auc_all:.4f}, AP: {ap_all:.4f}\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a826631-0b25-48f1-af7a-a46eadb07d93",
   "metadata": {},
   "source": [
    "### 5. Baseline models "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65685d02-2e09-4147-9f8e-359946eae8b3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "karim_research",
   "language": "python",
   "name": "karim_research"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
