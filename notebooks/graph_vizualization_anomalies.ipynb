{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Vizualization of the graphs and anomalies\n",
    "\n",
    "This notebook is made for vizualizing the graphs and anomalies injected"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the extension\n",
    "%load_ext autoreload\n",
    "# Configure it to reload all modules before each cell execution\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "sys.path.append('..')\n",
    "\n",
    "import torch\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# Import custom modules\n",
    "from src.data.dataloader import load_member_features, load_provider_features, load_claims_data, prepare_hetero_data, inject_anomalies\n",
    "from src.models.main_model import BipartiteGraphAutoEncoder\n",
    "from src.models.baseline_models import MLPAutoencoder, SklearnBaseline, GCNAutoencoder, GATAutoencoder, SAGEAutoencoder\n",
    "from src.utils.train_utils import train_model\n",
    "from src.utils.eval_utils import evaluate_anomaly_detection, plot_precision_recall_curves, plot_anomaly_distribution, compare_models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load and prepare data\n",
    "df_member_features, members_dataset = load_member_features(\"../data/final_members_df.pickle\")\n",
    "df_provider_features, providers_dataset = load_provider_features(\"../data/final_df.pickle\")\n",
    "df_edges = load_claims_data(\"../data/df_descriptions.pickle\", members_dataset, providers_dataset)\n",
    "data = prepare_hetero_data(df_member_features, df_provider_features, df_edges)\n",
    "\n",
    "# Set random seed\n",
    "np.random.seed(42)\n",
    "torch.manual_seed(42)\n",
    "\n",
    "# Inject anomalies with tracking\n",
    "print(\"Injecting anomalies with tracking...\")\n",
    "modified_data, gt_labels, anomaly_tracking = inject_mixed_anomalies_with_tracking(\n",
    "    data, \n",
    "    percentages={'structural': 0.03, 'feature': 0.04, 'healthcare': 0.03},\n",
    "    methods=['structural', 'feature', 'healthcare']\n",
    ")\n",
    "\n",
    "# Print statistics about injected anomalies\n",
    "print(f\"Injected {gt_labels['member'].sum().item()} member anomalies\")\n",
    "print(f\"Injected {gt_labels['provider'].sum().item()} provider anomalies\")\n",
    "\n",
    "# Display anomaly type distribution\n",
    "print(\"\\nAnomaly type distribution:\")\n",
    "member_type_counts = {}\n",
    "provider_type_counts = {}\n",
    "\n",
    "for idx, types in anomaly_tracking['member'].items():\n",
    "    for anomaly_type in types:\n",
    "        member_type_counts[anomaly_type] = member_type_counts.get(anomaly_type, 0) + 1\n",
    "\n",
    "for idx, types in anomaly_tracking['provider'].items():\n",
    "    for anomaly_type in types:\n",
    "        provider_type_counts[anomaly_type] = provider_type_counts.get(anomaly_type, 0) + 1\n",
    "\n",
    "print(\"Member anomaly types:\")\n",
    "for anomaly_type, count in member_type_counts.items():\n",
    "    print(f\"  {anomaly_type}: {count}\")\n",
    "\n",
    "print(\"Provider anomaly types:\")\n",
    "for anomaly_type, count in provider_type_counts.items():\n",
    "    print(f\"  {anomaly_type}: {count}\")\n",
    "\n",
    "# Train your anomaly detection model\n",
    "# For demonstration, let's assume your trained model is in 'model'\n",
    "# and has been trained on the modified_data\n",
    "\n",
    "# Evaluate the model\n",
    "results = evaluate_anomaly_detection(model, modified_data, gt_labels)\n",
    "\n",
    "print(\"\\nOverall model performance:\")\n",
    "for node_type, metrics in results.items():\n",
    "    print(f\"{node_type}: AUC={metrics['auc']:.4f}, AP={metrics['ap']:.4f}\")\n",
    "\n",
    "# Now let's visualize the results\n",
    "\n",
    "# 1. Visualize the graph with anomalies highlighted\n",
    "print(\"\\nGenerating graph visualization...\")\n",
    "net = visualize_anomaly_graph(\n",
    "    modified_data, \n",
    "    gt_labels=gt_labels, \n",
    "    anomaly_tracking=anomaly_tracking,\n",
    "    sample_size=50,  # Adjust based on your graph size\n",
    "    filename='healthcare_anomalies.html'\n",
    ")\n",
    "# Note: In a Jupyter notebook, you can display the visualization with: net.show('healthcare_anomalies.html')\n",
    "print(f\"Graph visualization saved to healthcare_anomalies.html\")\n",
    "\n",
    "# 2. Visualize feature anomalies\n",
    "print(\"\\nGenerating feature anomaly visualizations...\")\n",
    "visualize_feature_anomalies(\n",
    "    modified_data, \n",
    "    gt_labels, \n",
    "    anomaly_tracking=anomaly_tracking,\n",
    "    node_type='provider',\n",
    "    max_nodes=30\n",
    ")\n",
    "\n",
    "visualize_feature_anomalies(\n",
    "    modified_data, \n",
    "    gt_labels, \n",
    "    anomaly_tracking=anomaly_tracking,\n",
    "    node_type='member',\n",
    "    max_nodes=30\n",
    ")\n",
    "\n",
    "# 3. Visualize model anomaly scores\n",
    "print(\"\\nGenerating anomaly score visualizations...\")\n",
    "visualize_anomaly_scores(\n",
    "    results,\n",
    "    anomaly_tracking=anomaly_tracking,\n",
    "    top_n=50\n",
    ")\n",
    "\n",
    "# 4. Evaluate model performance by anomaly type\n",
    "print(\"\\nEvaluating model performance by anomaly type...\")\n",
    "\n",
    "# For providers\n",
    "provider_scores = results['provider']['scores']\n",
    "provider_labels = results['provider']['labels']\n",
    "provider_anomaly_indices = np.where(provider_labels == 1)[0]\n",
    "\n",
    "anomaly_type_performance = {}\n",
    "for anomaly_type in provider_type_counts.keys():\n",
    "    # Get indices of anomalies with this type\n",
    "    indices = [idx for idx in provider_anomaly_indices if idx in anomaly_tracking['provider'] \n",
    "               and anomaly_type in anomaly_tracking['provider'][idx]]\n",
    "    \n",
    "    if not indices:\n",
    "        continue\n",
    "        \n",
    "    # Convert to numpy array\n",
    "    indices = np.array(indices)\n",
    "    \n",
    "    # Create binary labels for this specific anomaly type\n",
    "    type_labels = np.zeros_like(provider_labels)\n",
    "    type_labels[indices] = 1\n",
    "    \n",
    "    # Calculate AUC for this type only (where type_labels is 1)\n",
    "    from sklearn.metrics import roc_auc_score\n",
    "    \n",
    "    # Only evaluate where either the real label or type label is 1\n",
    "    eval_mask = (provider_labels == 1) | (type_labels == 1)\n",
    "    if np.sum(eval_mask) > 0 and np.sum(type_labels[eval_mask]) > 0:\n",
    "        try:\n",
    "            auc = roc_auc_score(type_labels[eval_mask], provider_scores[eval_mask])\n",
    "            anomaly_type_performance[anomaly_type] = auc\n",
    "        except Exception as e:\n",
    "            print(f\"Couldn't calculate AUC for {anomaly_type}: {e}\")\n",
    "\n",
    "print(\"\\nModel performance by anomaly type (Provider):\")\n",
    "for anomaly_type, auc in anomaly_type_performance.items():\n",
    "    print(f\"  {anomaly_type}: AUC={auc:.4f}\")\n",
    "\n",
    "# 5. Generate confusion matrices for different anomaly types\n",
    "import matplotlib.pyplot as plt\n",
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay\n",
    "\n",
    "# Determine a threshold for prediction (e.g., 95th percentile of scores)\n",
    "threshold = np.percentile(provider_scores, 95)\n",
    "y_pred = (provider_scores > threshold).astype(int)\n",
    "\n",
    "# Overall confusion matrix\n",
    "cm = confusion_matrix(provider_labels, y_pred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Normal', 'Anomaly'])\n",
    "disp.plot(cmap=plt.cm.Blues)\n",
    "plt.title('Overall Confusion Matrix (Providers)')\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "# Confusion matrices by anomaly type\n",
    "for anomaly_type in ['structural', 'feature', 'healthcare']:\n",
    "    # Get indices with this anomaly type\n",
    "    indices = np.array([i for i in range(len(provider_labels)) \n",
    "                      if i in anomaly_tracking['provider'] and \n",
    "                      any(atype.startswith(anomaly_type) for atype in anomaly_tracking['provider'][i])])\n",
    "    \n",
    "    if len(indices) == 0:\n",
    "        continue\n",
    "        \n",
    "    # Create binary labels\n",
    "    type_labels = np.zeros_like(provider_labels)\n",
    "    type_labels[indices] = 1\n",
    "    \n",
    "    # Calculate confusion matrix\n",
    "    cm = confusion_matrix(type_labels, y_pred)\n",
    "    disp = ConfusionMatrixDisplay(confusion_matrix=cm, display_labels=['Normal', anomaly_type])\n",
    "    disp.plot(cmap=plt.cm.Blues)\n",
    "    plt.title(f'Confusion Matrix for {anomaly_type} Anomalies (Providers)')\n",
    "    plt.tight_layout()\n",
    "    plt.show()\n",
    "\n",
    "# 6. Save the anomaly information for later use\n",
    "import pickle\n",
    "anomaly_data = {\n",
    "    'modified_data': modified_data,\n",
    "    'gt_labels': gt_labels,\n",
    "    'anomaly_tracking': anomaly_tracking,\n",
    "    'detection_results': results\n",
    "}\n",
    "\n",
    "with open('anomaly_data.pkl', 'wb') as f:\n",
    "    pickle.dump(anomaly_data, f)\n",
    "\n",
    "print(\"\\nAnomalies data saved to 'anomaly_data.pkl'\")\n",
    "\n",
    "# 7. Compare model performance across multiple runs with different anomaly configurations\n",
    "# This would typically be done as part of a larger experiment\n",
    "print(\"\\nComparing model performance with different anomaly configurations...\")\n",
    "\n",
    "def run_anomaly_experiment(data, model, configurations):\n",
    "    results = {}\n",
    "    \n",
    "    for config_name, config in configurations.items():\n",
    "        print(f\"\\nRunning experiment with configuration: {config_name}\")\n",
    "        \n",
    "        # Inject anomalies according to this configuration\n",
    "        modified_data, gt_labels, _ = inject_mixed_anomalies_with_tracking(\n",
    "            data, \n",
    "            percentages=config['percentages'],\n",
    "            methods=config['methods']\n",
    "        )\n",
    "        \n",
    "        # Evaluate model\n",
    "        eval_results = evaluate_anomaly_detection(model, modified_data, gt_labels)\n",
    "        \n",
    "        # Store results\n",
    "        results[config_name] = {\n",
    "            'provider_auc': eval_results['provider']['auc'],\n",
    "            'member_auc': eval_results['member']['auc']\n",
    "        }\n",
    "    \n",
    "    return results\n",
    "\n",
    "# Define different anomaly configurations to test\n",
    "anomaly_configurations = {\n",
    "    'structural_only': {\n",
    "        'percentages': {'structural': 0.05},\n",
    "        'methods': ['structural']\n",
    "    },\n",
    "    'feature_only': {\n",
    "        'percentages': {'feature': 0.05},\n",
    "        'methods': ['feature']\n",
    "    },\n",
    "    'healthcare_only': {\n",
    "        'percentages': {'healthcare': 0.05},\n",
    "        'methods': ['healthcare']\n",
    "    },\n",
    "    'mixed_equal': {\n",
    "        'percentages': {'structural': 0.03, 'feature': 0.03, 'healthcare': 0.03},\n",
    "        'methods': ['structural', 'feature', 'healthcare']\n",
    "    },\n",
    "    'mixed_weighted': {\n",
    "        'percentages': {'structural': 0.04, 'feature': 0.02, 'healthcare': 0.03},\n",
    "        'methods': ['structural', 'feature', 'healthcare']\n",
    "    }\n",
    "}\n",
    "\n",
    "# Run experiments (this would be part of your larger evaluation pipeline)\n",
    "# experiment_results = run_anomaly_experiment(data, model, anomaly_configurations)\n",
    "\n",
    "# Visualize comparative results (using dummy data since we didn't run the actual experiments)\n",
    "experiment_results = {\n",
    "    'structural_only': {'provider_auc': 0.92, 'member_auc': 0.88},\n",
    "    'feature_only': {'provider_auc': 0.85, 'member_auc': 0.82},\n",
    "    'healthcare_only': {'provider_auc': 0.79, 'member_auc': 0.75},\n",
    "    'mixed_equal': {'provider_auc': 0.83, 'member_auc': 0.80},\n",
    "    'mixed_weighted': {'provider_auc': 0.86, 'member_auc': 0.81}\n",
    "}\n",
    "\n",
    "# Plot comparison\n",
    "plt.figure(figsize=(12, 6))\n",
    "configurations = list(experiment_results.keys())\n",
    "provider_auc = [experiment_results[config]['provider_auc'] for config in configurations]\n",
    "member_auc = [experiment_results[config]['member_auc'] for config in configurations]\n",
    "\n",
    "x = np.arange(len(configurations))\n",
    "width = 0.35\n",
    "\n",
    "plt.bar(x - width/2, provider_auc, width, label='Provider AUC')\n",
    "plt.bar(x + width/2, member_auc, width, label='Member AUC')\n",
    "\n",
    "plt.ylabel('AUC Score')\n",
    "plt.title('Model Performance Across Different Anomaly Configurations')\n",
    "plt.xticks(x, configurations, rotation=45)\n",
    "plt.ylim(0.7, 1.0)\n",
    "plt.legend()\n",
    "plt.tight_layout()\n",
    "plt.show()\n",
    "\n",
    "print(\"\\nAnomalies visualization and analysis complete!\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "degree-project-gad-healthcare",
   "language": "python",
   "name": "degree-project-gad-healthcare"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
