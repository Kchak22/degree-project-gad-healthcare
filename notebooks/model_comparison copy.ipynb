{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Anomaly Detection in Bipartite Graphs - Model Comparison\n",
    "\n",
    "This notebook compares different models for anomaly detection in bipartite graphs."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 0. Reload"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Load the extension\n",
    "%load_ext autoreload\n",
    "# Configure it to reload all modules before each cell execution\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sys\n",
    "import os\n",
    "sys.path.append('..')\n",
    "\n",
    "# --- MPS Fallback ---\n",
    "os.environ['PYTORCH_ENABLE_MPS_FALLBACK'] = '1'\n",
    "print(f\"PYTORCH_ENABLE_MPS_FALLBACK set to: {os.environ.get('PYTORCH_ENABLE_MPS_FALLBACK')}\")\n",
    "\n",
    "import torch\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "import torch_geometric.transforms as T\n",
    "import torch_geometric\n",
    "from sklearn.metrics import precision_recall_curve\n",
    "\n",
    "# Import custom modules\n",
    "from src.data.dataloader import load_member_features, load_provider_features, load_claims_data, load_claims_data_with_splitting, prepare_hetero_data, prepare_hetero_data_with_splitting\n",
    "from src.data.anomaly_injection import  * \n",
    "from src.models.main_model import *\n",
    "from src.models.baseline_models import MLPAutoencoder, SklearnBaseline, GCNAutoencoder, GATAutoencoder, SAGEAutoencoder\n",
    "from src.utils.vizualize import *\n",
    "from src.utils.train_utils import *\n",
    "from src.utils.eval_utils import *\n",
    "from src.utils.stat_utils import *"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_member_features, members_dataset = load_member_features(\"../data/final_members_df.pickle\")\n",
    "df_provider_features, providers_dataset = load_provider_features(\"../data/final_df.pickle\")\n",
    "df_edges = load_claims_data(\"../data/df_descriptions.pickle\", members_dataset=members_dataset, providers_dataset=providers_dataset)\n",
    "df_edges, train_edges, val_edges, test_edges = load_claims_data_with_splitting(\"../data/df_descriptions.pickle\", members_dataset=members_dataset, providers_dataset=providers_dataset)\n",
    "\n",
    "\n",
    "import pickle\n",
    "with open(\"../data/df_descriptions.pickle\", 'rb') as pickle_file:\n",
    "        df = pickle.load(pickle_file)\n",
    "\n",
    "print(f\"Members: {len(members_dataset)}\")\n",
    "print(f\"Providers: {len(providers_dataset)}\")\n",
    "print(f\"Edges: {len(df_edges)}\")\n",
    "\n",
    "print(\"\\nMember features:\")\n",
    "display(df_member_features.head())\n",
    "print(\"\\nProvider features:\")\n",
    "display(df_provider_features.head())\n",
    "print(\"\\nEdges:\")\n",
    "display(df_edges.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create HeteroData object\n",
    "data = prepare_hetero_data(df_member_features, df_provider_features, df_edges)\n",
    "# Split the data into training, validation and testing sets based on temporal split \n",
    "train_data, val_data, test_data = prepare_hetero_data_with_splitting(df_member_features, df_provider_features, train_edges, val_edges, test_edges)\n",
    "print(data)\n",
    "print(f\"Member features shape: {data['member'].x.shape}\")\n",
    "print(f\"Provider features shape: {data['provider'].x.shape}\")\n",
    "print(f\"Number of edges: {data['provider', 'to', 'member'].edge_index.shape[1]}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Injecting Anomalies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "from src.data.new_anomaly_injection import *\n",
    "\n",
    "\n",
    "modified_graph, node_labels, edge_labels, tracking = inject_scenario_anomalies(\n",
    "        data,\n",
    "        p_node_anomalies=0.05,   # Target 10% of nodes per type\n",
    "        p_edge_anomalies=0.1,  # Target 5% of edges to be anomalous\n",
    "        lambda_structural=0.5, # 50% structural, 50% attribute focus\n",
    "        seed=42\n",
    "    )\n",
    "\n",
    "    \n",
    "summarize_injected_anomalies(\n",
    "      modified_graph,\n",
    "      node_labels,\n",
    "      edge_labels,\n",
    "      tracking\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "summarize_injected_anomalies(\n",
    "      modified_graph,\n",
    "      node_labels,\n",
    "      edge_labels,\n",
    "      tracking\n",
    " )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Splitting into training, validation and testing set"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3.1 Inductive Setting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.data_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Split the graph into train, val, and test sets (using temporal split)\n",
    "train_g, val_g, test_g = train_data, val_data, test_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# New version \n",
    "from src.data.new_anomaly_injection import *\n",
    "\n",
    "# Split the graph into train, val, and test sets (using temporal split)\n",
    "train_g, val_g, test_g = train_data, val_data, test_data\n",
    "\n",
    "p_node_anomalies = 0.05\n",
    "p_edge_anomalies = 0.1\n",
    "lambda_structural = 0.5\n",
    "seed = 42\n",
    "\n",
    "# Injecting anomalies in the training subgraph using the new version\n",
    "train_g, gt_node_labels_train, gt_edge_labels_dict_train, final_anomaly_tracking_train = inject_scenario_anomalies(\n",
    "        train_g,\n",
    "        p_node_anomalies=p_node_anomalies,   # Target 5% of nodes per type\n",
    "        p_edge_anomalies=p_edge_anomalies,  # Target 5% of edges to be anomalous\n",
    "        lambda_structural=lambda_structural, # 50% structural, 50% attribute focus\n",
    "        seed=seed\n",
    "    )\n",
    "print(\"\\n--- Training Graph Anomaly Summary ---\")\n",
    "summarize_injected_anomalies(\n",
    "      train_g   ,\n",
    "      gt_node_labels_train,\n",
    "      gt_edge_labels_dict_train,\n",
    "      final_anomaly_tracking_train\n",
    " )\n",
    " \n",
    " # Injecting anomalies in the validation subgraph using the new version\n",
    "val_g, gt_node_labels_val, gt_edge_labels_dict_val, final_anomaly_tracking_val = inject_scenario_anomalies(\n",
    "    val_g,\n",
    "    p_node_anomalies=p_node_anomalies,   # Target 10% of nodes per type\n",
    "    p_edge_anomalies=p_edge_anomalies,  # Target 5% of edges to be anomalous\n",
    "    lambda_structural=lambda_structural, # 50% structural, 50% attribute focus\n",
    "    seed=seed \n",
    ")\n",
    "print(\"\\n--- Validation Graph Anomaly Summary ---\")\n",
    "summarize_injected_anomalies(\n",
    "      val_g,\n",
    "      gt_node_labels_val,\n",
    "      gt_edge_labels_dict_val,\n",
    "      final_anomaly_tracking_val\n",
    ")  \n",
    "\n",
    "# Injecting anomalies in the testing subgraph using the new version\n",
    "test_g, gt_node_labels_test, gt_edge_labels_dict_test, final_anomaly_tracking_test = inject_scenario_anomalies(\n",
    "    test_g,\n",
    "    p_node_anomalies=p_node_anomalies,   # Target 10% of nodes per type\n",
    "    p_edge_anomalies=p_edge_anomalies,  # Target 5% of edges to be anomalous\n",
    "    lambda_structural=lambda_structural, # 50% structural, 50% attribute focus\n",
    "    seed=seed )\n",
    "print(\"\\n--- Testing Graph Anomaly Summary ---\")\n",
    "summarize_injected_anomalies(\n",
    "      test_g,\n",
    "      gt_node_labels_test,\n",
    "      gt_edge_labels_dict_test,\n",
    "      final_anomaly_tracking_test\n",
    " )\n",
    "\n",
    "\n",
    "comparison_input = {\n",
    "        'Train': (train_g, gt_node_labels_train, gt_edge_labels_dict_train, final_anomaly_tracking_train),\n",
    "        'Validation': (val_g, gt_node_labels_val, gt_edge_labels_dict_val, final_anomaly_tracking_val),\n",
    "        'Test': (test_g, gt_node_labels_test, gt_edge_labels_dict_test, final_anomaly_tracking_test)\n",
    "    }\n",
    "\n",
    "# Store ground truth node labels and anomaly tracking for each split\n",
    "GT_NODE_LABELS = {\n",
    "    \"train\": gt_node_labels_train,\n",
    "    \"val\": gt_node_labels_val,\n",
    "    \"test\": gt_node_labels_test\n",
    "}\n",
    "\n",
    "GT_EDGE_LABELS = {\n",
    "    \"train\": gt_edge_labels_dict_train,\n",
    "    \"val\": gt_edge_labels_dict_val,\n",
    "    \"test\": gt_edge_labels_dict_test\n",
    "}\n",
    "\n",
    "ANOMALY_TRACKING = {\n",
    "    \"train\": final_anomaly_tracking_train,\n",
    "    \"val\": final_anomaly_tracking_val,\n",
    "    \"test\": final_anomaly_tracking_test\n",
    "}\n",
    "\n",
    "# --- Run the Comparison ---\n",
    "compare_anomaly_splits(comparison_input)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(tuning_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Vizualize Anomalies \n",
    "visualize_anomaly_sample(\n",
    "        modified_graph,\n",
    "        node_labels,\n",
    "        edge_labels,\n",
    "        tracking,\n",
    "        num_instances_per_scenario=2, # Sample 2 nodes/instances per scenario\n",
    "        num_normal_nodes_per_type=30, # Increased normal nodes\n",
    "        neighborhood_hops=1,\n",
    "        provider_node_type='provider',\n",
    "        member_node_type='member',\n",
    "        output_filename=\"anomaly_sample_vis_shapes_full_struct.html\",\n",
    "        notebook=False\n",
    "    )"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Models' comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Check if MPS is available\n",
    "device = torch.device(\"mps\" if torch.backends.mps.is_available() else \"cpu\")\n",
    "print(f\"Using device: {device}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.1 Main model "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Move data to device for all splits\n",
    "print(f\"Moving data to device: {device}\")\n",
    "data_on_device = train_g.to(device)\n",
    "train_g_on_device = train_g.to(device)\n",
    "val_g_on_device = val_g.to(device)\n",
    "test_g_on_device = test_g.to(device)\n",
    "\n",
    "# Also move ground truth labels (needed for validation during training) for all splits\n",
    "gt_node_labels_on_device = {k: v.to(device) for k, v in gt_node_labels_train.items()}\n",
    "gt_node_labels_train_on_device = {k: v.to(device) for k, v in gt_node_labels_train.items()}\n",
    "gt_node_labels_val_on_device = {k: v.to(device) for k, v in gt_node_labels_val.items()}\n",
    "gt_node_labels_test_on_device = {k: v.to(device) for k, v in gt_node_labels_test.items()}\n",
    "\n",
    "# --- Model Hyperparameters ---\n",
    "# Set dimensions based on data\n",
    "in_dim_member = data_on_device['member'].x.size(1)\n",
    "in_dim_provider = data_on_device['provider'].x.size(1)\n",
    "hidden_dim = 64 \n",
    "latent_dim = 32 \n",
    "\n",
    "# Check if edge_attr exists and get its dimension\n",
    "target_edge_type_for_dim = ('provider', 'to', 'member')\n",
    "if hasattr(data_on_device[target_edge_type_for_dim], 'edge_attr') and data_on_device[target_edge_type_for_dim].edge_attr is not None:\n",
    "    edge_dim = data_on_device[target_edge_type_for_dim].edge_attr.size(1)\n",
    "    print(f\"Edge dimension (edge_dim) detected as: {edge_dim}\")\n",
    "else:\n",
    "    edge_dim = 0 # Or handle as error if edge attributes are expected\n",
    "    print(\"Warning: No edge_attr found for target edge type. Setting edge_dim=0.\")\n",
    "\n",
    "num_conv_layers = 2\n",
    "num_dec_layers = 2\n",
    "dropout_rate = 0.5 \n",
    "\n",
    "# --- Instantiate the Main Model ---\n",
    "# This will be the single model instance we train and evaluate\n",
    "bgae_model = BipartiteGraphAutoEncoder_ReportBased(\n",
    "    in_dim_member=in_dim_member,\n",
    "    in_dim_provider=in_dim_provider,\n",
    "    edge_dim=edge_dim,\n",
    "    hidden_dim=hidden_dim,\n",
    "    latent_dim=latent_dim,\n",
    "    num_conv_layers=num_conv_layers,\n",
    "    num_dec_layers=num_dec_layers,\n",
    "    dropout=dropout_rate\n",
    ").to(device)\n",
    "\n",
    "model = torch_geometric.compile(bgae_model)\n",
    "\n",
    "print(\"\\nBipartite GAE Model Instantiated:\")\n",
    "print(bgae_model)\n",
    "\n",
    "# --- Optimizer Definition ---\n",
    "learning_rate = 5e-4 \n",
    "weight_decay = 1e-5  \n",
    "optimizer = torch.optim.Adam(bgae_model.parameters(), lr=learning_rate, weight_decay=weight_decay)\n",
    "\n",
    "print(\"\\nOptimizer Defined.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.train_utils import *"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define training hyperparameters ---\n",
    "num_epochs = 200      # Use the intended number of epochs\n",
    "lambda_attr = 0.5      # Weight for attribute loss (both train and val scoring)\n",
    "lambda_struct = 0.5    # Weight for structure loss (both train and val scoring)\n",
    "k_neg_train = 5        # Negative samples for TRAINING loss calculation\n",
    "val_k_neg_score = 0    # Negative samples for VALIDATION scoring (set to 0 based on user's val_k_neg)\n",
    "eval_freq = 10         # Evaluate on validation set every 10 epochs\n",
    "target_edge_type = ('provider', 'to', 'member') # Primary edge type\n",
    "node_k_list_eval = [50, 100, 200] # K values for node P@K/R@K calculation during validation\n",
    "\n",
    "# --- Define Early Stopping & Saving Parameters ---\n",
    "early_stop_metric_choice = 'AP'         # Metric to monitor ('AUROC', 'AP', 'Best F1')\n",
    "early_stop_element_choice = 'Avg AP'  # Element to monitor ('provider', 'member', or str(target_edge_type))\n",
    "early_stop_patience_checks = 50         # How many validation CHECKS to wait for improvement\n",
    "best_model_save_path = \"./saved_models/best_bgae_model.pth\" # CORRECTED: Full file path\n",
    "\n",
    "\n",
    "print(f\"\\n--- Starting Training Run ---\")\n",
    "print(f\"Epochs: {num_epochs}, Val Freq: {eval_freq}, Patience: {early_stop_patience_checks} checks\")\n",
    "print(f\"Early Stopping: Monitor '{early_stop_element_choice}' '{early_stop_metric_choice}'\")\n",
    "print(f\"Saving best model to: {best_model_save_path}\")\n",
    "\n",
    "# Call the enhanced training function\n",
    "trained_model, history, saved_best_model_filepath = train_model_inductive_with_metrics(\n",
    "    model=bgae_model,\n",
    "    train_graph=train_g_on_device,\n",
    "    num_epochs=num_epochs,\n",
    "    optimizer=optimizer,\n",
    "    lambda_attr=lambda_attr,             # Used for training loss\n",
    "    lambda_struct=lambda_struct,           # Used for training loss\n",
    "    k_neg_samples=k_neg_train,           # Negative samples during training loss calc\n",
    "    target_edge_type=target_edge_type,\n",
    "    device=device,\n",
    "    log_freq=eval_freq,                 # How often to print logs\n",
    "\n",
    "    # --- Validation Arguments ---\n",
    "    val_graph=val_g_on_device,\n",
    "    gt_node_labels_val=GT_NODE_LABELS[\"val\"], # Pass node labels for validation metrics\n",
    "    gt_edge_labels_val=GT_EDGE_LABELS[\"val\"], # Pass edge labels for validation metrics\n",
    "    val_log_freq=eval_freq,              # How often validation logic runs\n",
    "\n",
    "    # --- Validation Scoring Arguments ---\n",
    "    val_lambda_attr=lambda_attr,         # Use same lambda for consistency in validation scoring\n",
    "    val_lambda_struct=lambda_struct,       # Use same lambda for consistency in validation scoring\n",
    "    val_k_neg_samples_score=val_k_neg_score, # K for negative sampling during validation *score* calc (set to 0 by user)\n",
    "\n",
    "    # --- Node Metrics Argument ---\n",
    "    node_k_list=node_k_list_eval,        # K values for P@K, R@K calculation\n",
    "\n",
    "    # --- Early Stopping Arguments ---\n",
    "    early_stopping_metric=early_stop_metric_choice,\n",
    "    early_stopping_element=early_stop_element_choice,\n",
    "    patience=early_stop_patience_checks,\n",
    "    save_best_model_path=best_model_save_path # Pass the file path\n",
    ")\n",
    "\n",
    "print(\"\\n--- Training Call Finished ---\")\n",
    "if saved_best_model_filepath:\n",
    "    print(f\"Best model artifact saved at: {saved_best_model_filepath}\")\n",
    "    # The 'trained_model' variable returned should now hold the state of the best model.\n",
    "else:\n",
    "    print(\"No best model was saved (check logs for early stopping/improvement status).\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from typing import Dict, Optional, Tuple, List\n",
    "import os\n",
    "\n",
    "# Ensure necessary functions are available (if using PR curves later)\n",
    "# from sklearn.metrics import average_precision_score, precision_recall_curve, PrecisionRecallDisplay\n",
    "\n",
    "# Set a visually appealing style for the plots\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"deep\", font_scale=1.1)\n",
    "\n",
    "# Define check_val_data helper function (outside main function)\n",
    "def check_val_data(history: Dict, key: str, val_epochs: List) -> bool:\n",
    "    \"\"\"Checks if validation data for a given key exists and matches epoch length.\"\"\"\n",
    "    data = history.get(key, [])\n",
    "    # Ensure data exists, is iterable, not empty, and matches val_epochs length\n",
    "    return data is not None and hasattr(data, '__len__') and len(data) > 0 and len(val_epochs) > 0 and len(data) == len(val_epochs)\n",
    "\n",
    "# --- Main Plotting Function ---\n",
    "def plot_training_validation_performance_report_combined_loss(\n",
    "    history: Dict,\n",
    "    # lambda_attr: float, # REMOVED - No longer needed\n",
    "    # lambda_struct: float, # REMOVED - No longer needed\n",
    "    # val_lambda_attr: float, # REMOVED - No longer needed\n",
    "    # val_lambda_struct: float, # REMOVED - No longer needed\n",
    "    target_edge_type: tuple = ('provider', 'to', 'member'),\n",
    "    figsize_loss: Tuple[int, int] = (10, 6),\n",
    "    figsize_metrics: Tuple[int, int] = (10, 12),\n",
    "    save_dir: Optional[str] = None\n",
    "    ):\n",
    "    \"\"\"\n",
    "    Generates publication-quality plots for training/validation loss (combined components)\n",
    "    and validation metrics (separate plots per element type).\n",
    "    \"\"\"\n",
    "    print(\"Generating Report Plots (Combined Losses and Validation Metrics)...\")\n",
    "\n",
    "    # --- Data Extraction and Validation ---\n",
    "    train_epochs = np.arange(1, len(history.get('train_loss', [])) + 1)\n",
    "    val_epochs_raw = history.get('epochs_validated', [])\n",
    "    val_epochs = list(val_epochs_raw) if isinstance(val_epochs_raw, np.ndarray) else val_epochs_raw\n",
    "    best_epoch = history.get('best_epoch', -1)\n",
    "\n",
    "    # --- Loss Plotting Section (Already Modified) ---\n",
    "\n",
    "    # Check which loss components exist\n",
    "    has_train_attr = history.get('train_loss_attr') is not None and len(history.get('train_loss_attr', [])) > 0\n",
    "    has_train_struct = history.get('train_loss_struct') is not None and len(history.get('train_loss_struct', [])) > 0\n",
    "    has_val_attr = check_val_data(history, 'val_loss_attr', val_epochs) # Use helper\n",
    "    has_val_struct = check_val_data(history, 'val_loss_struct', val_epochs) # Use helper\n",
    "\n",
    "    has_any_components = has_train_attr or has_train_struct or has_val_attr or has_val_struct\n",
    "    has_total_loss = (history.get('train_loss') is not None and len(history.get('train_loss', [])) > 0) or \\\n",
    "                     check_val_data(history, 'val_loss', val_epochs) # Use helper\n",
    "\n",
    "    num_plots_loss = (1 if has_total_loss else 0) + (1 if has_any_components else 0)\n",
    "\n",
    "    if num_plots_loss > 0:\n",
    "        fig_height = figsize_loss[1] if num_plots_loss == 2 else figsize_loss[1] * 0.7 # Adjusted height factor\n",
    "        fig_loss, axes_loss = plt.subplots(num_plots_loss, 1, figsize=(figsize_loss[0], fig_height), sharex=True, squeeze=False)\n",
    "        axes_loss = axes_loss.flatten()\n",
    "\n",
    "        fig_loss.suptitle('BGAE Model Loss Trajectories', fontsize=16, y=1.02)\n",
    "        plot_idx_loss = 0\n",
    "        grid_style = {'linestyle': ':', 'alpha': 0.7}\n",
    "        line_width = 1.8\n",
    "\n",
    "        # Plot 1: Total Loss\n",
    "        if has_total_loss:\n",
    "            ax = axes_loss[plot_idx_loss]\n",
    "            plotted_total = False\n",
    "            if history.get('train_loss') is not None and len(history.get('train_loss', [])) > 0:\n",
    "                ax.plot(train_epochs, history['train_loss'], label='Train Total Loss', color='royalblue', linewidth=line_width, alpha=0.9)\n",
    "                plotted_total = True\n",
    "            if check_val_data(history, 'val_loss', val_epochs): # Use helper\n",
    "                ax.plot(val_epochs, history['val_loss'], label='Validation Total Loss', color='darkorange', marker='.', linestyle='--', linewidth=line_width*0.9, markersize=5)\n",
    "                plotted_total = True\n",
    "\n",
    "            if plotted_total:\n",
    "                ax.set_ylabel('Total Loss')\n",
    "                # Add title only if it's not the only plot (to avoid redundancy with suptitle)\n",
    "                if num_plots_loss > 1: ax.set_title('Total Training and Validation Loss')\n",
    "                if best_epoch != -1:\n",
    "                    best_epoch_label = f'Best Epoch ({best_epoch})' if plot_idx_loss == 0 else \"_nolegend_\"\n",
    "                    ax.axvline(x=best_epoch, color='crimson', linestyle=':', linewidth=1.5, label=best_epoch_label)\n",
    "                ax.legend(loc='best')\n",
    "                ax.grid(True, **grid_style)\n",
    "            else: ax.set_visible(False)\n",
    "            plot_idx_loss += 1\n",
    "\n",
    "        # Plot 2: Combined Loss Components\n",
    "        if has_any_components:\n",
    "            ax = axes_loss[plot_idx_loss]\n",
    "            plotted_component = False\n",
    "            if has_train_attr:\n",
    "                ax.plot(train_epochs, history['train_loss_attr'], label='Train Attr Comp.', color='forestgreen', linestyle='-', linewidth=line_width*0.8, alpha=0.9)\n",
    "                plotted_component = True\n",
    "            if has_train_struct:\n",
    "                ax.plot(train_epochs, history['train_loss_struct'], label='Train Struct Comp.', color='mediumpurple', linestyle='--', linewidth=line_width*0.8, alpha=0.9)\n",
    "                plotted_component = True\n",
    "            if has_val_attr:\n",
    "                ax.plot(val_epochs, history['val_loss_attr'], label='Val Attr Comp.', color='limegreen', linestyle='-', marker='x', markersize=5, linewidth=line_width*0.7, alpha=0.8)\n",
    "                plotted_component = True\n",
    "            if has_val_struct:\n",
    "                ax.plot(val_epochs, history['val_loss_struct'], label='Val Struct Comp.', color='blueviolet', linestyle='--', marker='+', markersize=6, linewidth=line_width*0.7, alpha=0.8)\n",
    "                plotted_component = True\n",
    "\n",
    "            if plotted_component:\n",
    "                 ax.set_ylabel('Loss Component Value')\n",
    "                 # Add title only if it's not the only plot\n",
    "                 if num_plots_loss > 1: ax.set_title('Loss Components (Pre-Lambda Weights)')\n",
    "                 if best_epoch != -1:\n",
    "                     ax.axvline(x=best_epoch, color='crimson', linestyle=':', linewidth=1.5, label=\"_nolegend_\")\n",
    "                 ax.legend(loc='best')\n",
    "                 ax.grid(True, **grid_style)\n",
    "                 ax.set_xlabel('Epoch') # Add X label only to the last plot\n",
    "            else: ax.set_visible(False)\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.95])\n",
    "\n",
    "        if save_dir:\n",
    "            try:\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                save_loss_path = os.path.join(save_dir, 'report_loss_curves_combined.png')\n",
    "                fig_loss.savefig(save_loss_path, dpi=300, bbox_inches='tight')\n",
    "                print(f\"Combined loss plot saved to {save_loss_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving combined loss plot: {e}\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(\"No loss data found to plot.\")\n",
    "\n",
    "\n",
    "    # --- Figure 2: Validation AP Metrics (Single Plot with Average) ---\n",
    "    print(\"--- Validation AP Metrics Plotting ---\")\n",
    "    metric_to_plot = 'AP'\n",
    "    element_types = ['provider', 'member', 'edge']\n",
    "    num_plots_metrics = 1\n",
    "\n",
    "    has_any_val_ap = any(\n",
    "        check_val_data(history, f'val_{elem}_{metric_to_plot}', val_epochs)\n",
    "        for elem in element_types\n",
    "    )\n",
    "\n",
    "    if has_any_val_ap and len(val_epochs) > 0:\n",
    "        fig_metrics, ax = plt.subplots(1, 1, figsize=figsize_metrics)\n",
    "\n",
    "        # --- Set ONLY the main figure title ---\n",
    "        fig_metrics.suptitle(f'BGAE Model Validation {metric_to_plot} vs. Epoch', fontsize=16, y=1.0) # Adjusted y slightly higher maybe\n",
    "\n",
    "        element_colors = {'provider': 'mediumblue', 'member': 'darkgreen', 'edge': 'firebrick', 'average': 'black'} # Changed edge color slightly\n",
    "        element_markers = {'provider': '.', 'member': '^', 'edge': 's', 'average': ''}\n",
    "        element_linestyles = {'provider': '-', 'member': '--', 'edge': ':', 'average': '-.'}\n",
    "        marker_size = 5\n",
    "        line_width_metrics = 1.7\n",
    "        best_epoch_style = {'color': 'crimson', 'linestyle': ':', 'linewidth': 1.5}\n",
    "        # grid_style defined above is reused\n",
    "\n",
    "        ap_values_for_avg = []\n",
    "        elements_with_ap = []\n",
    "        for element_type in element_types:\n",
    "             key = f'val_{element_type}_{metric_to_plot}'\n",
    "             if check_val_data(history, key, val_epochs):\n",
    "                 ap_values_for_avg.append(np.array(history[key]))\n",
    "                 elements_with_ap.append(element_type)\n",
    "        average_ap = []\n",
    "        if ap_values_for_avg:\n",
    "            stacked_ap = np.vstack(ap_values_for_avg)\n",
    "            with warnings.catch_warnings():\n",
    "                 warnings.simplefilter(\"ignore\", category=RuntimeWarning)\n",
    "                 average_ap = np.nanmean(stacked_ap, axis=0)\n",
    "\n",
    "        plotted_anything_on_axis = False\n",
    "        for element_type in element_types:\n",
    "            key = f'val_{element_type}_{metric_to_plot}'\n",
    "            if check_val_data(history, key, val_epochs):\n",
    "                label = 'Edge' if element_type == 'edge' else f'{element_type.capitalize()} Node'\n",
    "                ax.plot(val_epochs, history[key], label=label,\n",
    "                        color=element_colors[element_type], marker=element_markers[element_type],\n",
    "                        linestyle=element_linestyles[element_type], markersize=marker_size,\n",
    "                        linewidth=line_width_metrics)\n",
    "                plotted_anything_on_axis = True\n",
    "\n",
    "        if len(average_ap) > 0:\n",
    "             ax.plot(val_epochs, average_ap, label='Average AP',\n",
    "                     color=element_colors['average'], linestyle=element_linestyles['average'],\n",
    "                     linewidth=line_width_metrics + 0.3)\n",
    "             plotted_anything_on_axis = True\n",
    "\n",
    "        # Configure the subplot axes and grid\n",
    "        # *** REMOVED SUBPLOT TITLE ***\n",
    "        # ax.set_title(f'Validation {metric_to_plot} Metric by Element Type')\n",
    "        ax.set_ylabel(f'{metric_to_plot} Score')\n",
    "        ax.set_xlabel('Epoch')\n",
    "        ax.set_ylim(bottom=-0.05, top=0.7)\n",
    "        ax.grid(True, **grid_style)\n",
    "\n",
    "        if plotted_anything_on_axis:\n",
    "             if best_epoch != -1:\n",
    "                 ax.axvline(x=best_epoch, label=f'Best Epoch ({best_epoch})', **best_epoch_style)\n",
    "             # *** ADDED fontsize parameter to legend ***\n",
    "             ax.legend(title='Element Type', loc='best', fontsize='small') # Or 'x-small', 8, etc.\n",
    "\n",
    "        plt.tight_layout(rect=[0, 0.03, 1, 0.96]) # Adjust rect to fit suptitle\n",
    "\n",
    "        if save_dir:\n",
    "            try:\n",
    "                os.makedirs(save_dir, exist_ok=True)\n",
    "                save_metrics_path = os.path.join(save_dir, f'report_validation_{metric_to_plot}_metric.png')\n",
    "                fig_metrics.savefig(save_metrics_path, dpi=300, bbox_inches='tight')\n",
    "                print(f\"AP metrics plot saved to {save_metrics_path}\")\n",
    "            except Exception as e:\n",
    "                print(f\"Error saving {metric_to_plot} metrics plot: {e}\")\n",
    "        plt.show()\n",
    "    else:\n",
    "        print(f\"No validation {metric_to_plot} data (or validation epochs) found to plot.\")\n",
    "\n",
    "\n",
    "\n",
    "# Create the example directory if it doesn't exist\n",
    "save_directory = \"report_plots_example\"\n",
    "if not os.path.exists(save_directory):\n",
    "    os.makedirs(save_directory)\n",
    "\n",
    "# Call the plotting function\n",
    "plot_training_validation_performance_report_combined_loss(\n",
    "     history=history,\n",
    "     target_edge_type=('provider', 'to', 'member'),\n",
    "     # *** Corrected save_dir to be a directory ***\n",
    "     figsize_metrics=(10, 4),\n",
    "     save_dir=save_directory\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hyperparameter_tuning(\n",
    "    num_trials: int,\n",
    "    param_space: dict,\n",
    "    train_g_dev: HeteroData,\n",
    "    val_g_dev: HeteroData,\n",
    "    gt_node_labels_val_dev: dict,\n",
    "    gt_edge_labels_val_dev: dict,\n",
    "    in_dim_member: int,\n",
    "    in_dim_provider: int,\n",
    "    edge_dim: int,\n",
    "    device: str,\n",
    "    # --- Assume BipartiteGraphAutoEncoder_ReportBased class is available ---\n",
    "    num_epochs_per_trial: int = 200,\n",
    "    early_stopping_metric: str = 'AP',\n",
    "    early_stopping_element: str = 'Avg AP',\n",
    "    patience: int = 10,\n",
    "    val_freq: int = 10,\n",
    "    target_edge: tuple = ('provider', 'to', 'member')\n",
    "    ) -> Tuple[Optional[dict], pd.DataFrame]: # Return type includes Optional for best_params\n",
    "    \"\"\"\n",
    "    Performs hyperparameter tuning using either random search or exhaustive\n",
    "    grid search if the total number of valid combinations is less than num_trials.\n",
    "\n",
    "    Args:\n",
    "        \n",
    "\n",
    "    Returns:\n",
    "        Tuple[Optional[dict], pd.DataFrame]: Best parameters found (or None) and DataFrame of all trial results.\n",
    "    \"\"\"\n",
    "    tuning_results_list = []\n",
    "    best_overall_metric = -np.inf\n",
    "    best_overall_params = None\n",
    "\n",
    "    # --- 1. Generate and Filter All Valid Combinations ---\n",
    "    print(\"--- Calculating All Valid Hyperparameter Combinations ---\")\n",
    "    param_names = list(param_space.keys())\n",
    "    param_value_lists = [param_space[name] for name in param_names]\n",
    "\n",
    "    all_possible_combinations = itertools.product(*param_value_lists)\n",
    "    valid_combinations = []\n",
    "\n",
    "    for combo_values in all_possible_combinations:\n",
    "        params = dict(zip(param_names, combo_values))\n",
    "\n",
    "        # Apply Constraints\n",
    "        # a) lambda_attr + lambda_struct = 1\n",
    "        #    (This is handled by how 'lambda_struct_only' is used below)\n",
    "        # b) latent_dim <= hidden_dim\n",
    "        if params['latent_dim'] > params['hidden_dim']:\n",
    "            continue # Skip invalid combination\n",
    "        # c) num_dec_layers = num_conv_layers (for symmetry)\n",
    "        params['num_dec_layers'] = params['num_conv_layers']\n",
    "\n",
    "        # d) Handle lambda derivation\n",
    "        lambda_struct_sampled = params.pop('lambda_struct_only') # Remove temp key\n",
    "        params['lambda_struct'] = lambda_struct_sampled          # Add final key\n",
    "        # No need to store lambda_attr explicitly in params, it's derived during use\n",
    "\n",
    "        valid_combinations.append(params)\n",
    "\n",
    "    total_valid_combinations = len(valid_combinations)\n",
    "    print(f\"Total valid combinations found: {total_valid_combinations}\")\n",
    "\n",
    "    # --- 2. Decide Search Strategy and Prepare Trials ---\n",
    "    if total_valid_combinations <= num_trials:\n",
    "        print(f\"\\n--- Starting EXHAUSTIVE Grid Search ({total_valid_combinations} Trials) ---\")\n",
    "        search_mode = \"Exhaustive\"\n",
    "        trials_to_run = valid_combinations # List of parameter dicts\n",
    "        actual_num_trials = total_valid_combinations\n",
    "    else:\n",
    "        print(f\"\\n--- Starting RANDOM Search ({num_trials} Trials out of {total_valid_combinations} possible) ---\")\n",
    "        search_mode = \"Random\"\n",
    "        trials_to_run = range(num_trials) # Just use the number for loop iterations\n",
    "        actual_num_trials = num_trials\n",
    "\n",
    "    print(f\"  Objective: Maximize Validation '{early_stopping_element}' '{early_stopping_metric}'\")\n",
    "\n",
    "    # --- 3. Run Tuning Loop ---\n",
    "    for i, trial_input in enumerate(trials_to_run):\n",
    "        trial_num = i + 1\n",
    "        print(f\"\\n--- Trial {trial_num}/{actual_num_trials} ({search_mode}) ---\")\n",
    "\n",
    "        # a) Get/Sample Hyperparameters for this trial\n",
    "        if search_mode == \"Exhaustive\":\n",
    "            params = trial_input # Directly use the dict from the list\n",
    "        else: # Random Search\n",
    "            # Sample randomly (could reuse the generation/filtering logic,\n",
    "            # but sampling directly is simpler for pure random)\n",
    "            while True: # Keep sampling until a valid combo is found\n",
    "                params = {name: random.choice(values) for name, values in param_space.items()}\n",
    "                if params['latent_dim'] <= params['hidden_dim']:\n",
    "                    # Apply lambda constraint and symmetry\n",
    "                    lambda_struct_sampled = params.pop('lambda_struct_only')\n",
    "                    params['lambda_struct'] = lambda_struct_sampled\n",
    "                    params['num_dec_layers'] = params['num_conv_layers']\n",
    "                    break # Valid combo found\n",
    "\n",
    "        # Calculate derived lambda_attr for use\n",
    "        lambda_attr_calc = 1.0 - params['lambda_struct']\n",
    "        print(f\"  Params: {params} (Derived lambda_attr = {lambda_attr_calc:.2f})\")\n",
    "\n",
    "        # b) Instantiate Model and Optimizer\n",
    "        try:\n",
    "            model = BipartiteGraphAutoEncoder_ReportBased( # Use the passed model class\n",
    "                in_dim_member=in_dim_member,\n",
    "                in_dim_provider=in_dim_provider,\n",
    "                edge_dim=edge_dim,\n",
    "                hidden_dim=params['hidden_dim'],\n",
    "                latent_dim=params['latent_dim'],\n",
    "                num_conv_layers=params['num_conv_layers'],\n",
    "                num_dec_layers=params['num_dec_layers'],\n",
    "                dropout=params['dropout']\n",
    "            ).to(device)\n",
    "        except Exception as e:\n",
    "             print(f\"ERROR: Failed to instantiate model '{model_class.__name__}': {e}. Skipping trial.\")\n",
    "             continue\n",
    "\n",
    "        # --- IMPORTANT: DO NOT USE torch.compile here due to previous errors ---\n",
    "        # If compile is needed, debug it separately first.\n",
    "        # model = torch.compile(model)\n",
    "        # -------------------------------------------------------------------\n",
    "\n",
    "        optimizer = torch.optim.Adam(\n",
    "            model.parameters(),\n",
    "            lr=params['learning_rate'],\n",
    "            weight_decay=params.get('weight_decay', 1e-5) # Assuming not tuned here\n",
    "        )\n",
    "\n",
    "        # c) Train Model\n",
    "        print(f\"  Starting training for trial {trial_num}...\")\n",
    "        try:\n",
    "            # Make sure the training function is available\n",
    "            _, history, _ = train_model_inductive_with_metrics(\n",
    "                model=model,\n",
    "                train_graph=train_g_dev,\n",
    "                num_epochs=num_epochs_per_trial,\n",
    "                optimizer=optimizer,\n",
    "                lambda_attr=lambda_attr_calc, # Use calculated attr lambda for train loss\n",
    "                lambda_struct=params['lambda_struct'], # Use sampled struct lambda for train loss\n",
    "                k_neg_samples=5,\n",
    "                target_edge_type=target_edge,\n",
    "                device=device,\n",
    "                log_freq=val_freq * 5, # Log even less often during tuning\n",
    "                val_graph=val_g_dev,\n",
    "                gt_node_labels_val=gt_node_labels_val_dev,\n",
    "                gt_edge_labels_val=gt_edge_labels_val_dev,\n",
    "                val_log_freq=val_freq,\n",
    "                val_lambda_attr=lambda_attr_calc, # Use same derived lambdas for validation loss\n",
    "                val_lambda_struct=params['lambda_struct'],\n",
    "                val_k_neg_samples_score=1,\n",
    "                node_k_list=[50, 100, 200],\n",
    "                early_stopping_metric=early_stopping_metric,\n",
    "                early_stopping_element=early_stopping_element,\n",
    "                patience=patience,\n",
    "                save_best_model_path=None # No saving during tuning\n",
    "            )\n",
    "        except Exception as e:\n",
    "             print(f\"ERROR during training for trial {trial_num}: {e}\")\n",
    "             history = {'best_val_metric_value': -np.inf, 'best_epoch': -1}\n",
    "\n",
    "\n",
    "        # d) Record Results\n",
    "        best_metric_in_trial = history.get('best_val_metric_value', -np.inf)\n",
    "        best_epoch_in_trial = history.get('best_epoch', -1)\n",
    "        print(f\"  Trial {trial_num} Result: Best Val '{early_stopping_element}' '{early_stopping_metric}' = {best_metric_in_trial:.4f} (at epoch {best_epoch_in_trial})\")\n",
    "\n",
    "        trial_data = {'trial': trial_num, 'search_mode': search_mode}\n",
    "        trial_data.update(params) # Store hyperparameters used\n",
    "        trial_data['best_val_metric'] = best_metric_in_trial\n",
    "        trial_data['best_epoch'] = best_epoch_in_trial\n",
    "        tuning_results_list.append(trial_data)\n",
    "\n",
    "        # Update overall best\n",
    "        # Use >= to favor later trials slightly in case of exact ties\n",
    "        if best_metric_in_trial >= best_overall_metric:\n",
    "            # Add small tolerance check to avoid fluctuations due to floating point noise\n",
    "            if best_metric_in_trial > best_overall_metric + 1e-7:\n",
    "                 print(f\"  ** New Best Overall Found! **\")\n",
    "            best_overall_metric = best_metric_in_trial\n",
    "            # Need to store the params dict *before* lambda_struct_only was popped\n",
    "            best_overall_params = next(\n",
    "                 (p for p in valid_combinations if p == params), # Find the original dict in valid_combinations\n",
    "                 params # Fallback just in case (shouldn't happen)\n",
    "            )\n",
    "\n",
    "    print(\"\\n--- Hyperparameter Tuning Finished ---\")\n",
    "\n",
    "    # 4. Report Final Results\n",
    "    results_df = pd.DataFrame(tuning_results_list)\n",
    "\n",
    "    if best_overall_params:\n",
    "        print(\"\\n--- Best Hyperparameters Found ---\")\n",
    "        print(f\"  Search Mode Run: {search_mode}\")\n",
    "        print(f\"  Best Validation '{early_stopping_element}' '{early_stopping_metric}': {best_overall_metric:.4f}\")\n",
    "        print(\"  Optimal Parameters:\")\n",
    "        # Recalculate the derived lambda_attr for the final print\n",
    "        best_lambda_struct = best_overall_params['lambda_struct']\n",
    "        best_lambda_attr = 1.0 - best_lambda_struct\n",
    "        print(f\"    lambda_attr: {best_lambda_attr:.2f} (derived from constraint)\")\n",
    "        # Print other params from the best dict\n",
    "        for param, value in best_overall_params.items():\n",
    "             print(f\"    {param}: {value}\")\n",
    "\n",
    "        # Add derived lambda_attr to the best_overall_params for completeness if needed later\n",
    "        best_overall_params_final = best_overall_params.copy()\n",
    "        best_overall_params_final['lambda_attr'] = best_lambda_attr\n",
    "\n",
    "    else:\n",
    "        best_overall_params_final = None # Ensure None is returned if no best params found\n",
    "        print(\"No successful trials completed or no improvement found.\")\n",
    "\n",
    "    print(\"\\n--- Tuning Results Summary (Top 5 by best_val_metric) ---\")\n",
    "    print(results_df.sort_values(by='best_val_metric', ascending=False).head().to_string()) # Use to_string for better console format\n",
    "\n",
    "    return best_overall_params_final, results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Search Space ---\n",
    "# Reduced for quick example run - expand as needed\n",
    "tuning_param_space = {\n",
    "    'hidden_dim': [32, 128],\n",
    "    'latent_dim': [8, 32], # Will be capped by hidden_dim\n",
    "    'num_conv_layers': [1, 3],\n",
    "    'learning_rate': [5e-4, 1e-3],\n",
    "    'dropout': [0.3, 0.7],\n",
    "    'lambda_struct_only': [0.25, 0.75] # Tune this, attr = 1 - this\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "best_params, tuning_df = run_hyperparameter_tuning_repeated(\n",
    "    num_trials=10, \n",
    "    n_iter=3,\n",
    "    param_space=tuning_param_space,\n",
    "    train_g_dev=train_g_on_device,\n",
    "    val_g_dev=val_g_on_device,\n",
    "    gt_node_labels_val_dev=GT_NODE_LABELS[\"val\"],\n",
    "    gt_edge_labels_val_dev=GT_EDGE_LABELS[\"val\"], # Pass edge labels\n",
    "    in_dim_member=in_dim_member,\n",
    "    in_dim_provider=in_dim_provider,\n",
    "    edge_dim=edge_dim,\n",
    "    device=device,\n",
    "    num_epochs_per_trial=200, # Low epochs for quick example\n",
    "    early_stopping_metric='AP',\n",
    "    early_stopping_element='Avg AP', # Monitor average AP\n",
    "    patience=15,  \n",
    "    val_freq=10\n",
    ")\n",
    "\n",
    "print(\"\\n--- Final Best Parameters from Tuning ---\")\n",
    "print(best_params)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 154,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_hyperparameter_tuning_repeated(\n",
    "    # --- Inputs ---\n",
    "    num_trials: int,\n",
    "    param_space: dict,\n",
    "    train_g_dev: HeteroData,\n",
    "    val_g_dev: HeteroData,\n",
    "    gt_node_labels_val_dev: dict,\n",
    "    gt_edge_labels_val_dev: dict,\n",
    "    in_dim_member: int,\n",
    "    in_dim_provider: int,\n",
    "    edge_dim: int,\n",
    "    device: str,\n",
    "    # --- NEW: Repetition Parameter ---\n",
    "    n_iter: int = 1, # Number of times to repeat each trial\n",
    "    # --- Fixed Training/Eval Parameters ---\n",
    "    num_epochs_per_trial: int = 200,\n",
    "    early_stopping_metric: str = 'AP',\n",
    "    early_stopping_element: str = 'Avg AP',\n",
    "    patience: int = 10,\n",
    "    val_freq: int = 10,\n",
    "    k_neg_train: int = 5,\n",
    "    k_neg_val_score: int = 1,\n",
    "    target_edge: tuple = ('provider', 'to', 'member')\n",
    "    ) -> Tuple[Optional[dict], pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Performs hyperparameter tuning (random or exhaustive) by repeating each\n",
    "    trial `n_iter` times to estimate mean performance and confidence.\n",
    "\n",
    "    Args:\n",
    "        model_class: The GNN model class to instantiate (e.g., BipartiteGraphAutoEncoder_ReportBased).\n",
    "        n_iter (int): Number of repetitions for each hyperparameter combination.\n",
    "        (Other args similar to previous function)\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Optional[dict], pd.DataFrame]:\n",
    "            - Best parameters found based on mean performance (or None).\n",
    "            - DataFrame summarizing all trials, including mean, std dev, and CI of the validation metric.\n",
    "    \"\"\"\n",
    "    tuning_results_list = [] # Stores aggregated results per parameter combo\n",
    "    best_overall_mean_metric = -np.inf\n",
    "    best_overall_params = None\n",
    "    # --- Input Validation ---\n",
    "    if not isinstance(n_iter, int) or n_iter < 1:\n",
    "        print(\"Warning: n_iter must be a positive integer. Setting n_iter=1.\")\n",
    "        n_iter = 1\n",
    "\n",
    "    # --- 1. Generate and Filter All Valid Combinations ---\n",
    "    print(\"--- Calculating All Valid Hyperparameter Combinations ---\")\n",
    "    param_names = list(param_space.keys())\n",
    "    param_value_lists = [param_space[name] for name in param_names]\n",
    "    all_possible_combinations = itertools.product(*param_value_lists)\n",
    "    valid_combinations = []\n",
    "\n",
    "    for combo_values in all_possible_combinations:\n",
    "        params = dict(zip(param_names, combo_values))\n",
    "        if params.get('latent_dim', 0) > params.get('hidden_dim', float('inf')): continue\n",
    "        # Add other constraints if necessary\n",
    "        # We handle lambda and layer symmetry within the loop now\n",
    "        valid_combinations.append(params) # Store original sampled dict with lambda_struct_only\n",
    "\n",
    "    total_valid_combinations = len(valid_combinations)\n",
    "    print(f\"Total valid combinations found (before constraints): {total_valid_combinations}\") # Note: size constraint checked later\n",
    "\n",
    "    # --- 2. Decide Search Strategy and Prepare Trials ---\n",
    "    if total_valid_combinations <= num_trials:\n",
    "        print(f\"\\n--- Starting EXHAUSTIVE Grid Search ({total_valid_combinations} combinations, {n_iter} runs each) ---\")\n",
    "        search_mode = \"Exhaustive\"\n",
    "        # Use the pre-generated combinations (still need to filter based on constraints inside loop)\n",
    "        param_combinations_to_run = list(itertools.product(*param_value_lists))\n",
    "        actual_num_unique_configs = total_valid_combinations # May be lower after filtering\n",
    "    else:\n",
    "        print(f\"\\n--- Starting RANDOM Search ({num_trials} combinations, {n_iter} runs each, from {total_valid_combinations} possible) ---\")\n",
    "        search_mode = \"Random\"\n",
    "        param_combinations_to_run = range(num_trials) # Generate random ones inside the loop\n",
    "        actual_num_unique_configs = num_trials\n",
    "\n",
    "    print(f\"  Objective: Maximize Mean Validation '{early_stopping_element}' '{early_stopping_metric}' over {n_iter} runs\")\n",
    "\n",
    "    # --- 3. Run Tuning Loop (Iterating through Unique Configurations) ---\n",
    "    processed_configs = set() # To avoid re-running same config in random search if sampled twice\n",
    "\n",
    "    for i in range(actual_num_unique_configs):\n",
    "        # --- a) Get/Sample Parameters for this Configuration ---\n",
    "        if search_mode == \"Exhaustive\":\n",
    "            # Get the i-th potential combo and check constraints\n",
    "            combo_values = param_combinations_to_run[i]\n",
    "            params_sampled = dict(zip(param_names, combo_values))\n",
    "            # Check constraints\n",
    "            if params_sampled.get('latent_dim', 0) > params_sampled.get('hidden_dim', float('inf')):\n",
    "                continue # Skip this invalid combination\n",
    "        else: # Random Search\n",
    "            attempts = 0\n",
    "            while attempts < 100: # Prevent infinite loop\n",
    "                params_sampled = {name: random.choice(values) for name, values in param_space.items()}\n",
    "                # Check constraints\n",
    "                if params_sampled.get('latent_dim', 0) <= params_sampled.get('hidden_dim', float('inf')):\n",
    "                     # Avoid duplicates in random search\n",
    "                     params_tuple_key = tuple(sorted(params_sampled.items()))\n",
    "                     if params_tuple_key not in processed_configs:\n",
    "                         processed_configs.add(params_tuple_key)\n",
    "                         break # Valid and unique combo found\n",
    "                attempts += 1\n",
    "            if attempts == 100:\n",
    "                 print(f\"Warning: Could not find a unique valid random combo after 100 attempts for trial {i+1}. Skipping.\")\n",
    "                 continue\n",
    "\n",
    "\n",
    "        # Apply fixed constraints/derivations\n",
    "        params = params_sampled.copy() # Work with a copy\n",
    "        lambda_struct_sampled = params.pop('lambda_struct_only')\n",
    "        params['lambda_struct'] = lambda_struct_sampled\n",
    "        params['num_dec_layers'] = params['num_conv_layers']\n",
    "        lambda_attr_calc = 1.0 - params['lambda_struct']\n",
    "\n",
    "        print(f\"\\n--- Configuration {i + 1}/{actual_num_unique_configs} ({search_mode}) ---\")\n",
    "        print(f\"  Params: {params} (Derived lambda_attr = {lambda_attr_calc:.2f})\")\n",
    "        print(f\"  Running {n_iter} times...\")\n",
    "\n",
    "        run_metrics = [] # Store metrics for each run of this config\n",
    "        run_epochs = []  # Store best epochs for each run\n",
    "\n",
    "        # --- b) Inner Loop: Repeat n_iter times ---\n",
    "        for run in range(n_iter):\n",
    "            print(f\"    Run {run + 1}/{n_iter}: Starting training...\")\n",
    "            # **CRITICAL: Instantiate NEW model and optimizer for each run**\n",
    "            try:\n",
    "                # Set seeds for this specific run for reproducibility *within* the run\n",
    "                # This allows stochasticity between runs but consistency if a single run needs debugging\n",
    "                run_seed = i * n_iter + run # Simple way to get a unique seed per run\n",
    "                random.seed(run_seed)\n",
    "                np.random.seed(run_seed)\n",
    "                torch.manual_seed(run_seed)\n",
    "                if device == 'cuda': torch.cuda.manual_seed_all(run_seed)\n",
    "\n",
    "                model = BipartiteGraphAutoEncoder_ReportBased( # Use the passed model class\n",
    "                    in_dim_member=in_dim_member,\n",
    "                    in_dim_provider=in_dim_provider,\n",
    "                    edge_dim=edge_dim,\n",
    "                    hidden_dim=params['hidden_dim'],\n",
    "                    latent_dim=params['latent_dim'],\n",
    "                    num_conv_layers=params['num_conv_layers'],\n",
    "                    num_dec_layers=params['num_dec_layers'],\n",
    "                    dropout=params['dropout']\n",
    "                ).to(device)\n",
    "            except Exception as e:\n",
    "                 print(f\"    ERROR: Failed to instantiate model '{model_class.__name__}' on run {run+1}: {e}. Skipping run.\")\n",
    "                 run_metrics.append(np.nan) # Record failure\n",
    "                 run_epochs.append(-1)\n",
    "                 continue\n",
    "\n",
    "            optimizer = torch.optim.Adam(\n",
    "                model.parameters(),\n",
    "                lr=params['learning_rate'],\n",
    "                # weight_decay=params.get('weight_decay', 1e-5)\n",
    "            )\n",
    "\n",
    "            try:\n",
    "                _, history, _ = train_model_inductive_with_metrics(\n",
    "                    model=model,\n",
    "                    train_graph=train_g_dev,\n",
    "                    num_epochs=num_epochs_per_trial,\n",
    "                    optimizer=optimizer,\n",
    "                    lambda_attr=lambda_attr_calc,\n",
    "                    lambda_struct=params['lambda_struct'],\n",
    "                    k_neg_samples=k_neg_train,\n",
    "                    target_edge_type=target_edge,\n",
    "                    device=device,\n",
    "                    log_freq=9999, # Suppress logs within the inner loop usually\n",
    "                    val_graph=val_g_dev,\n",
    "                    gt_node_labels_val=gt_node_labels_val_dev,\n",
    "                    gt_edge_labels_val=gt_edge_labels_val_dev,\n",
    "                    val_log_freq=val_freq,\n",
    "                    val_lambda_attr=lambda_attr_calc,\n",
    "                    val_lambda_struct=params['lambda_struct'],\n",
    "                    val_k_neg_samples_score=k_neg_val_score,\n",
    "                    node_k_list=[50], # Only need base metrics for early stopping\n",
    "                    early_stopping_metric=early_stopping_metric,\n",
    "                    early_stopping_element=early_stopping_element,\n",
    "                    patience=patience,\n",
    "                    save_best_model_path=None # No saving during tuning\n",
    "                )\n",
    "                # Store the best metric achieved in this specific run\n",
    "                metric_value = history.get('best_val_metric_value', -np.inf)\n",
    "                run_metrics.append(metric_value if metric_value > -np.inf else np.nan) # Store NaN if no improvement\n",
    "                run_epochs.append(history.get('best_epoch', -1))\n",
    "\n",
    "            except Exception as e:\n",
    "                 print(f\"    ERROR during training for config {i+1}, run {run+1}: {e}\")\n",
    "                 run_metrics.append(np.nan) # Record failure\n",
    "                 run_epochs.append(-1)\n",
    "            finally:\n",
    "                 # Clean up GPU memory if applicable\n",
    "                 del model\n",
    "                 del optimizer\n",
    "                 if torch.cuda.is_available(): torch.cuda.empty_cache()\n",
    "                 elif torch.backends.mps.is_available(): torch.mps.empty_cache()\n",
    "\n",
    "\n",
    "        # --- c) Aggregate Results for this Configuration ---\n",
    "        valid_run_metrics = [m for m in run_metrics if not np.isnan(m)]\n",
    "        if valid_run_metrics:\n",
    "            mean_metric = np.mean(valid_run_metrics)\n",
    "            std_metric = np.std(valid_run_metrics) if len(valid_run_metrics) > 1 else 0.0\n",
    "            ci_radius = 1.96 * std_metric / np.sqrt(len(valid_run_metrics)) if len(valid_run_metrics) > 1 else 0.0\n",
    "            mean_best_epoch = np.mean([e for e in run_epochs if e != -1]) if any(e != -1 for e in run_epochs) else -1\n",
    "            n_successful_runs = len(valid_run_metrics)\n",
    "        else:\n",
    "            mean_metric = -np.inf\n",
    "            std_metric = np.nan\n",
    "            ci_radius = np.nan\n",
    "            mean_best_epoch = -1\n",
    "            n_successful_runs = 0\n",
    "\n",
    "        print(f\"  Config {i + 1} Aggregated Result ({n_successful_runs}/{n_iter} succ. runs): Mean '{early_stopping_element}' '{early_stopping_metric}' = {mean_metric:.4f} ± {std_metric:.4f}\")\n",
    "\n",
    "        # Store aggregated data\n",
    "        trial_data = {'config_idx': i + 1, 'search_mode': search_mode, 'n_successful_runs': n_successful_runs}\n",
    "        trial_data.update(params) # Store hyperparameters used\n",
    "        trial_data[f'mean_{early_stopping_element}_{early_stopping_metric}'] = mean_metric\n",
    "        trial_data[f'std_{early_stopping_element}_{early_stopping_metric}'] = std_metric\n",
    "        trial_data['ci_95_radius'] = ci_radius\n",
    "        trial_data['mean_best_epoch'] = mean_best_epoch\n",
    "        tuning_results_list.append(trial_data)\n",
    "\n",
    "        # Update overall best based on mean metric\n",
    "        if mean_metric >= best_overall_mean_metric:\n",
    "             # Add small tolerance check\n",
    "            if mean_metric > best_overall_mean_metric + 1e-7:\n",
    "                 print(f\"    ** New Best Overall Mean Metric Found! **\")\n",
    "            # Check std dev as tie-breaker (lower is better)\n",
    "            elif mean_metric == best_overall_mean_metric and std_metric < (tuning_results_list[np.argmax([t[f'mean_{early_stopping_element}_{early_stopping_metric}'] for t in tuning_results_list])][f'std_{early_stopping_element}_{early_stopping_metric}'] if best_overall_params else float('inf')):\n",
    "                  print(f\"    ** Same Mean Metric, Lower Std Dev Found! **\")\n",
    "            else: # Same mean, same or higher std dev -> keep existing best\n",
    "                  continue\n",
    "\n",
    "            best_overall_mean_metric = mean_metric\n",
    "            # Store the original sampled params before lambda derivation etc.\n",
    "            best_overall_params = params_sampled.copy() # Store the version with lambda_struct_only\n",
    "\n",
    "    print(\"\\n--- Hyperparameter Tuning Finished ---\")\n",
    "\n",
    "    # --- 4. Report Final Results ---\n",
    "    results_df = pd.DataFrame(tuning_results_list)\n",
    "    results_df = results_df.sort_values(by=f'mean_{early_stopping_element}_{early_stopping_metric}', ascending=False)\n",
    "\n",
    "\n",
    "    if best_overall_params:\n",
    "        print(\"\\n--- Best Hyperparameters Found ---\")\n",
    "        print(f\"  Search Mode Run: {search_mode}\")\n",
    "        print(f\"  Best Mean Validation '{early_stopping_element}' '{early_stopping_metric}': {best_overall_mean_metric:.4f}\")\n",
    "        best_row = results_df.iloc[0] # Get row corresponding to best mean metric\n",
    "        print(f\"  Std Dev: {best_row[f'std_{early_stopping_element}_{early_stopping_metric}']:.4f} | 95% CI Radius: {best_row['ci_95_radius']:.4f} (over {best_row['n_successful_runs']} runs)\")\n",
    "        print(\"  Optimal Parameters (sampled values):\")\n",
    "        # Apply constraints again to print final derived params\n",
    "        final_params_print = best_overall_params.copy()\n",
    "        final_lambda_struct = final_params_print.pop('lambda_struct_only')\n",
    "        final_params_print['lambda_struct'] = final_lambda_struct\n",
    "        final_params_print['num_dec_layers'] = final_params_print['num_conv_layers']\n",
    "        final_lambda_attr = 1.0 - final_lambda_struct\n",
    "        print(f\"    lambda_attr: {final_lambda_attr:.2f} (derived)\")\n",
    "        for param, value in final_params_print.items():\n",
    "             print(f\"    {param}: {value}\")\n",
    "        # Prepare the final parameter dict to return (matching structure of original best_overall_params)\n",
    "        final_best_params_to_return = final_params_print # Return derived version\n",
    "    else:\n",
    "        final_best_params_to_return = None\n",
    "        print(\"No successful trials completed or no improvement found.\")\n",
    "\n",
    "    print(\"\\n--- Tuning Results Summary (Top 5 by Mean Metric) ---\")\n",
    "    print(results_df.head().to_string())\n",
    "\n",
    "    # Save full results if needed\n",
    "    # results_df.to_csv(\"full_tuning_results.csv\", index=False)\n",
    "\n",
    "    return final_best_params_to_return, results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Define Search Space ---\n",
    "# Reduced for quick example run - expand as needed\n",
    "tuning_param_space_2 = {\n",
    "    'hidden_dim': [32, 64],\n",
    "    'latent_dim': [32], # Will be capped by hidden_dim\n",
    "    'num_conv_layers': [1, 2],\n",
    "    'learning_rate': [5e-4, 1e-3],\n",
    "    'dropout': [0.7],\n",
    "    'lambda_struct_only': [0.75] # Tune this, attr = 1 - this\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "best_params_test, tuning_df_test = run_hyperparameter_tuning_repeated(\n",
    "    num_trials=2, \n",
    "    param_space=tuning_param_space_2,\n",
    "    train_g_dev=train_g_on_device,\n",
    "    val_g_dev=val_g_on_device,\n",
    "    gt_node_labels_val_dev=GT_NODE_LABELS[\"val\"],\n",
    "    gt_edge_labels_val_dev=GT_EDGE_LABELS[\"val\"], # Pass edge labels\n",
    "    in_dim_member=in_dim_member,\n",
    "    n_iter=2,\n",
    "    in_dim_provider=in_dim_provider,\n",
    "    edge_dim=edge_dim,\n",
    "    device=device,\n",
    "    num_epochs_per_trial=20, # Low epochs for quick example\n",
    "    early_stopping_metric='AP',\n",
    "    early_stopping_element='Avg AP', # Monitor average AP\n",
    "    patience=30,  \n",
    "    val_freq=10\n",
    ")\n",
    "\n",
    "print(\"\\n--- Final Best Parameters from Tuning ---\")\n",
    "print(best_params_test)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools # Needed for product to generate combinations\n",
    "# --- Define Search Space ---\n",
    "# Reduced for quick example run - expand as needed\n",
    "tuning_param_space_3 = {\n",
    "    'hidden_dim': [64],\n",
    "    'latent_dim': [32], # Will be capped by hidden_dim\n",
    "    'num_conv_layers': [1],\n",
    "    'learning_rate': [5e-4],\n",
    "    'dropout': [0.5, 0.7],\n",
    "    'lambda_struct_only': [0.5, 0.75] # Tune this, attr = 1 - this\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "best_params_3, tuning_df_3 = run_hyperparameter_tuning(\n",
    "    num_trials=5, \n",
    "    param_space=tuning_param_space_3,\n",
    "    train_g_dev=train_g_on_device,\n",
    "    val_g_dev=val_g_on_device,\n",
    "    gt_node_labels_val_dev=GT_NODE_LABELS[\"val\"],\n",
    "    gt_edge_labels_val_dev=GT_EDGE_LABELS[\"val\"], # Pass edge labels\n",
    "    in_dim_member=in_dim_member,\n",
    "    in_dim_provider=in_dim_provider,\n",
    "    edge_dim=edge_dim,\n",
    "    device=device,\n",
    "    num_epochs_per_trial=300, # Low epochs for quick example\n",
    "    early_stopping_metric='AP',\n",
    "    early_stopping_element='Avg AP', # Monitor average AP\n",
    "    patience=30,  \n",
    "    val_freq=10\n",
    ")\n",
    "\n",
    "print(\"\\n--- Final Best Parameters from Tuning ---\")\n",
    "print(best_params_3)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools # Needed for product to generate combinations\n",
    "# --- Define Search Space ---\n",
    "# Reduced for quick example run - expand as needed\n",
    "tuning_param_space_4 = {\n",
    "    'hidden_dim': [64],\n",
    "    'latent_dim': [32], # Will be capped by hidden_dim\n",
    "    'num_conv_layers': [1],\n",
    "    'learning_rate': [5e-4],\n",
    "    'dropout': [0.5],\n",
    "    'lambda_struct_only': [0.1, 0.25, 0.5, 0.6, 0.75, 0.9] # Tune this, attr = 1 - this\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "best_params_4, tuning_df_4 = run_hyperparameter_tuning_repeated(\n",
    "    num_trials=6, \n",
    "    param_space=tuning_param_space_4,\n",
    "    train_g_dev=train_g_on_device,\n",
    "    val_g_dev=val_g_on_device,\n",
    "    gt_node_labels_val_dev=GT_NODE_LABELS[\"val\"],\n",
    "    gt_edge_labels_val_dev=GT_EDGE_LABELS[\"val\"], # Pass edge labels\n",
    "    in_dim_member=in_dim_member,\n",
    "    in_dim_provider=in_dim_provider,\n",
    "    edge_dim=edge_dim,\n",
    "    device=device,\n",
    "    num_epochs_per_trial=300, # Low epochs for quick example\n",
    "    early_stopping_metric='AP',\n",
    "    early_stopping_element='Avg AP', # Monitor average AP\n",
    "    patience=30,  \n",
    "    val_freq=10\n",
    ")\n",
    "\n",
    "print(\"\\n--- Final Best Parameters from Tuning ---\")\n",
    "print(best_params_4)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import itertools # Needed for product to generate combinations\n",
    "# --- Define Search Space ---\n",
    "# Reduced for quick example run - expand as needed\n",
    "tuning_param_space_5 = {\n",
    "    'hidden_dim': [64],\n",
    "    'latent_dim': [32], # Will be capped by hidden_dim\n",
    "    'num_conv_layers': [1],\n",
    "    'learning_rate': [5e-4],\n",
    "    'dropout': [0.5],\n",
    "    'lambda_struct_only': [0.1, 0.25, 0.5, 0.6, 0.75, 0.9] # Tune this, attr = 1 - this\n",
    "}\n",
    "\n",
    "\n",
    "\n",
    "best_params_5, tuning_df_5 = run_hyperparameter_tuning_repeated(\n",
    "    num_trials=6, \n",
    "    param_space=tuning_param_space_5,\n",
    "    train_g_dev=train_g_on_device,\n",
    "    val_g_dev=val_g_on_device,\n",
    "    n_iter=3,\n",
    "    gt_node_labels_val_dev=GT_NODE_LABELS[\"val\"],\n",
    "    gt_edge_labels_val_dev=GT_EDGE_LABELS[\"val\"], # Pass edge labels\n",
    "    in_dim_member=in_dim_member,\n",
    "    in_dim_provider=in_dim_provider,\n",
    "    edge_dim=edge_dim,\n",
    "    device=device,\n",
    "    num_epochs_per_trial=200, # Low epochs for quick example\n",
    "    early_stopping_metric='AP',\n",
    "    early_stopping_element='Avg AP', # Monitor average AP\n",
    "    patience=15,  \n",
    "    val_freq=50\n",
    ")\n",
    "\n",
    "print(\"\\n--- Final Best Parameters from Tuning ---\")\n",
    "print(best_params_5)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params = best_params_5.copy()\n",
    "\n",
    "# ---  Train Final Model ---\n",
    "if best_params:\n",
    "    print(\"\\n--- Training Final Model with Best Parameters ---\")\n",
    "    # Combine train and validation for final training (optional, common practice)\n",
    "    # final_train_g = combine_graphs(train_g_dev, val_g_dev) # Requires a helper function\n",
    "    # final_gt_nodes = merge_labels(gt_node_labels_train_dev, gt_node_labels_val_dev) # Helper needed\n",
    "    # Or just train longer on training set\n",
    "\n",
    "    final_lambda_struct = best_params['lambda_struct']\n",
    "    final_lambda_attr = 1.0 - final_lambda_struct\n",
    "    num_epochs = 1000\n",
    "\n",
    "    final_model = BipartiteGraphAutoEncoder_ReportBased(\n",
    "        in_dim_member=in_dim_member,\n",
    "        in_dim_provider=in_dim_provider,\n",
    "        edge_dim=edge_dim,\n",
    "        hidden_dim=best_params['hidden_dim'],\n",
    "        latent_dim=best_params['latent_dim'],\n",
    "        num_conv_layers=best_params['num_conv_layers'],\n",
    "        num_dec_layers=best_params['num_dec_layers'],\n",
    "        dropout=best_params['dropout']\n",
    "    ).to(device)\n",
    "\n",
    "    final_optimizer = torch.optim.Adam(final_model.parameters(), lr=best_params['learning_rate'], weight_decay=weight_decay)\n",
    "\n",
    "    # Train for potentially more epochs or until convergence on train/val combined\n",
    "    final_model, final_history, saved_best_final_model_filepath = train_model_inductive_with_metrics(\n",
    "        model=final_model,\n",
    "        train_graph=train_g_on_device,\n",
    "        num_epochs=num_epochs,\n",
    "        optimizer=final_optimizer,\n",
    "        lambda_attr=final_lambda_attr,             # Used for training loss\n",
    "        lambda_struct=final_lambda_struct,           # Used for training loss\n",
    "        k_neg_samples=k_neg_train,           # Negative samples during training loss calc\n",
    "        target_edge_type=target_edge_type,\n",
    "        device=device,\n",
    "        log_freq=eval_freq,                 # How often to print logs\n",
    "\n",
    "        # --- Validation Arguments ---\n",
    "        val_graph=val_g_on_device,\n",
    "        gt_node_labels_val=GT_NODE_LABELS[\"val\"], # Pass node labels for validation metrics\n",
    "        gt_edge_labels_val=GT_EDGE_LABELS[\"val\"], # Pass edge labels for validation metrics\n",
    "        val_log_freq=eval_freq,              # How often validation logic runs\n",
    "\n",
    "        # --- Validation Scoring Arguments ---\n",
    "        val_lambda_attr=lambda_attr,         # Use same lambda for consistency in validation scoring\n",
    "        val_lambda_struct=lambda_struct,       # Use same lambda for consistency in validation scoring\n",
    "        val_k_neg_samples_score=val_k_neg_score, # K for negative sampling during validation *score* calc (set to 0 by user)\n",
    "\n",
    "        # --- Node Metrics Argument ---\n",
    "        node_k_list=node_k_list_eval,        # K values for P@K, R@K calculation\n",
    "\n",
    "        # --- Early Stopping Arguments ---\n",
    "        early_stopping_metric=early_stop_metric_choice,\n",
    "        early_stopping_element=early_stop_element_choice,\n",
    "        #patience=early_stop_patience_checks,\n",
    "        patience=100,\n",
    "        save_best_model_path=best_model_save_path # Pass the file path\n",
    "    )\n",
    "    # Then evaluate this final_model on the *test set* using evaluate_model_inductively"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "if not os.path.exists(\"report_plots_example\"):\n",
    "    os.makedirs(\"report_plots_example\")\n",
    "plot_training_validation_performance_report_combined_loss(\n",
    "     history=final_history, # The dictionary returned from training\n",
    "     target_edge_type=('provider', 'to', 'member'), # The edge type monitored\n",
    "     figsize_metrics=(10,4),\n",
    "     save_dir='training_validation_curves.png' # Optional save path\n",
    " )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 169,
   "metadata": {},
   "outputs": [],
   "source": [
    "best_params[\"lambda_attr\"] = 1 - best_params[\"lambda_struct\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calculate Anomaly Scores on Training and Validation Graphs ---\n",
    "print(\"\\n--- Calculating Anomaly Scores on Training and Validation Graphs ---\")\n",
    "# Use the same lambdas for scoring as for training loss weighting\n",
    "node_scores_train, edge_scores_train = calculate_anomaly_scores(\n",
    "    trained_model=final_model,\n",
    "    eval_graph_data=train_g_on_device,\n",
    "    lambda_attr=best_params[\"lambda_attr\"],\n",
    "    lambda_struct=best_params[\"lambda_struct\"],\n",
    "    target_edge_type=target_edge_type\n",
    ")\n",
    "\n",
    "node_scores_val, edge_scores_val = calculate_anomaly_scores(\n",
    "    trained_model=final_model,\n",
    "    eval_graph_data=val_g_on_device,\n",
    "    lambda_attr=best_params[\"lambda_attr\"],\n",
    "    lambda_struct=best_params[\"lambda_struct\"],\n",
    "    target_edge_type=target_edge_type\n",
    ")\n",
    "print(\"--- Score Calculation Finished ---\")\n",
    "\n",
    "# --- Evaluate Performance on Training and Validation Graphs ---\n",
    "print(\"\\n--- Evaluating Performance on Training and Validation Graphs ---\")\n",
    "k_values_for_eval = [50, 100, 200, 500]\n",
    "\n",
    "# Evaluate on Training Graph\n",
    "results_train = evaluate_performance_inductive(\n",
    "    node_scores=node_scores_train,\n",
    "    edge_scores=edge_scores_train,\n",
    "    gt_node_labels_eval=gt_node_labels_train,\n",
    "    gt_edge_labels_eval=gt_edge_labels_dict_train,\n",
    "    k_list=k_values_for_eval\n",
    ")\n",
    "\n",
    "# Evaluate on Validation Graph\n",
    "results_val = evaluate_performance_inductive(\n",
    "    node_scores=node_scores_val,\n",
    "    edge_scores=edge_scores_val,\n",
    "    gt_node_labels_eval=gt_node_labels_val,\n",
    "    gt_edge_labels_eval=gt_edge_labels_dict_val,\n",
    "    k_list=k_values_for_eval\n",
    ")\n",
    "\n",
    "# --- Display Results ---\n",
    "print(\"\\n--- Inductive Evaluation Results ---\")\n",
    "\n",
    "print(\"\\nTraining Graph Node Results:\")\n",
    "df_node_results_train = pd.DataFrame(results_train['nodes']).T\n",
    "display(df_node_results_train)\n",
    "\n",
    "print(\"\\nTraining Graph Edge Results:\")\n",
    "edge_results_train_str_keys = {str(k): v for k, v in results_train['edges'].items()}\n",
    "df_edge_results_train = pd.DataFrame(edge_results_train_str_keys).T\n",
    "display(df_edge_results_train)\n",
    "\n",
    "print(\"\\nValidation Graph Node Results:\")\n",
    "df_node_results_val = pd.DataFrame(results_val['nodes']).T\n",
    "display(df_node_results_val)\n",
    "\n",
    "print(\"\\nValidation Graph Edge Results:\")\n",
    "edge_results_val_str_keys = {str(k): v for k, v in results_val['edges'].items()}\n",
    "df_edge_results_val = pd.DataFrame(edge_results_val_str_keys).T\n",
    "display(df_edge_results_val)\n",
    "\n",
    "print(\"\\n--- Analysis Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Calculate Anomaly Scores on Test Graph ---\n",
    "print(\"\\n--- Calculating Anomaly Scores on Test Graph ---\")\n",
    "# Use the same lambdas for scoring as for training loss weighting\n",
    "node_scores_test, edge_scores_test = calculate_anomaly_scores(\n",
    "    trained_model=final_model,\n",
    "    eval_graph_data=test_g_on_device,\n",
    "    lambda_attr=best_params[\"lambda_attr\"],\n",
    "    lambda_struct=best_params[\"lambda_struct\"],\n",
    "    target_edge_type=target_edge_type\n",
    ")\n",
    "print(\"--- Score Calculation Finished ---\")\n",
    "\n",
    "# --- Evaluate Performance on Test Graph ---\n",
    "print(\"\\n--- Evaluating Performance on Test Graph ---\")\n",
    "k_values_for_eval = [50, 100, 200, 500]\n",
    "\n",
    "# Evaluate on Test Graph\n",
    "results_test = evaluate_performance_inductive(\n",
    "    node_scores=node_scores_test,\n",
    "    edge_scores=edge_scores_test,\n",
    "    gt_node_labels_eval=gt_node_labels_test,\n",
    "    gt_edge_labels_eval=gt_edge_labels_dict_test,\n",
    "    k_list=k_values_for_eval\n",
    ")\n",
    "\n",
    "# --- Display Results ---\n",
    "print(\"\\nTest Graph Node Results:\")\n",
    "df_node_results_test = pd.DataFrame(results_test['nodes']).T\n",
    "display(df_node_results_test)\n",
    "\n",
    "print(\"\\nTest Graph Edge Results:\")\n",
    "edge_results_test_str_keys = {str(k): v for k, v in results_test['edges'].items()}\n",
    "df_edge_results_test = pd.DataFrame(edge_results_test_str_keys).T\n",
    "display(df_edge_results_test)\n",
    "\n",
    "print(\"\\n--- Test Graph Analysis Complete ---\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.train_utils import calculate_anomaly_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.utils.eval_utils import *\n",
    "k_neg = k_values_for_eval.copy()\n",
    "\n",
    "ev_params = {\n",
    "    \"k_list\": [50, 100, 200, 500],\n",
    "    \"lambda_attr\" : best_params[\"lambda_attr\"],\n",
    "    \"lambda_struct\" : best_params[\"lambda_struct\"],\n",
    "    \"k_neg_samples\": k_neg,\n",
    "}\n",
    "all_scores, summary_df, anomaly_type_df = evaluate_model_inductively(\n",
    "    trained_model=final_model,\n",
    "    train_graph=train_g,\n",
    "    val_graph=val_g,\n",
    "    test_graph=test_g,\n",
    "    gt_node_labels=GT_NODE_LABELS,\n",
    "    gt_edge_labels=GT_EDGE_LABELS,\n",
    "    anomaly_tracking_all=ANOMALY_TRACKING, \n",
    "    device=device,\n",
    "    eval_params=ev_params,\n",
    "    target_edge_type=target_edge_type,\n",
    "    plot=True,\n",
    "    verbose=True\n",
    ")\n",
    "\n",
    "print(\"\\n--- Overall Metrics Summary ---\")\n",
    "display(summary_df)\n",
    "\n",
    "print(\"\\n--- Anomaly Type Performance Summary ---\")\n",
    "display(anomaly_type_df)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "# Set a suitable style for report plots\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"viridis\", font_scale=1.1) # Using viridis palette\n",
    "\n",
    "def extract_main_category(tag):\n",
    "    \"\"\"Helper function to extract the main category from the tag.\"\"\"\n",
    "    if pd.isna(tag):\n",
    "        return 'Unknown'\n",
    "    if isinstance(tag, str):\n",
    "        # Handle edge type strings explicitly if they appear in Anomaly Tag column\n",
    "        if tag.startswith(\"('\"):\n",
    "            return 'Edge' # Classify edge types simply as 'Edge' for grouping\n",
    "        if '/' in tag:\n",
    "            category = tag.split('/')[0]\n",
    "            # Standardize capitalization\n",
    "            if category.lower() == 'structural': return 'Structural'\n",
    "            if category.lower() == 'attribute': return 'Attribute'\n",
    "            return category # Keep original if not standard\n",
    "        elif tag.lower() == 'combined':\n",
    "            return 'Combined'\n",
    "        elif tag.lower() == 'unknown':\n",
    "             return 'Unknown'\n",
    "        else:\n",
    "             # Basic keyword check for tags without '/'\n",
    "             if 'attribute' in tag.lower(): return 'Attribute'\n",
    "             if 'structural' in tag.lower(): return 'Structural'\n",
    "             if 'combined' in tag.lower(): return 'Combined'\n",
    "             return 'Other' # Fallback category\n",
    "    return 'Unknown' # Default for non-string types or unparseable strings\n",
    "\n",
    "\n",
    "def analyze_anomaly_types(\n",
    "    anomaly_type_df: pd.DataFrame,\n",
    "    split_name: str = 'test',\n",
    "    sort_metric: str = 'AP',\n",
    "    metrics_to_analyze: List[str] = ['AUROC', 'AP', 'Best F1'], # Focus on performance metrics\n",
    "    plot_metric_comparison: str = 'AP', # Metric for the first bar plot\n",
    "    plot_score_distributions: bool = True, # Flag to generate score plot\n",
    "    plot_figsize_comparison: Tuple[int, int] = (10, 6),\n",
    "    plot_figsize_distribution: Tuple[int, int] = (12, 6),\n",
    "    save_dir: Optional[str] = None\n",
    "    ) -> Tuple[Optional[pd.DataFrame], Optional[pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Analyzes performance breakdown by anomaly type for a specific data split,\n",
    "    generating essential tables and plots for reporting.\n",
    "\n",
    "    Args:\n",
    "        anomaly_type_df (pd.DataFrame): DataFrame with per-tag metrics vs normals\n",
    "                                        (expected columns: 'Split', 'Node Type', 'Node Type',\n",
    "                                        'Anomaly Tag', 'Count', 'Mean Score', 'Median Score', 'AUROC', 'AP', 'Best F1').\n",
    "        all_scores (Dict): The scores dictionary output by evaluate_model_inductively,\n",
    "                           structured as {'split': {'nodes': {type: scores}, 'edges': {type: scores}}}.\n",
    "                           Needed for plotting score distributions.\n",
    "        split_name (str): The split to analyze ('train', 'val', or 'test').\n",
    "        sort_metric (str): Metric used to sort the detailed tag results ('AP', 'AUROC', 'Best F1').\n",
    "        metrics_to_analyze (List[str]): List of performance metric columns for summaries.\n",
    "        plot_metric_comparison (str): The metric to visualize in the category comparison bar plot.\n",
    "        plot_score_distributions (bool): Whether to generate the score distribution violin plot.\n",
    "        plot_figsize_comparison: Figure size for the metric comparison plot.\n",
    "        plot_figsize_distribution: Figure size for the score distribution plot.\n",
    "        save_dir (Optional[str]): Directory to save the output tables and plots.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Optional[pd.DataFrame], Optional[pd.DataFrame]]:\n",
    "            - df_category_summary: Aggregated metrics by Main Category and Node Type.\n",
    "            - df_detailed_sorted: Detailed metrics per Anomaly Tag, sorted.\n",
    "            (Returns None, None if analysis fails)\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Analyzing Anomaly Type Performance for Split: '{split_name}' ---\")\n",
    "\n",
    "    if anomaly_type_df is None or anomaly_type_df.empty:\n",
    "        print(\"Input DataFrame `anomaly_type_df` is empty. Cannot perform analysis.\")\n",
    "        return None, None\n",
    "\n",
    "    # --- 1. Preprocessing ---\n",
    "    df_split = anomaly_type_df[anomaly_type_df['Split'] == split_name].copy()\n",
    "    if df_split.empty:\n",
    "        print(f\"No data found for split '{split_name}' in `anomaly_type_df`.\")\n",
    "        return None, None\n",
    "\n",
    "    # Check required metric columns\n",
    "    required_metrics = metrics_to_analyze + ([plot_metric_comparison] if plot_metric_comparison not in metrics_to_analyze else [])\n",
    "    required_cols_base = ['Node Type', 'Anomaly Tag', 'Count']\n",
    "    missing_cols = [col for col in required_cols_base + required_metrics if col not in df_split.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Error: Missing required columns in DataFrame: {missing_cols}\")\n",
    "        return None, None\n",
    "\n",
    "    # Add Main Category\n",
    "    df_split['Main Category'] = df_split['Anomaly Tag'].apply(extract_main_category)\n",
    "\n",
    "    # Calculate Total Anomalies per Node Type for proportion calculation\n",
    "    # Ensure 'Node Type' exists before grouping\n",
    "    if 'Node Type' not in df_split.columns:\n",
    "         print(\"Error: 'Node Type' column missing, cannot calculate proportions.\")\n",
    "         return None, None\n",
    "    total_anomalies_per_element = df_split.groupby('Node Type')['Count'].sum()\n",
    "\n",
    "    def get_proportion(row):\n",
    "        total = total_anomalies_per_element.get(row['Node Type'], 0)\n",
    "        return (row['Count'] / total * 100) if total > 0 else 0\n",
    "    df_split['Proportion (%)'] = df_split.apply(get_proportion, axis=1)\n",
    "\n",
    "    # --- 2. Table 1: Performance Summary by Main Category & Node Type ---\n",
    "    print(\"\\n--- Table 1: Performance Summary by Main Anomaly Category & Node Type ---\")\n",
    "    df_category_summary = None\n",
    "    try:\n",
    "        category_group = df_split.groupby(['Main Category', 'Node Type'])\n",
    "        df_category_summary = category_group[metrics_to_analyze].mean() # Calculate mean of specified metrics\n",
    "\n",
    "        # Add counts and proportions\n",
    "        df_category_summary['Total Anomalies'] = category_group['Count'].sum()\n",
    "        df_category_summary['Category Proportion (%)'] = category_group['Proportion (%)'].sum()\n",
    "\n",
    "        # Reorder and round\n",
    "        cols_order = ['Total Anomalies', 'Category Proportion (%)'] + metrics_to_analyze\n",
    "        df_category_summary = df_category_summary.reindex(columns=cols_order, fill_value=np.nan).round(3)\n",
    "\n",
    "        print(df_category_summary.to_string())\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating category summary table: {e}\")\n",
    "\n",
    "\n",
    "    # --- 3. Table 2: Detailed Performance by Specific Anomaly Tag (Sorted) ---\n",
    "    print(f\"\\n--- Table 2: Detailed Performance by Specific Anomaly Tag (Sorted by {sort_metric}) ---\")\n",
    "    df_detailed_sorted = None\n",
    "    try:\n",
    "        # Include score stats in detailed table if present\n",
    "        score_stats = [col for col in ['Mean Score', 'Median Score'] if col in df_split.columns]\n",
    "        cols_detailed = ['Node Type', 'Main Category', 'Anomaly Tag', 'Count', 'Proportion (%)'] + metrics_to_analyze + score_stats\n",
    "        cols_detailed = [col for col in cols_detailed if col in df_split.columns] # Ensure all selected columns exist\n",
    "\n",
    "        df_detailed_sorted = df_split.sort_values(\n",
    "            by=['Node Type', 'Main Category', sort_metric],\n",
    "            ascending=[True, True, False] # Sort metric descending\n",
    "        )[cols_detailed].round(3)\n",
    "\n",
    "        print(df_detailed_sorted.to_string(index=False, max_rows=50)) # Print more rows\n",
    "    except KeyError:\n",
    "         print(f\"Error: Sort metric '{sort_metric}' not found in DataFrame columns.\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating detailed sorted table: {e}\")\n",
    "\n",
    "\n",
    "    # --- 4. Plot 1: Metric Comparison by Main Category ---\n",
    "    print(f\"\\n--- Plot 1: Comparison of Average {plot_metric_comparison} by Main Category ---\")\n",
    "    if df_category_summary is not None and plot_metric_comparison in df_category_summary.columns:\n",
    "        try:\n",
    "            plot_data_comp = df_category_summary.reset_index()\n",
    "            plt.figure(figsize=plot_figsize_comparison)\n",
    "\n",
    "            ax = sns.barplot(\n",
    "                data=plot_data_comp,\n",
    "                x='Main Category',\n",
    "                y=plot_metric_comparison,\n",
    "                hue='Node Type', # Group by node/edge type\n",
    "                palette='viridis',\n",
    "                edgecolor='grey',\n",
    "                linewidth=0.75\n",
    "            )\n",
    "            plt.title(f'Average {plot_metric_comparison} by Main Anomaly Category ({split_name.capitalize()} Set)')\n",
    "            plt.xlabel('Main Anomaly Category')\n",
    "            plt.ylabel(f'Average {plot_metric_comparison}')\n",
    "            plt.xticks(rotation=0)\n",
    "            plt.legend(title='Node Type', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "            ax.grid(True, axis='y', linestyle=':', alpha=0.7)\n",
    "            # Add value labels on bars\n",
    "            for container in ax.containers:\n",
    "                 ax.bar_label(container, fmt='%.2f', label_type='edge', padding=2, fontsize=9)\n",
    "\n",
    "            plt.ylim(bottom=0) # Start y-axis at 0 for bar plots\n",
    "            plt.tight_layout(rect=[0, 0, 0.88, 0.97]) # Adjust right margin for legend\n",
    "\n",
    "            # Save plot if directory specified\n",
    "            if save_dir:\n",
    "                plot_path_comp = os.path.join(save_dir, f'plot_category_comparison_{plot_metric_comparison}_{split_name}.png')\n",
    "                try:\n",
    "                    plt.savefig(plot_path_comp, dpi=300, bbox_inches='tight')\n",
    "                    print(f\"Comparison plot saved to {plot_path_comp}\")\n",
    "                except Exception as e: print(f\"Error saving comparison plot: {e}\")\n",
    "            plt.show()\n",
    "\n",
    "        except Exception as e: print(f\"Error generating comparison plot: {e}\")\n",
    "    else: print(f\"Cannot generate comparison plot: Summary data missing or plot metric '{plot_metric_comparison}' not found.\")\n",
    "\n",
    "\n",
    "    # --- 5. Plot 2: Score Distribution by Main Category ---\n",
    "    print(\"\\n--- Plot 2: Anomaly Score Distributions by Main Category ---\")\n",
    "    if plot_score_distributions:\n",
    "        # Check if Mean Score and Count columns exist for the workaround\n",
    "        if 'Mean Score' not in df_split.columns or 'Count' not in df_split.columns:\n",
    "            print(\"Cannot generate score distribution plot: 'Mean Score' or 'Count' column missing in DataFrame.\")\n",
    "        else:\n",
    "            try:\n",
    "                dist_data = []\n",
    "                # Corrected iteration: Removed 'index' column which likely doesn't exist\n",
    "                # We only need columns already present in df_split for the workaround\n",
    "                required_cols_for_dist = ['Node Type', 'Main Category', 'Anomaly Tag', 'Mean Score', 'Count']\n",
    "                if not all(col in df_split.columns for col in required_cols_for_dist):\n",
    "                    print(f\"Cannot generate score distribution plot: Missing one of {required_cols_for_dist}\")\n",
    "\n",
    "                else:\n",
    "                    # Iterate directly over the necessary columns\n",
    "                    for element_type, main_category, tag, mean_score, count in df_split[required_cols_for_dist].itertuples(index=False, name=None):\n",
    "                        # WORKAROUND: Add multiple points based on mean score and count\n",
    "                        # This approximates the distribution for visualization purposes\n",
    "                        # Note: Using median_score might be another option if available\n",
    "                        if not pd.isna(mean_score) and not pd.isna(count) and count > 0:\n",
    "                            # Add points based on count, using the mean score as the value\n",
    "                            dist_data.extend([{'Node Type': element_type, 'Main Category': main_category, 'Score': mean_score}] * int(count))\n",
    "\n",
    "                    if dist_data:\n",
    "                        df_dist = pd.DataFrame(dist_data)\n",
    "                        plt.figure(figsize=plot_figsize_distribution)\n",
    "                        ax = sns.violinplot(\n",
    "                            data=df_dist,\n",
    "                            x='Main Category',\n",
    "                            y='Score',\n",
    "                            hue='Node Type',\n",
    "                            palette='viridis',\n",
    "                            cut=0,\n",
    "                            inner='quartile',\n",
    "                            linewidth=1.0, # Slightly thinner lines for violin\n",
    "                            split=False,\n",
    "                            scale='width' # Scale violins to have approx same width for comparison\n",
    "                        )\n",
    "                        plt.title(f'Approximated Score Distributions by Main Category ({split_name.capitalize()} Set)')\n",
    "                        plt.xlabel('Main Anomaly Category')\n",
    "                        plt.ylabel('Anomaly Score (Approximated: based on Mean Score per Tag)')\n",
    "                        plt.xticks(rotation=0)\n",
    "                        plt.legend(title='Node Type', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "                        ax.grid(True, axis='y', linestyle=':', alpha=0.7)\n",
    "                        plt.tight_layout(rect=[0, 0, 0.88, 0.97])\n",
    "\n",
    "                        # Save plot\n",
    "                        if save_dir:\n",
    "                            plot_path_dist = os.path.join(save_dir, f'plot_category_score_distribution_{split_name}.png')\n",
    "                            try:\n",
    "                                plt.savefig(plot_path_dist, dpi=300, bbox_inches='tight')\n",
    "                                print(f\"Score distribution plot saved to {plot_path_dist}\")\n",
    "                            except Exception as e: print(f\"Error saving score distribution plot: {e}\")\n",
    "                        plt.show()\n",
    "\n",
    "                    else:\n",
    "                        print(\"Could not generate score distribution plot: No valid data points extracted.\")\n",
    "\n",
    "            except Exception as e:\n",
    "                print(f\"Error generating score distribution plot: {e}\")\n",
    "                # import traceback # Uncomment for detailed debugging\n",
    "                # traceback.print_exc()\n",
    "\n",
    "    else:\n",
    "        print(\"Skipping score distribution plot as per request.\")\n",
    "\n",
    "\n",
    "\n",
    "    # --- Save DataFrames ---\n",
    "    if save_dir:\n",
    "        if df_category_summary is not None:\n",
    "            cat_path = os.path.join(save_dir, f'summary_category_{split_name}.csv')\n",
    "            try: df_category_summary.to_csv(cat_path)\n",
    "            except Exception as e: print(f\"Error saving category summary: {e}\")\n",
    "        if df_detailed_sorted is not None:\n",
    "             det_path = os.path.join(save_dir, f'summary_detailed_tags_{split_name}.csv')\n",
    "             try: df_detailed_sorted.to_csv(det_path, index=False)\n",
    "             except Exception as e: print(f\"Error saving detailed summary: {e}\")\n",
    "        if df_category_summary is not None or df_detailed_sorted is not None:\n",
    "             print(f\"Summary tables saved to {save_dir}\")\n",
    "\n",
    "    return df_category_summary, df_detailed_sorted\n",
    "\n",
    "\n",
    "\n",
    "category_summary, detailed_summary = analyze_anomaly_types(\n",
    "    anomaly_type_df=anomaly_type_df,\n",
    "    #all_scores=all_scores, # Pass the raw scores here\n",
    "    split_name='train',\n",
    "    sort_metric='AP',\n",
    "    metrics_to_analyze=['AUROC', 'AP', 'Best F1'], # Focus on performance metrics\n",
    "    plot_metric_comparison='AP',\n",
    "    plot_score_distributions=True,\n",
    "    #save_dir='analysis_results_v2'\n",
    ")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4.2 Baseline Models"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.1 Feature-based baseline models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Add necessary imports (should already be there from previous step)\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.svm import OneClassSVM\n",
    "from sklearn.metrics import roc_auc_score, average_precision_score\n",
    "from src.utils.baseline_utils import *\n",
    "\n",
    "\n",
    "train_graph = train_g.clone()\n",
    "val_graph = val_g.clone()\n",
    "test_graph = test_g.clone()\n",
    "\n",
    "# --- 1. Augment Features for Each Split  ---\n",
    "print(\"Augmenting features for train, val, and test splits...\")\n",
    "augmented_features_train = augment_features_for_sklearn(train_graph)\n",
    "augmented_features_val = augment_features_for_sklearn(val_graph)\n",
    "augmented_features_test = augment_features_for_sklearn(test_graph)\n",
    "print(\"Feature augmentation complete.\")\n",
    "\n",
    "# --- Initialize results storage ---\n",
    "results_baselines_separate = {} # Stores scores per model, split, type\n",
    "eval_summary_separate = []      # Stores evaluation metrics\n",
    "k_list_baselines = [50, 100, 200] # Or define elsewhere\n",
    "\n",
    "# --- 2. Loop through Node Types for Separate Training and Evaluation ---\n",
    "for node_type in ['provider', 'member']:\n",
    "    print(f\"\\n--- Processing Node Type: {node_type} ---\")\n",
    "\n",
    "    # --- 2a. Extract Features and Labels for this Node Type ---\n",
    "    print(f\"  Extracting {node_type} features and labels...\")\n",
    "    try:\n",
    "        X_train_nt = augmented_features_train[node_type]\n",
    "        y_train_nt = gt_node_labels_train[node_type].cpu().numpy()\n",
    "\n",
    "        X_val_nt = augmented_features_val[node_type]\n",
    "        y_val_nt = gt_node_labels_val[node_type].cpu().numpy()\n",
    "\n",
    "        X_test_nt = augmented_features_test[node_type]\n",
    "        y_test_nt = gt_node_labels_test[node_type].cpu().numpy()\n",
    "\n",
    "        # Basic validation\n",
    "        if not (X_train_nt.shape[0] == y_train_nt.shape[0] and\n",
    "                X_val_nt.shape[0] == y_val_nt.shape[0] and\n",
    "                X_test_nt.shape[0] == y_test_nt.shape[0]):\n",
    "            raise ValueError(\"Feature/label shape mismatch for a split.\")\n",
    "\n",
    "        if X_train_nt.shape[0] == 0:\n",
    "             print(f\"  Skipping {node_type}: No training data found.\")\n",
    "             continue\n",
    "\n",
    "    except (KeyError, ValueError) as e:\n",
    "        print(f\"  Skipping {node_type}: Error extracting data - {e}\")\n",
    "        continue\n",
    "\n",
    "    print(f\"    Train shape: {X_train_nt.shape}\")\n",
    "    print(f\"    Val shape: {X_val_nt.shape}\")\n",
    "    print(f\"    Test shape: {X_test_nt.shape}\")\n",
    "\n",
    "    # --- 2b. Preprocess (Scale) Features ---\n",
    "    print(f\"  Scaling {node_type} features...\")\n",
    "    scaler_nt = StandardScaler()\n",
    "    #X_train_scaled_nt = scaler_nt.fit_transform(X_train_nt)\n",
    "    # Handle cases where val/test might be empty for this node type\n",
    "    #X_val_scaled_nt = scaler_nt.transform(X_val_nt) if X_val_nt.shape[0] > 0 else X_val_nt\n",
    "    # X_test_scaled_nt = scaler_nt.transform(X_test_nt) if X_test_nt.shape[0] > 0 else X_test_nt\n",
    "    X_train_scaled_nt = X_train_nt.copy()\n",
    "    X_val_scaled_nt = X_val_nt.copy()\n",
    "    X_test_scaled_nt = X_test_nt.copy()\n",
    "    print(\"    Scaling complete.\")\n",
    "\n",
    "    # --- 2c. Define and Train Baseline Models ---\n",
    "    print(f\"  Training baseline models for {node_type}...\")\n",
    "    contamination_est_nt = y_train_nt.mean()\n",
    "    print(f\"    Estimated contamination ({node_type}): {contamination_est_nt:.4f}\")\n",
    "\n",
    "    models_nt = {}\n",
    "    # Isolation Forest\n",
    "    iforest_nt = IsolationForest(contamination=contamination_est_nt, random_state=42)\n",
    "    iforest_nt.fit(X_train_scaled_nt)\n",
    "    models_nt['IsolationForest'] = iforest_nt\n",
    "    print(\"    Isolation Forest trained.\")\n",
    "\n",
    "    # One-Class SVM\n",
    "    ocsvm_nt = OneClassSVM(nu=max(0.01, min(0.99, contamination_est_nt)), kernel='rbf', gamma='auto') # Ensure nu in (0, 1)\n",
    "    ocsvm_nt.fit(X_train_scaled_nt)\n",
    "    models_nt['OneClassSVM'] = ocsvm_nt\n",
    "    print(\"    One-Class SVM trained.\")\n",
    "    print(f\"    Baseline training for {node_type} complete.\")\n",
    "\n",
    "    # --- 2d. Predict/Score Anomalies ---\n",
    "    print(f\"  Generating anomaly scores for {node_type}...\")\n",
    "    if node_type not in results_baselines_separate:\n",
    "        results_baselines_separate[node_type] = {}\n",
    "\n",
    "    for model_name, model in models_nt.items():\n",
    "        if model_name not in results_baselines_separate[node_type]:\n",
    "            results_baselines_separate[node_type][model_name] = {}\n",
    "\n",
    "        scores_train_nt = model.decision_function(X_train_scaled_nt)\n",
    "        # Handle potentially empty val/test splits\n",
    "        scores_val_nt = model.decision_function(X_val_scaled_nt) if X_val_scaled_nt.shape[0] > 0 else np.array([])\n",
    "        scores_test_nt = model.decision_function(X_test_scaled_nt) if X_test_scaled_nt.shape[0] > 0 else np.array([])\n",
    "\n",
    "        # Negate scores\n",
    "        scores_train_nt = -scores_train_nt\n",
    "        scores_val_nt = -scores_val_nt\n",
    "        scores_test_nt = -scores_test_nt\n",
    "\n",
    "        results_baselines_separate[node_type][model_name]['train'] = {'scores': scores_train_nt, 'labels': y_train_nt}\n",
    "        results_baselines_separate[node_type][model_name]['val'] = {'scores': scores_val_nt, 'labels': y_val_nt}\n",
    "        results_baselines_separate[node_type][model_name]['test'] = {'scores': scores_test_nt, 'labels': y_test_nt}\n",
    "    print(f\"    Scoring for {node_type} complete.\")\n",
    "\n",
    "    # --- 2e. Evaluate ---\n",
    "    print(f\"  Evaluating baseline models for {node_type}...\")\n",
    "    for model_name, split_results in results_baselines_separate[node_type].items():\n",
    "        print(f\"    Model: {model_name}\")\n",
    "        for split_name, data in split_results.items():\n",
    "            scores = data['scores']\n",
    "            labels = data['labels']\n",
    "\n",
    "            if len(scores) == 0:\n",
    "                print(f\"      Split: {split_name} - No data to evaluate.\")\n",
    "                continue\n",
    "\n",
    "            print(f\"      Split: {split_name} - Items: {len(scores)}, Anomalies: {int(np.sum(labels))}\")\n",
    "            metrics = compute_evaluation_metrics(scores, labels, k_list=k_list_baselines)\n",
    "\n",
    "            summary_row = {\n",
    "                'Model': model_name,\n",
    "                'Split': split_name,\n",
    "                'Element': f'Node ({node_type})', # Store node type info\n",
    "                'Num Items': len(scores),\n",
    "                'Num Anomalies': int(np.sum(labels)),\n",
    "                '% Anomalies': (np.sum(labels) / len(scores) * 100) if len(scores) > 0 else 0\n",
    "            }\n",
    "            summary_row.update(metrics)\n",
    "            eval_summary_separate.append(summary_row)\n",
    "\n",
    "            # Print key metrics\n",
    "            print(f\"        AUROC: {metrics.get('AUROC', 0.0):.4f}, AP: {metrics.get('AP', 0.0):.4f}, Best F1: {metrics.get('Best F1', 0.0):.4f}\")\n",
    "\n",
    "# --- 3. Display Combined Results ---\n",
    "all_model_summaries = defaultdict({})\n",
    "baseline_summary_df_separate = pd.DataFrame(eval_summary_separate)\n",
    "# Reorder columns for clarity\n",
    "ordered_cols = ['Model', 'Split', 'Element', 'Num Items', 'Num Anomalies', '% Anomalies',\n",
    "               'AUROC', 'AP', 'Best F1', 'Best F1 Threshold'] + \\\n",
    "               [f'{p}@{k}' for k in k_list_baselines for p in ['Precision', 'Recall']]\n",
    "# Ensure only existing columns are used for reindexing\n",
    "existing_cols_ordered = [col for col in ordered_cols if col in baseline_summary_df_separate.columns]\n",
    "baseline_summary_df_separate = baseline_summary_df_separate.reindex(columns=existing_cols_ordered, fill_value=np.nan)\n",
    "\n",
    "\n",
    "print(\"\\n--- Baseline Evaluation Summary (Separate Models) ---\")\n",
    "# Increase display options for pandas DataFrame\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 1000)\n",
    "print(baseline_summary_df_separate.to_string())\n",
    "print(\"Baseline evaluation complete.\")\n",
    "\n",
    "# Reset display options if desired\n",
    "# pd.reset_option('display.max_rows')\n",
    "# pd.reset_option('display.max_columns')\n",
    "# pd.reset_option('display.width')\n",
    "\n",
    "\n",
    "# Store the final summary DataFrame\n",
    "all_model_summaries['FeatureBasedSeparate'] = baseline_summary_df_separate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- 3. Display Combined Results ---\n",
    "all_model_summaries = defaultdict()\n",
    "baseline_summary_df_separate = pd.DataFrame(eval_summary_separate)\n",
    "# Reorder columns for clarity\n",
    "ordered_cols = ['Model', 'Split', 'Element', 'Num Items', 'Num Anomalies', '% Anomalies',\n",
    "               'AUROC', 'AP', 'Best F1', 'Best F1 Threshold'] + \\\n",
    "               [f'{p}@{k}' for k in k_list_baselines for p in ['Precision', 'Recall']]\n",
    "# Ensure only existing columns are used for reindexing\n",
    "existing_cols_ordered = [col for col in ordered_cols if col in baseline_summary_df_separate.columns]\n",
    "baseline_summary_df_separate = baseline_summary_df_separate.reindex(columns=existing_cols_ordered, fill_value=np.nan)\n",
    "\n",
    "\n",
    "print(\"\\n--- Baseline Evaluation Summary (Separate Models) ---\")\n",
    "# Increase display options for pandas DataFrame\n",
    "pd.set_option('display.max_rows', 500)\n",
    "pd.set_option('display.max_columns', 50)\n",
    "pd.set_option('display.width', 1000)\n",
    "print(baseline_summary_df_separate.to_string())\n",
    "print(\"Baseline evaluation complete.\")\n",
    "\n",
    "# Reset display options if desired\n",
    "# pd.reset_option('display.max_rows')\n",
    "# pd.reset_option('display.max_columns')\n",
    "# pd.reset_option('display.width')\n",
    "\n",
    "\n",
    "# Store the final summary DataFrame\n",
    "all_model_summaries['FeatureBasedSeparate'] = baseline_summary_df_separate"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.2 Structure-based Baseline models (GAE and OddBall)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### ODDBALL"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 208,
   "metadata": {},
   "outputs": [],
   "source": [
    "from src.external_repos.OddBall.egonet_extractor import EgonetFeatureExtractor\n",
    "from src.external_repos.OddBall.utils import select_group_nodes, get_node_property_list\n",
    "from src.external_repos.OddBall.anomaly_detection import (\n",
    "        StarCliqueAnomalyDetection, HeavyVicinityAnomalyDetection, DominantEdgeAnomalyDetection,\n",
    "        StarCliqueLOFAnomalyDetection, HeavyVicinityLOFAnomalyDetection, DominantEdgeLOFAnomalyDetection\n",
    "    )\n",
    "from src.external_repos.OddBall.oddball_runner import *\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- OddBall Baseline Evaluation ---\n",
    "print(\"\\n--- Starting OddBall Baseline Evaluation ---\")\n",
    "\n",
    "# Define OddBall parameters\n",
    "oddball_anomaly_type = 'sc' # Options: 'sc', 'hv', 'de'\n",
    "oddball_use_lof = False     # Keep False for simplicity first\n",
    "\n",
    "oddball_results_list = []\n",
    "k_list_baselines = [50, 100, 200] # Use same K as other baselines\n",
    "\n",
    "graphs_dict = {'train': train_g, 'val': val_g, 'test': test_g} # Assuming these are your splits\n",
    "\n",
    "for split_name, graph_split in graphs_dict.items():\n",
    "    print(f\"\\n===== Processing Split: {split_name} =====\")\n",
    "    gt_labels_split = GT_NODE_LABELS[split_name] # Get the GT labels for this split\n",
    "\n",
    "    for node_type in ['provider', 'member']:\n",
    "         # Check if node type exists in this split\n",
    "         if node_type not in graph_split.node_types or graph_split[node_type].num_nodes == 0:\n",
    "             print(f\"Skipping Oddball for {node_type} in {split_name} split (no nodes).\")\n",
    "             continue\n",
    "\n",
    "         # Run and evaluate OddBall for this node type\n",
    "         metrics = evaluate_oddball_inductive(\n",
    "             graph_split=graph_split,\n",
    "             gt_node_labels_split=gt_labels_split,\n",
    "             anomaly_type=oddball_anomaly_type,\n",
    "             node_type_to_eval=node_type,\n",
    "             use_lof=oddball_use_lof,\n",
    "             k_list=k_list_baselines\n",
    "         )\n",
    "\n",
    "         if metrics: # Check if evaluation was successful\n",
    "             num_items = graph_split[node_type].num_nodes\n",
    "             num_anomalies = int(gt_labels_split.get(node_type, torch.tensor([])).sum().item())\n",
    "             perc = (num_anomalies / num_items * 100) if num_items > 0 else 0\n",
    "             summary_row = {\n",
    "                 'Model': f'OddBall ({oddball_anomaly_type}, LOF={oddball_use_lof})',\n",
    "                 'Split': split_name,\n",
    "                 'Element': f'Node ({node_type})',\n",
    "                 'Num Items': num_items,\n",
    "                 'Num Anomalies': num_anomalies,\n",
    "                 '% Anomalies': perc\n",
    "             }\n",
    "             summary_row.update(metrics)\n",
    "             oddball_results_list.append(summary_row)\n",
    "         else:\n",
    "              print(f\"OddBall evaluation failed for {node_type} in {split_name} split.\")\n",
    "\n",
    "\n",
    "# --- Display OddBall Results ---\n",
    "if oddball_results_list:\n",
    "    oddball_summary_df = pd.DataFrame(oddball_results_list)\n",
    "    # Reorder columns for clarity (similar to other baselines)\n",
    "    ordered_cols = ['Model', 'Split', 'Element', 'Num Items', 'Num Anomalies', '% Anomalies',\n",
    "                   'AUROC', 'AP', 'Best F1', 'Best F1 Threshold'] + \\\n",
    "                   [f'{p}@{k}' for k in k_list_baselines for p in ['Precision', 'Recall']]\n",
    "    existing_cols_ordered = [col for col in ordered_cols if col in oddball_summary_df.columns]\n",
    "    oddball_summary_df = oddball_summary_df.reindex(columns=existing_cols_ordered, fill_value=np.nan)\n",
    "\n",
    "    print(\"\\n--- OddBall Evaluation Summary ---\")\n",
    "    # Increase display options for pandas DataFrame\n",
    "    pd.set_option('display.max_rows', 500)\n",
    "    pd.set_option('display.max_columns', 50)\n",
    "    pd.set_option('display.width', 1000)\n",
    "    print(oddball_summary_df.to_string())\n",
    "else:\n",
    "    print(\"\\n--- No OddBall results were generated. ---\")\n",
    "\n",
    "all_model_summaries['StructureBased'] = oddball_summary_df\n",
    "combined_summary = pd.concat([baseline_summary_df_separate, oddball_summary_df], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4.2.3 Hybrid methods (DOMINANT)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch_geometric.data import Data\n",
    "from torch_geometric.utils import to_undirected\n",
    "from pygod.detector import DOMINANT, GAE\n",
    "from src.utils.eval_utils import compute_evaluation_metrics # Ensure this is imported\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from tqdm.notebook import tqdm # For progress bars\n",
    "import time\n",
    "\n",
    "# --- Configuration ---\n",
    "k_list_baselines = [50, 100, 200, 500]\n",
    "# <<< Using CPU for consistency and avoiding potential MPS issues with baselines >>>\n",
    "baseline_device = torch.device(\"cpu\")\n",
    "print(f\"Using device for PyGOD graph baselines: {baseline_device}\")\n",
    "target_edge_type = ('provider', 'to', 'member')\n",
    "\n",
    "# Store results\n",
    "graph_baseline_results = {} # Renamed for clarity\n",
    "all_baseline_scores = {} # Stores raw scores (can reuse)\n",
    "\n",
    "# --- Helper Function to Prepare Homogeneous Data (Keep as before) ---\n",
    "def prepare_homogeneous_data(hetero_graph: HeteroData, target_device: torch.device, make_undirected: bool = True) -> Optional[Data]:\n",
    "    \"\"\"Converts HeteroData to a homogeneous Data object on the target device.\"\"\"\n",
    "    try:\n",
    "        hetero_graph_cpu = hetero_graph.cpu()\n",
    "        homo_data = hetero_graph_cpu.to_homogeneous(node_attrs=['x'])\n",
    "\n",
    "        if not hasattr(homo_data, 'edge_index') or homo_data.edge_index is None:\n",
    "            print(\"Warning: No 'edge_index' found after to_homogeneous. Creating empty edge_index.\")\n",
    "            homo_data.edge_index = torch.empty((2, 0), dtype=torch.long)\n",
    "\n",
    "        if make_undirected and homo_data.edge_index.shape[1] > 0:\n",
    "             # Ensure edge indices are valid before making undirected\n",
    "             if homo_data.num_nodes is None:\n",
    "                 if homo_data.edge_index.numel() > 0:\n",
    "                      homo_data.num_nodes = int(homo_data.edge_index.max().item()) + 1\n",
    "                 else:\n",
    "                      homo_data.num_nodes = homo_data.x.shape[0] if hasattr(homo_data, 'x') and homo_data.x is not None else 0\n",
    "\n",
    "             if homo_data.num_nodes is not None and homo_data.edge_index.max().item() < homo_data.num_nodes:\n",
    "                 homo_data.edge_index = to_undirected(homo_data.edge_index, num_nodes=homo_data.num_nodes)\n",
    "             else:\n",
    "                 print(f\"Warning: Cannot make graph undirected. Max edge index {homo_data.edge_index.max().item() if homo_data.edge_index.numel() > 0 else 'N/A'} >= num_nodes {homo_data.num_nodes}. Keeping directed.\")\n",
    "\n",
    "\n",
    "        if not hasattr(homo_data, 'num_nodes') or homo_data.num_nodes is None:\n",
    "             if hasattr(homo_data, 'x') and homo_data.x is not None:\n",
    "                 homo_data.num_nodes = homo_data.x.shape[0]\n",
    "             elif homo_data.edge_index.numel() > 0:\n",
    "                 homo_data.num_nodes = int(homo_data.edge_index.max().item()) + 1\n",
    "             else:\n",
    "                 homo_data.num_nodes = 0\n",
    "                 print(f\"Warning: Manually set num_nodes to 0.\")\n",
    "\n",
    "        return homo_data.to(target_device)\n",
    "    except Exception as e:\n",
    "        print(f\"Error during to_homogeneous conversion: {e}\")\n",
    "        return None\n",
    "\n",
    "# --- Prepare Homogeneous Data Splits (Keep as before) ---\n",
    "print(\"Preparing homogeneous data splits for graph baselines...\")\n",
    "train_g_homo = prepare_homogeneous_data(train_g, target_device=baseline_device, make_undirected=True)\n",
    "val_g_homo = prepare_homogeneous_data(val_g, target_device=baseline_device, make_undirected=True)\n",
    "test_g_homo = prepare_homogeneous_data(test_g, target_device=baseline_device, make_undirected=True)\n",
    "\n",
    "if train_g_homo is not None and val_g_homo is not None and test_g_homo is not None:\n",
    "    print(\"Homogeneous data prepared.\")\n",
    "else:\n",
    "    print(\"Failed to prepare homogeneous data. Skipping graph baselines.\")\n",
    "    train_g_homo, val_g_homo, test_g_homo = None, None, None\n",
    "\n",
    "def map_homo_results(homo_data: Data, homo_scores: np.ndarray, original_hetero_graph: HeteroData, gt_node_labels_split: Dict[str, torch.Tensor]) -> Dict[str, Dict]:\n",
    "    \"\"\"Maps scores from homogeneous graph back and aligns with original GT labels.\"\"\"\n",
    "    results = {}\n",
    "    if not hasattr(homo_data, 'node_type'):\n",
    "        print(\"Error: Homogeneous data is missing 'node_type'. Cannot map results.\")\n",
    "        return {}\n",
    "    node_type_tensor = homo_data.node_type.cpu()\n",
    "\n",
    "    current_gt_labels = {}\n",
    "    start_idx = 0\n",
    "    for i, node_type in enumerate(original_hetero_graph.node_types):\n",
    "        num_nodes_in_split = (node_type_tensor == i).sum().item()\n",
    "        if node_type in gt_node_labels_split:\n",
    "            if len(gt_node_labels_split[node_type]) != num_nodes_in_split:\n",
    "                print(f\"Warning: Label mapping mismatch for {node_type}. Expected {num_nodes_in_split} labels in split GT, found {len(gt_node_labels_split[node_type])}.\")\n",
    "                current_gt_labels[node_type] = gt_node_labels_split[node_type][:num_nodes_in_split].cpu().numpy()\n",
    "            else:\n",
    "                current_gt_labels[node_type] = gt_node_labels_split[node_type].cpu().numpy()\n",
    "        else:\n",
    "            current_gt_labels[node_type] = np.zeros(num_nodes_in_split)\n",
    "\n",
    "\n",
    "    for i, node_type in enumerate(original_hetero_graph.node_types): # Iterate in original order\n",
    "        mask = (node_type_tensor == i)\n",
    "        num_nodes_homo_type = mask.sum().item()\n",
    "\n",
    "        if num_nodes_homo_type == 0: continue\n",
    "\n",
    "        type_scores = homo_scores[mask]\n",
    "        type_labels = current_gt_labels.get(node_type, np.zeros(num_nodes_homo_type)) # Get aligned labels\n",
    "\n",
    "        if len(type_scores) != len(type_labels):\n",
    "             print(f\"CRITICAL WARNING: Final score/label length mismatch for {node_type} after mapping. Scores: {len(type_scores)}, Labels: {len(type_labels)}. Skipping evaluation for this type.\")\n",
    "             continue\n",
    "\n",
    "        results[node_type] = {'scores': type_scores, 'labels': type_labels}\n",
    "    return results\n",
    "\n",
    "# --- Define Baseline Models ---\n",
    "pygod_baselines = {}\n",
    "if train_g_homo is not None:\n",
    "    # Estimate contamination (use same estimate for both)\n",
    "    total_train_nodes = train_g_homo.num_nodes\n",
    "    total_train_anomalies = sum(GT_NODE_LABELS['train'][nt].sum().item() for nt in GT_NODE_LABELS['train'] if nt in train_g.node_types)\n",
    "    contamination_est = total_train_anomalies / total_train_nodes if total_train_nodes > 0 else 0.1\n",
    "    contamination_est = max(0.001, min(0.5, contamination_est)) # Ensure in valid range (0, 0.5]\n",
    "\n",
    "    # 1. DOMINANT \n",
    "    # DOMINANT uses features, check if they exist\n",
    "    if hasattr(train_g_homo, 'x') and train_g_homo.x is not None:\n",
    "         print(f\"Defining DOMINANT with contamination={contamination_est:.4f}\")\n",
    "         pygod_baselines['DOMINANT'] = DOMINANT(\n",
    "             epoch=50, # Adjust epochs as needed\n",
    "             contamination=contamination_est,\n",
    "             gpu=-1, # Force CPU\n",
    "             verbose=0\n",
    "         )\n",
    "    else:\n",
    "        print(\"Skipping DOMINANT baseline definition: Missing 'x' features in homogeneous graph.\")\n",
    "\n",
    "    # 2. GAE (focused on structure) - Keep this, but requires dependency install\n",
    "    if hasattr(train_g_homo, 'x') and train_g_homo.x is not None:\n",
    "        print(f\"Defining GAE_Structure with contamination={contamination_est:.4f}\")\n",
    "        pygod_baselines['GAE_Structure'] = GAE(\n",
    "            epoch=50,\n",
    "            contamination=contamination_est,\n",
    "            recon_s=True, # Focus on structural reconstruction\n",
    "            gpu=-1, # Force CPU\n",
    "            verbose=0\n",
    "        )\n",
    "    else:\n",
    "         print(\"Skipping GAE_Structure baseline definition: Missing 'x' features.\")\n",
    "\n",
    "\n",
    "# --- Run and Evaluate PyGOD Baselines (Keep the loop mostly the same) ---\n",
    "if train_g_homo and pygod_baselines:\n",
    "    # ... (The loop structure from the previous answer can be kept) ...\n",
    "    # ... (Just ensure the models being iterated are now DOMINANT and GAE_Structure) ...\n",
    "    for model_name, model in tqdm(pygod_baselines.items(), desc=\"Running PyGOD Graph Baselines\"): # Updated progress bar desc\n",
    "        print(f\"\\n--- Processing Baseline: {model_name} ---\")\n",
    "        model_scores = {'train': {}, 'val': {}, 'test': {}}\n",
    "\n",
    "        try:\n",
    "            # --- Fit on Training Data ---\n",
    "            print(f\"Fitting {model_name} on training data...\")\n",
    "            fit_data = train_g_homo # Already on CPU\n",
    "\n",
    "            if fit_data is None or fit_data.num_nodes == 0:\n",
    "                print(f\"Skipping fit for {model_name}: Invalid or empty training data.\")\n",
    "                continue\n",
    "\n",
    "            # GAE needs 'x', DOMINANT needs 'x' and 'edge_index'\n",
    "            if not hasattr(fit_data, 'x') or fit_data.x is None:\n",
    "                 print(f\"Skipping fit for {model_name}: Missing 'x' features required by model.\")\n",
    "                 continue\n",
    "            if not hasattr(fit_data, 'edge_index') or fit_data.edge_index is None or fit_data.edge_index.numel() == 0:\n",
    "                 # DOMINANT/GAE might behave unexpectedly without edges\n",
    "                 print(f\"Warning: Fitting {model_name} on graph with no edges.\")\n",
    "\n",
    "\n",
    "            start_fit = time.time()\n",
    "            model.fit(fit_data)\n",
    "            end_fit = time.time()\n",
    "            print(f\"Fit completed in {end_fit - start_fit:.2f}s.\")\n",
    "\n",
    "            # --- Get Scores (Decision Function) ---\n",
    "            print(f\"Calculating scores for {model_name}...\")\n",
    "            for split_name, hetero_graph, homo_graph in [('train', train_g, train_g_homo),\n",
    "                                                        ('val', val_g, val_g_homo),\n",
    "                                                        ('test', test_g, test_g_homo)]:\n",
    "                if homo_graph is None:\n",
    "                    print(f\"  Skipping scoring for {split_name}: Homogeneous graph is None.\")\n",
    "                    continue\n",
    "\n",
    "                try:\n",
    "                    eval_data = homo_graph # Already on CPU\n",
    "\n",
    "                    if eval_data.num_nodes == 0:\n",
    "                         print(f\"  Skipping scoring for {split_name}: No nodes in homogeneous graph.\")\n",
    "                         homo_scores_split = np.array([])\n",
    "                    # Check for necessary inputs for the specific model\n",
    "                    elif (not hasattr(eval_data, 'x') or eval_data.x is None):\n",
    "                         print(f\"  Skipping scoring for {split_name} with {model_name}: Missing 'x' features.\")\n",
    "                         homo_scores_split = np.zeros(eval_data.num_nodes) # Placeholder\n",
    "                    elif (not hasattr(eval_data, 'edge_index') or eval_data.edge_index is None or eval_data.edge_index.numel()==0):\n",
    "                         print(f\"  Warning: Scoring {split_name} with {model_name} on graph with no edges.\")\n",
    "                         # Models might still produce scores based on features alone\n",
    "                         homo_scores_split = model.decision_function(eval_data)\n",
    "                    else:\n",
    "                         homo_scores_split = model.decision_function(eval_data)\n",
    "\n",
    "\n",
    "                    gt_labels_split = GT_NODE_LABELS.get(split_name, {})\n",
    "                    mapped_scores = map_homo_results(homo_graph, homo_scores_split, hetero_graph, gt_labels_split)\n",
    "                    model_scores[split_name] = mapped_scores\n",
    "\n",
    "                except Exception as e_score:\n",
    "                    print(f\"Error scoring {split_name} data with {model_name}: {e_score}\")\n",
    "                    model_scores[split_name] = {}\n",
    "\n",
    "            all_baseline_scores[model_name] = model_scores\n",
    "\n",
    "            # --- Evaluate Mapped Scores ---\n",
    "            # In the Baseline Evaluation Loop:\n",
    "\n",
    "            # --- Evaluate Mapped Scores ---\n",
    "            print(f\"Evaluating {model_name}...\")\n",
    "            for split_name, split_scores_mapped in model_scores.items():\n",
    "                print(f\"  Split: {split_name}\")\n",
    "                if not split_scores_mapped:\n",
    "                    print(f\"    Skipping evaluation for {split_name}: No mapped scores found.\")\n",
    "                    continue\n",
    "                for node_type, type_data in split_scores_mapped.items():\n",
    "                    # <<< FIX: Ensure scores and labels are NumPy arrays >>>\n",
    "                    scores_raw = type_data.get('scores')\n",
    "                    labels_raw = type_data.get('labels')\n",
    "\n",
    "                    # Check if they exist and convert if necessary\n",
    "                    if scores_raw is not None and isinstance(scores_raw, torch.Tensor):\n",
    "                        scores_np = scores_raw.cpu().numpy()\n",
    "                    elif scores_raw is not None: # Assume it's already numpy or list-like\n",
    "                        scores_np = np.asarray(scores_raw)\n",
    "                    else:\n",
    "                        scores_np = np.array([])\n",
    "\n",
    "                    if labels_raw is not None and isinstance(labels_raw, torch.Tensor):\n",
    "                        labels_np = labels_raw.cpu().numpy()\n",
    "                    elif labels_raw is not None: # Assume it's already numpy or list-like\n",
    "                        labels_np = np.asarray(labels_raw)\n",
    "                    else:\n",
    "                        labels_np = np.array([])\n",
    "\n",
    "                    # Proceed with evaluation using NumPy arrays\n",
    "                    if len(scores_np) > 0 and len(labels_np) > 0 and len(scores_np) == len(labels_np):\n",
    "                        # Pass the guaranteed NumPy arrays to the metrics function\n",
    "                        metrics = compute_evaluation_metrics(scores_np, labels_np, k_list=k_list_baselines)\n",
    "\n",
    "                        # Store metrics (rest of the storing logic remains the same)\n",
    "                        if model_name not in graph_baseline_results:\n",
    "                            graph_baseline_results[model_name] = {}\n",
    "                        if split_name not in graph_baseline_results[model_name]:\n",
    "                            graph_baseline_results[model_name][split_name] = {}\n",
    "                        graph_baseline_results[model_name][split_name][node_type] = metrics\n",
    "                        print(f\"    {node_type}: AUROC={metrics.get('AUROC', 0):.4f}, AP={metrics.get('AP', 0):.4f}, Best F1={metrics.get('Best F1', 0):.4f}\")\n",
    "                    elif len(scores_np) != len(labels_np):\n",
    "                        print(f\"    {node_type}: Skipping evaluation due to score/label length mismatch ({len(scores_np)} vs {len(labels_np)}).\")\n",
    "                    else:\n",
    "                        print(f\"    {node_type}: No scores/labels found for evaluation.\")\n",
    "\n",
    "        except ImportError as ie:\n",
    "             if model_name == 'GAE_Structure' and ('torch_sparse' in str(ie) or 'NeighborSampler' in str(ie) or 'pyg-lib' in str(ie)):\n",
    "                  print(f\"\\n****** MISSING DEPENDENCY for {model_name} ******\")\n",
    "                  print(f\"Error: {ie}\")\n",
    "                  print(f\"Please install 'torch-sparse' or 'pyg-lib'.\")\n",
    "                  print(f\"  pip install torch-sparse  OR  pip install pyg-lib\")\n",
    "                  print(f\"*******************************\\n\")\n",
    "             else:\n",
    "                  print(f\"ImportError running {model_name}: {ie}\")\n",
    "        except AttributeError as ae:\n",
    "             print(f\"AttributeError running {model_name}: {ae}. \")\n",
    "             print(\"  Ensure homogeneous graph has 'x' and 'edge_index'.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error running baseline {model_name}: {e}\")\n",
    "\n",
    "\n",
    "# --- Display Structural Baseline Results ---\n",
    "print(\"\\n--- Graph-Based Baseline Evaluation Summary ---\") # Updated title\n",
    "summary_graph_list = []\n",
    "# ... (rest of the summary list creation and DataFrame display - use graph_baseline_results) ...\n",
    "for model_name, splits in graph_baseline_results.items(): # Use renamed dict\n",
    "    for split_name, types in splits.items():\n",
    "        for node_type, metrics in types.items():\n",
    "             num_items = len(GT_NODE_LABELS.get(split_name, {}).get(node_type, []))\n",
    "             num_anomalies = int(GT_NODE_LABELS.get(split_name, {}).get(node_type, torch.tensor([])).sum().item())\n",
    "             perc = (num_anomalies / num_items * 100) if num_items > 0 else 0\n",
    "             row = {\n",
    "                 'Model': model_name,\n",
    "                 'Split': split_name,\n",
    "                 'Element': f'Node ({node_type})',\n",
    "                 'Num Items': num_items,\n",
    "                 'Num Anomalies': num_anomalies,\n",
    "                 '% Anomalies': perc\n",
    "             }\n",
    "             row.update(metrics)\n",
    "             summary_graph_list.append(row)\n",
    "\n",
    "graph_summary_df = pd.DataFrame(summary_graph_list) # Use new name\n",
    "if not graph_summary_df.empty:\n",
    "    ordered_cols_struct = ['Model', 'Split', 'Element', 'Num Items', 'Num Anomalies', '% Anomalies',\n",
    "                           'AUROC', 'AP', 'Best F1', 'Best F1 Threshold'] + \\\n",
    "                          [f'{p}@{k}' for k in k_list_baselines for p in ['Precision', 'Recall']]\n",
    "    existing_cols_struct = [col for col in ordered_cols_struct if col in graph_summary_df.columns]\n",
    "    graph_summary_df = graph_summary_df.reindex(columns=existing_cols_struct, fill_value=np.nan)\n",
    "\n",
    "print(graph_summary_df.to_string())\n",
    "print(\"Graph-based baseline evaluation complete.\")\n",
    "\n",
    "# Store the summary\n",
    "all_model_summaries['GraphBased'] = graph_summary_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 214,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_model_summaries['GraphAutoEncoderFramemwork'] = graph_summary_df\n",
    "combined_summary = pd.concat([graph_summary_df, combined_summary], ignore_index=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 5. Compare Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 241,
   "metadata": {},
   "outputs": [],
   "source": [
    "model_name_map = {\n",
    "    \"Oddball\": \"Oddball (Akoglu, 2010)\", # Placeholder reference/description if no specific paper\n",
    "    \"GAE_Structure\": \"GAE (Kipf & Welling, 2016)\", # Ref: Variational Graph Auto-Encoders\n",
    "    \"DOMINANT\": \"DOMINANT (Ding et al., 2019)\",    # Ref: Deep Anomaly Detection on Attributed Networks\n",
    "    \"OneClassSVM\": \"One-Class SVM (Schölkopf et al., 2001)\", # Ref: Estimating the support of a high-dimensional distribution\n",
    "    \"IsolationForest\": \"Isolation Forest (Liu et al., 2008)\"  # Ref: Isolation forest\n",
    "}\n",
    "conditions_rename = [combined_summary[\"Model\"] == original_name for original_name in model_name_map.keys()]\n",
    "choices_rename = list(model_name_map.values())\n",
    "\n",
    "\n",
    "combined_summary[\"Model\"] = np.select(\n",
    "    conditions_rename,\n",
    "    choices_rename,\n",
    "    default=combined_summary[\"Model\"] # Keep the original value if no condition matches\n",
    ")\n",
    "\n",
    "conditions_category = [\n",
    "    combined_summary[\"Model\"].isin([\n",
    "        \"Isolation Forest (Liu et al., 2008)\",\n",
    "        \"One-Class SVM (Schölkopf et al., 2001)\"\n",
    "    ]),\n",
    "    combined_summary[\"Model\"] == \"Oddball (Structural Context)\",\n",
    "    combined_summary[\"Model\"].isin([\n",
    "        \"DOMINANT (Ding et al., 2019)\",\n",
    "        \"GAE (Kipf & Welling, 2016)\"\n",
    "    ])\n",
    "]\n",
    "\n",
    "choices_category = [\n",
    "    \"feature-based baseline\",\n",
    "    \"structure-based baseline\",\n",
    "    \"graph_autoencoder-based baseline\"\n",
    "]\n",
    "\n",
    "combined_summary[\"Category\"] = np.select(\n",
    "    conditions_category,\n",
    "    choices_category,\n",
    "    default=\"Other Category\" # Assign a default category if none of the above match\n",
    ")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "metadata": {},
   "outputs": [],
   "source": [
    "models_to_keep = [\"Isolation Forest (Liu et al., 2008)\", \"Oddball (Akoglu, 2010)\", \"DOMINANT (Ding et al., 2019)\"]\n",
    "combined_summary = combined_summary_old[combined_summary_old.Model.isin(models_to_keep)]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Main model summary : \n",
    "summary_main_model = summary_df.copy()\n",
    "summary_main_model[\"Model\"] = \"BGAE (Proposed model)\"\n",
    "summary_main_model[\"Category\"] = \"bipartite_graph_autoencoder-based main model\"\n",
    "summary_all = pd.concat([summary_main_model, combined_summary], axis=0)\n",
    "\n",
    "print(\"Average AP for all models (nodes only) :\")\n",
    "display(\n",
    "    summary_all.loc[((summary_all.Split == \"test\") & (~summary_all.Element.str.startswith(\"E\")) )]  # Filter for test split\n",
    "    .groupby([\"Model\", \"Category\"])                       # Group by BOTH Model and Category\n",
    "    .agg({\"AP\": \"mean\"})                                  # Calculate mean AP for each group\n",
    "    .rename(columns={\"AP\": \"Avg AP\"})                     # Rename the aggregated column\n",
    "    .sort_values(by=[\"Model\",\"Avg AP\"])                              # Sort the results (by Model level of index)\n",
    "    .reset_index()\n",
    "    .head(10)                                             # Display the top 10 rows\n",
    ")\n",
    "print(\"Metrics for all models evaluated on the test set :\")\n",
    "columns_metrics = [\"Model\", \"Element\", \"AUROC\", \"AP\", \"Best F1\", \n",
    "                   'Precision@50', 'Recall@50', 'Precision@100', 'Recall@100', 'Precision@200', 'Recall@200']\n",
    "display(summary_all.loc[summary_all.Split==\"test\"][columns_metrics].sort_values(by=\"Model\"))\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import io # For creating the sample dataframes\n",
    "from IPython.display import display # For displaying styled table in notebooks\n",
    "\n",
    "\n",
    "combined_summary = summary_all.copy()\n",
    "\n",
    "\n",
    "\n",
    "model_category_map = {\n",
    "    \"BGAE (Proposed model)\": \"Our Model (BGAE)\", # Renamed for clarity\n",
    "    \"DOMINANT (Ding et al., 2019)\": \"DOMINANT\",\n",
    "    \"Isolation Forest (Liu et al., 2008)\": \"Isolation Forest\",\n",
    "    \"Oddball (Akoglu, 2010)\": \"Oddball\"\n",
    "    # Mapping for categories themselves - will be used in plot legend\n",
    "}\n",
    "category_display_map = {\n",
    "    \"bipartite_graph_autoencoder-based main model\": \"Our Approach\",\n",
    "    \"graph_autoencoder-based baseline\": \"Graph Autoencoder Baseline\",\n",
    "    \"feature-based baseline\": \"Feature-based Baseline\",\n",
    "    \"structure-based baseline\": \"Structure-based Baseline\"\n",
    "}\n",
    "\n",
    "# Apply the model name mapping\n",
    "combined_summary[\"Model\"] = combined_summary[\"Model\"].map(model_category_map).fillna(combined_summary[\"Model\"])\n",
    "\n",
    "# Add the Category column based on the NEW model names (if needed) or original logic\n",
    "# Assuming the category mapping is based on the original model names or types:\n",
    "# Re-apply or ensure category is correct based on the logic you used previously\n",
    "# For this sample, I'll create a simplified category mapping based on the new names\n",
    "simplified_category_map = {\n",
    "    \"Our Model (BGAE)\": \"Our Approach\",\n",
    "    \"DOMINANT\": \"Graph Autoencoder Baseline\",\n",
    "    \"Isolation Forest\": \"Feature-based Baseline\",\n",
    "    \"Oddball\": \"Structure-based Baseline\"\n",
    "}\n",
    "combined_summary[\"Category\"] = combined_summary[\"Model\"].map(simplified_category_map).fillna(\"Unknown Category\")\n",
    "\n",
    "\n",
    "# --- Configuration for plots ---\n",
    "sns.set_theme(style=\"whitegrid\") # Use a nice theme\n",
    "plt.rcParams['figure.dpi'] = 300 # High resolution for academic figures\n",
    "plt.rcParams['savefig.dpi'] = 300\n",
    "\n",
    "# Define colors for models for consistency across plots\n",
    "# Ensure enough colors for all unique models AFTER mapping\n",
    "unique_models = combined_summary['Model'].unique()\n",
    "model_colors = sns.color_palette('tab10', n_colors=len(unique_models))\n",
    "model_color_map = dict(zip(unique_models, model_colors))\n",
    "\n",
    "# Define colors for categories for Figure 1 legend\n",
    "unique_categories = combined_summary['Category'].unique()\n",
    "category_colors = sns.color_palette('viridis', n_colors=len(unique_categories))\n",
    "category_color_map = dict(zip(unique_categories, category_colors))\n",
    "\n",
    "\n",
    "# --- Filter out 'Edge' element early for plots ---\n",
    "# This ensures plots only include Node anomalies\n",
    "combined_summary_nodes_only = combined_summary.loc[combined_summary.Element.isin([\"Node (member)\", \"Node (provider)\"])].copy()\n",
    "\n",
    "\n",
    "# --- 2. Create the Main Results Table ---\n",
    "# This table shows key metrics per Model and Element Type on the test set.\n",
    "# Select relevant columns, sort, and format.\n",
    "\n",
    "# Filter for test split and select columns\n",
    "main_table_data = combined_summary[combined_summary.Split == \"test\"][\n",
    "    [\"Model\", \"Category\", \"Element\", \"AUROC\", \"AP\", \"Best F1\",\n",
    "     \"Precision@50\", \"Recall@50\", \"Precision@100\", \"Recall@100\",\n",
    "     \"Precision@200\", \"Recall@200\"]\n",
    "].copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "\n",
    "# Sort for consistent table order\n",
    "main_table_data = main_table_data.sort_values(by=[\"Model\", \"Element\"])\n",
    "\n",
    "# Define formatting for decimal places\n",
    "format_dict = {\n",
    "    \"AUROC\": \"{:.3f}\",\n",
    "    \"AP\": \"{:.3f}\",\n",
    "    \"Best F1\": \"{:.3f}\",\n",
    "    \"Precision@50\": \"{:.2f}\",\n",
    "    \"Recall@50\": \"{:.3f}\",\n",
    "    \"Precision@100\": \"{:.2f}\",\n",
    "    \"Recall@100\": \"{:.3f}\",\n",
    "    \"Precision@200\": \"{:.2f}\",\n",
    "    \"Recall@200\": \"{:.3f}\",\n",
    "}\n",
    "\n",
    "# Apply formatting using Styler\n",
    "styled_table = main_table_data.style.format(format_dict)\n",
    "\n",
    "\n",
    "# --- 2. Generate LaTeX code from the styled table ---\n",
    "\n",
    "latex_string = styled_table.to_latex(\n",
    "    caption=\"Performance metrics of anomaly detection models on the test set, by element type.\", # Your table caption\n",
    "    label=\"tab:main_results\", # LaTeX label for cross-referencing\n",
    "    column_format=\"lll\" + \"c\" * (main_table_data.shape[1] - 3), # lll for first 3 columns, c for the rest\n",
    "    position=\"!htbp\", # Standard position specifier\n",
    "    hrules=True, # Use booktabs horizontal rules (\\toprule, \\midrule, \\bottomrule)\n",
    "    multicol_align=\"c\", # Alignment for multicolumns (if any, usually 'c')\n",
    "    environment=\"tabular\", # Use 'tabular' environment (often wrapped in 'table')\n",
    "    convert_css=False, # Don't convert CSS styles (handled by format())\n",
    "    # index=False is the default for Styler.to_latex, but explicit is fine\n",
    ")\n",
    "\n",
    "# Print the LaTeX code\n",
    "print(\"--- LaTeX Table Code ---\")\n",
    "print(latex_string)\n",
    "\n",
    "# Display the styled table (in environments that support it, like Jupyter)\n",
    "# In a report, you'd typically save this as a LaTeX table or similar.\n",
    "print(\"--- Main Results Table (Test Set) ---\")\n",
    "# To print the raw data frame (less pretty formatting but works anywhere)\n",
    "# print(main_table_data.to_string(index=False, formatters=format_dict))\n",
    "# To display the styled version (better for notebooks/HTML)\n",
    "display(styled_table)\n",
    "\n",
    "# --- 3. Create Figure 1: Overall Average Node AP Comparison ---\n",
    "# This figure shows the average AP for Node anomalies across all models.\n",
    "\n",
    "# Use the combined_summary_nodes_only data\n",
    "node_avg_ap = combined_summary_nodes_only[\n",
    "    combined_summary_nodes_only.Split == \"test\"\n",
    "].groupby([\"Model\", \"Category\"])[\"AP\"].mean().reset_index()\n",
    "\n",
    "# Sort by Average AP descending to easily see the best model\n",
    "node_avg_ap = node_avg_ap.sort_values(by=\"AP\", ascending=False)\n",
    "\n",
    "# Get the sorted order of models for the x-axis\n",
    "model_order_fig1 = node_avg_ap[\"Model\"].tolist()\n",
    "\n",
    "# Adjust figure size (smaller)\n",
    "plt.figure(figsize=(7, 5))\n",
    "ax = sns.barplot( # Get the axes object\n",
    "    x=\"Model\",\n",
    "    y=\"AP\",\n",
    "    hue=\"Category\",\n",
    "    data=node_avg_ap,\n",
    "    palette=category_color_map, # Use category colors for hue\n",
    "    order=model_order_fig1 # Set the order based on sorted data\n",
    ")\n",
    "\n",
    "plt.title(\"Average Precision (AP) for Node Anomaly Detection on Test Set\", fontsize=14)\n",
    "plt.xlabel(\"Model\", fontsize=12)\n",
    "plt.ylabel(\"Average Precision (AP)\", fontsize=12)\n",
    "plt.xticks(rotation=45, ha=\"right\")\n",
    "\n",
    "# Adjust legend title and position\n",
    "# Use handles and labels from the axes to ensure correctness with hue\n",
    "handles, labels = ax.get_legend_handles_labels()\n",
    "# Map original labels to friendlier display names\n",
    "friendly_labels = [category_display_map.get(label, label) for label in labels]\n",
    "\n",
    "ax.legend(handles, friendly_labels, title=\"Model Type\", bbox_to_anchor=(1.05, 1), loc='upper left')\n",
    "\n",
    "\n",
    "# Add AP values on top of bars (mean only)\n",
    "# Iterate through bars to add labels\n",
    "# bar_label might be tricky with hue - manual text is often more reliable here\n",
    "# Or try bar_label on each container\n",
    "for container in ax.containers:\n",
    "    ax.bar_label(container, fmt='%.3f', label_type='center', padding=3)\n",
    "\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.savefig(\"avg_node_ap_barplot.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- 4. Create Figure 2: Key Metric Comparison by Anomaly Type (AP and Best F1) ---\n",
    "# Exclude AUROC and Edge\n",
    "\n",
    "# Filter test data, nodes only, select AP and Best F1\n",
    "metrics_to_plot_fig2 = [\"AP\", \"Best F1\"]\n",
    "detailed_metrics = combined_summary_nodes_only[combined_summary_nodes_only.Split == \"test\"][\n",
    "    [\"Model\", \"Element\"] + metrics_to_plot_fig2\n",
    "]\n",
    "\n",
    "# To sort bars within facets, we need to define a consistent order.\n",
    "# Let's sort the data by Element, then Metric, then Score (descending)\n",
    "detailed_metrics_melted = detailed_metrics.melt(\n",
    "    id_vars=[\"Model\", \"Element\"],\n",
    "    value_vars=metrics_to_plot_fig2,\n",
    "    var_name=\"Metric\",\n",
    "    value_name=\"Score\"\n",
    ").sort_values(by=[\"Element\", \"Metric\", \"Score\"], ascending=[True, True, False]) # Sort by score descending\n",
    "\n",
    "# Get the order of models for the x-axis within each facet based on the sorted data\n",
    "# This is still tricky with catplot, it tends to default to internal sorting.\n",
    "# A simpler approach for consistent order is to define it explicitly based on avg performance (like fig 1)\n",
    "# Let's reuse model_order_fig1 for consistency across plots where possible.\n",
    "\n",
    "# Adjust size (smaller), use row=\"Element\", col=\"Metric\"\n",
    "g = sns.catplot(\n",
    "    x=\"Model\",\n",
    "    y=\"Score\",\n",
    "    hue=\"Model\", # Color bars by model\n",
    "    col=\"Element\",    # Columns for Element types\n",
    "    row=\"Metric\",     # Rows for Metric types\n",
    "    data=detailed_metrics_melted,\n",
    "    kind=\"bar\",\n",
    "    palette=model_color_map, # Use model colors for hue\n",
    "    sharey=False,\n",
    "    height=3.5,       # Reduced height\n",
    "    aspect=1,         # Adjusted aspect ratio\n",
    "    order=model_order_fig1, # Set the order for x-axis\n",
    "    legend_out=True # Explicitly keep legend outside\n",
    ")\n",
    "\n",
    "# Improve plot appearance\n",
    "g.fig.suptitle(\"Anomaly Detection Metrics by Node Type on Test Set\", y=1.02, fontsize=16)\n",
    "g.set_axis_labels(\"Model\", \"Score\")\n",
    "g.set_xticklabels(rotation=45, ha=\"right\")\n",
    "# Adjust titles for each subplot row/column\n",
    "g.set_titles(row_template=\"{row_name}\", col_template=\"{col_name} Anomalies\") # e.g., \"AP\" and \"Node (member) Anomalies\"\n",
    "\n",
    "# Add a single legend for models\n",
    "# The legend is automatically added by catplot with hue=, just need to position it\n",
    "g.add_legend(title=\"Model\", bbox_to_anchor=(1.02, 0.5), loc='center left')\n",
    "\n",
    "# Omit values on bars as discussed.\n",
    "\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1.02]) # Adjust layout to make space for legend on the right\n",
    "plt.savefig(\"detailed_metrics_by_node_type.png\")\n",
    "plt.show()\n",
    "\n",
    "\n",
    "# --- 5. Create Figure 3: Precision and Recall at Top-K Anomalies ---\n",
    "# Exclude Edge\n",
    "\n",
    "# Use the combined_summary_nodes_only data\n",
    "pr_k_cols = [\"Precision@50\", \"Recall@50\", \"Precision@100\", \"Recall@100\", \"Precision@200\", \"Recall@200\"]\n",
    "pr_k_data = combined_summary_nodes_only[combined_summary_nodes_only.Split == \"test\"][[\"Model\", \"Element\"] + pr_k_cols]\n",
    "\n",
    "# Melt the DataFrame\n",
    "pr_k_melted = pr_k_data.melt(\n",
    "    id_vars=[\"Model\", \"Element\"],\n",
    "    value_vars=pr_k_cols,\n",
    "    var_name=\"Metric_k\",\n",
    "    value_name=\"Score\"\n",
    ")\n",
    "\n",
    "# Extract Metric Type and k value\n",
    "pr_k_melted[['Metric_Type', 'k']] = pr_k_melted['Metric_k'].str.split('@', expand=True)\n",
    "pr_k_melted['k'] = pr_k_melted['k'].astype(int)\n",
    "\n",
    "# Sort the melted data to potentially influence line drawing order (optional but can make consistent)\n",
    "pr_k_melted = pr_k_melted.sort_values(by=[\"Element\", \"Metric_Type\", \"k\", \"Score\"], ascending=[True, True, True, False])\n",
    "\n",
    "\n",
    "\n",
    "# Example Placeholder Color Map (replace with your actual map)\n",
    "model_color_map = {\n",
    "    'Our Model (BGAE)': sns.color_palette('tab10')[0],\n",
    "    'DOMINANT': sns.color_palette('tab10')[1],\n",
    "    'Isolation Forest': sns.color_palette('tab10')[2],\n",
    "    'Oddball': sns.color_palette('tab10')[3]\n",
    "}\n",
    "\n",
    "# --- Figure 3: Precision and Recall at Top-K Anomalies ---\n",
    "\n",
    "# Adjust size (smaller)\n",
    "# Let relplot create the legend automatically (default legend_out=True)\n",
    "g = sns.relplot(\n",
    "    x=\"k\",\n",
    "    y=\"Score\",\n",
    "    hue=\"Model\",         # Color lines by model\n",
    "    style=\"Metric_Type\", # Use different line styles for Precision and Recall\n",
    "    col=\"Element\",       # Create a column of plots for each Element type\n",
    "    data=pr_k_melted,\n",
    "    kind=\"line\",\n",
    "    palette=model_color_map,\n",
    "    facet_kws={'sharey': False, 'sharex': True}, # Share x-axis (k), but not y-axis (score range might differ)\n",
    "    height=4,            # Adjusted height\n",
    "    aspect=1.2           # Adjusted aspect ratio\n",
    "    # No explicit legend=True/False or add_legend here\n",
    ")\n",
    "\n",
    "# Improve plot appearance\n",
    "g.fig.suptitle(\"Precision and Recall at Top-K Node Anomalies on Test Set\", y=1.02, fontsize=16)\n",
    "g.set_axis_labels(\"Number of Top Anomalies (k)\", \"Score\")\n",
    "g.set_titles(\"{col_name} Anomalies\") # Titles for each subplot (e.g., \"Node (member) Anomalies\")\n",
    "\n",
    "# Set x-axis ticks to correspond to your k values (50, 100, 200)\n",
    "g.set(xticks=[50, 100, 200], xlim=(40, 210)) # Adjust xlim slightly to give space\n",
    "\n",
    "# --- Adjust the LEGEND created automatically by relplot ---\n",
    "# Access the legend object created by relplot when using hue/style/size\n",
    "# It contains entries for both hue ('Model') and style ('Metric_Type')\n",
    "legend = g.legend\n",
    "\n",
    "# Set its title\n",
    "# You can set a single combined title, or remove the default ones if they cause overlap\n",
    "legend.set_title(\"Legend\") # Simple combined title\n",
    "\n",
    "# Set its position relative to the figure\n",
    "# Use a tuple (x, y) where (0,0) is bottom-left and (1,1) is top-right of the figure\n",
    "# (1.02, 0.5) places the anchor point just outside the right edge, vertically centered\n",
    "legend.set_bbox_to_anchor((1.02, 0.5))\n",
    "\n",
    "# Set the location of the legend box relative to its anchor point\n",
    "# 'center left' means the center-left corner of the legend box is placed at (1.02, 0.5)\n",
    "legend.loc = 'center left'\n",
    "\n",
    "# Optional: Further adjustments if the internal titles/entries still overlap\n",
    "# You might need to iterate through legend.get_texts() or legend.get_lines()\n",
    "# This is more advanced and often unnecessary if positioning is correct.\n",
    "# For instance, to hide default titles:\n",
    "# for text in legend.get_texts():\n",
    "#     if text.get_text() in ['Model', 'Metric_Type']:\n",
    "#         text.set_visible(False)\n",
    "\n",
    "\n",
    "# Use tight_layout with rect to make space for the legend on the right\n",
    "# The rect tuple [left, bottom, right, top] defines the area for the grid.\n",
    "# We make the 'right' edge less than 1.0 to leave space on the right for the legend.\n",
    "# Adjust the 'right' value (e.g., 0.85) until the legend fits comfortably.\n",
    "plt.tight_layout(rect=[0, 0, 0.85, 1.02]) # Adjust rect as needed (e.g., 0.85 leaves ~15% space on the right)\n",
    "\n",
    "\n",
    "plt.savefig(\"pr_at_k_node_lines.png\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 6. Robust evaluation : Several Runs "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 335,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from collections import defaultdict\n",
    "import time\n",
    "import random\n",
    "import os\n",
    "from typing import Dict, Tuple, Optional, List"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_seeds = list(range(5))\n",
    "\n",
    "# Use a standard dictionary for this structure\n",
    "DATA_SPLITS_ITERATIONS = {}\n",
    "\n",
    "# --- Load Base Data (Once) ---\n",
    "print(\"Loading base data...\")\n",
    "df_member_features, members_dataset = load_member_features(\"../data/final_members_df.pickle\")\n",
    "df_provider_features, providers_dataset = load_provider_features(\"../data/final_df.pickle\")\n",
    "_, train_edges, val_edges, test_edges = load_claims_data_with_splitting(\"../data/df_descriptions.pickle\", members_dataset=members_dataset, providers_dataset=providers_dataset)\n",
    "\n",
    "# Create base HeteroData splits\n",
    "base_train_g, base_val_g, base_test_g = prepare_hetero_data_with_splitting(df_member_features, df_provider_features, train_edges, val_edges, test_edges)\n",
    "print(\"Base data loaded and split.\")\n",
    "\n",
    "# --- Anomaly Injection Params ---\n",
    "p_node_anomalies = 0.05\n",
    "p_edge_anomalies = 0.1\n",
    "lambda_structural_injection = 0.5\n",
    "\n",
    "# --- Loop through seeds to generate and store data variations ---\n",
    "print(f\"Generating {len(random_seeds)} data iterations...\")\n",
    "for random_seed in random_seeds:\n",
    "    print(f\"  Processing seed: {random_seed}\")\n",
    "    # Use .clone() to avoid modifying base graphs if inject_scenario_anomalies works in-place\n",
    "    current_train_g = base_train_g.clone()\n",
    "    current_val_g = base_val_g.clone()\n",
    "    current_test_g = base_test_g.clone()\n",
    "\n",
    "    # Inject anomalies for each split using the current seed\n",
    "    train_g_inj, gt_n_tr, gt_e_tr, track_tr = inject_scenario_anomalies(\n",
    "        current_train_g, p_node_anomalies=p_node_anomalies, p_edge_anomalies=p_edge_anomalies, lambda_structural=lambda_structural_injection, seed=random_seed\n",
    "    )\n",
    "    val_g_inj, gt_n_val, gt_e_val, track_val = inject_scenario_anomalies(\n",
    "        current_val_g, p_node_anomalies=p_node_anomalies, p_edge_anomalies=p_edge_anomalies, lambda_structural=lambda_structural_injection, seed=current_seed\n",
    "    )\n",
    "    test_g_inj, gt_n_test, gt_e_test, track_test = inject_scenario_anomalies(\n",
    "        current_test_g, p_node_anomalies=p_node_anomalies, p_edge_anomalies=p_edge_anomalies, lambda_structural=lambda_structural_injection, seed=current_seed\n",
    "    )\n",
    "\n",
    "    # Store the data for this iteration, keyed by the seed\n",
    "    DATA_SPLITS_ITERATIONS[random_seed] = {\n",
    "        'train_graph': train_g_inj,\n",
    "        'val_graph': val_g_inj,\n",
    "        'test_graph': test_g_inj,\n",
    "        'gt_node_labels': {\n",
    "            \"train\": gt_n_tr,\n",
    "            \"val\": gt_n_val,\n",
    "            \"test\": gt_n_test\n",
    "        },\n",
    "        'gt_edge_labels': {\n",
    "            \"train\": gt_e_tr,\n",
    "            \"val\": gt_e_val,\n",
    "            \"test\": gt_e_test\n",
    "        },\n",
    "        'anomaly_tracking': {\n",
    "            \"train\": track_tr,\n",
    "            \"val\": track_val,\n",
    "            \"test\": track_test\n",
    "        }\n",
    "    }\n",
    "\n",
    "print(f\"Stored data and labels for {len(DATA_SPLITS_ITERATIONS)} iterations.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 337,
   "metadata": {},
   "outputs": [],
   "source": [
    "def run_bgae_iteration(\n",
    "    iteration_data: Dict,\n",
    "    best_params: Dict,\n",
    "    device: torch.device,\n",
    "    seed: int\n",
    "    ) -> Tuple[pd.DataFrame, pd.DataFrame, Dict]:\n",
    "    \"\"\"\n",
    "    Trains and evaluates the BGAE model for a single iteration (seed).\n",
    "\n",
    "    Args:\n",
    "        iteration_data (Dict): Dictionary containing 'train_graph', 'val_graph', 'test_graph',\n",
    "                               'gt_node_labels', 'gt_edge_labels', 'anomaly_tracking'.\n",
    "        best_params (Dict): Dictionary of the selected best hyperparameters for BGAE.\n",
    "        device: The torch device to use.\n",
    "        seed: The random seed used for this iteration (for reproducibility).\n",
    "\n",
    "    Returns:\n",
    "        Tuple[pd.DataFrame, pd.DataFrame, Dict]:\n",
    "            - summary_df: Overall performance metrics for this iteration.\n",
    "            - anomaly_type_df: Per-anomaly-type metrics for this iteration.\n",
    "            - all_scores: Raw anomaly scores for nodes and edges for this iteration.\n",
    "    \"\"\"\n",
    "    # 1. Set Seed for reproducibility of this specific run\n",
    "    random.seed(seed)\n",
    "    np.random.seed(seed)\n",
    "    torch.manual_seed(seed)\n",
    "    if device.type == 'cuda': torch.cuda.manual_seed_all(seed)\n",
    "    elif device.type == 'mps': torch.mps.manual_seed(seed)\n",
    "\n",
    "    # 2. Prepare Data for this Iteration\n",
    "    train_g = iteration_data['train_graph']\n",
    "    val_g = iteration_data['val_graph']\n",
    "    test_g = iteration_data['test_graph']\n",
    "    gt_node_labels = iteration_data['gt_node_labels']\n",
    "    gt_edge_labels = iteration_data['gt_edge_labels']\n",
    "    anomaly_tracking = iteration_data['anomaly_tracking']\n",
    "\n",
    "    train_g_dev = train_g.to(device)\n",
    "    val_g_dev = val_g.to(device)\n",
    "    test_g_dev = test_g.to(device)\n",
    "    gt_node_labels_val_dev = {k: v.to(device) for k, v in gt_node_labels[\"val\"].items()}\n",
    "    gt_edge_labels_val_dev = {k: v.to(device) for k, v in gt_edge_labels[\"val\"].items()}\n",
    "\n",
    "    # 3. Get Model Params & Instantiate Model/Optimizer\n",
    "    iter_params = best_params.copy()\n",
    "    final_lambda_struct = iter_params['lambda_struct']\n",
    "    final_lambda_attr = 1.0 - final_lambda_struct\n",
    "\n",
    "    # Assuming dimensions are consistent across iterations or obtained elsewhere\n",
    "    # Replace these with actual dimension retrievals if necessary\n",
    "    in_dim_member = train_g['member'].x.size(1)\n",
    "    in_dim_provider = train_g['provider'].x.size(1)\n",
    "    edge_attr = train_g.get(('provider', 'to', 'member'), {}).get('edge_attr', None)\n",
    "    edge_dim = edge_attr.size(1) if edge_attr is not None else 0\n",
    "\n",
    "\n",
    "    model = BipartiteGraphAutoEncoder_ReportBased(\n",
    "        in_dim_member=in_dim_member,\n",
    "        in_dim_provider=in_dim_provider,\n",
    "        edge_dim=edge_dim,\n",
    "        hidden_dim=iter_params['hidden_dim'],\n",
    "        latent_dim=iter_params['latent_dim'],\n",
    "        num_conv_layers=iter_params['num_conv_layers'],\n",
    "        num_dec_layers=iter_params['num_dec_layers'],\n",
    "        dropout=iter_params['dropout']\n",
    "    ).to(device)\n",
    "\n",
    "    optimizer = torch.optim.Adam(model.parameters(),\n",
    "                                 lr=iter_params['learning_rate'],\n",
    "                                 weight_decay=iter_params.get('weight_decay', 1e-5))\n",
    "\n",
    "    # 4. Train Model\n",
    "    num_epochs_final = 1000\n",
    "    k_neg_train = 5\n",
    "    val_k_neg_score = 0\n",
    "    eval_freq = 10 # Can make this larger if not needed for early stopping analysis here\n",
    "    target_edge_type = ('provider', 'to', 'member')\n",
    "    early_stop_patience_final = 100 # Patience used during final training\n",
    "\n",
    "    # Note: train_model_inductive_with_metrics is assumed available\n",
    "    trained_model_iter, _, _ = train_model_inductive_with_metrics(\n",
    "        model=model,\n",
    "        train_graph=train_g_dev,\n",
    "        num_epochs=num_epochs_final,\n",
    "        optimizer=optimizer,\n",
    "        lambda_attr=final_lambda_attr,\n",
    "        lambda_struct=final_lambda_struct,\n",
    "        k_neg_samples=k_neg_train,\n",
    "        target_edge_type=target_edge_type,\n",
    "        device=device,\n",
    "        log_freq=9999, # Suppress logs during repeated runs\n",
    "        # Validation Args\n",
    "        val_graph=val_g_dev,\n",
    "        gt_node_labels_val=gt_node_labels_val_dev,\n",
    "        gt_edge_labels_val=gt_edge_labels_val_dev,\n",
    "        val_log_freq=eval_freq,\n",
    "        # Validation Scoring Args\n",
    "        val_lambda_attr=final_lambda_attr,\n",
    "        val_lambda_struct=final_lambda_struct,\n",
    "        val_k_neg_samples_score=val_k_neg_score,\n",
    "        node_k_list=[50], # Minimal list sufficient for early stopping logic\n",
    "        # Early Stopping Args\n",
    "        early_stopping_metric='AP',\n",
    "        early_stopping_element='Avg AP',\n",
    "        patience=early_stop_patience_final,\n",
    "        save_best_model_path=None # No saving during these runs\n",
    "    )\n",
    "\n",
    "    # 5. Evaluate Model on All Splits\n",
    "    eval_params = {\n",
    "        \"k_list\": [50, 100, 200, 500], # Use desired Ks for final reporting\n",
    "        \"lambda_attr\": final_lambda_attr,\n",
    "        \"lambda_struct\": final_lambda_struct,\n",
    "        \"k_neg_samples_score\": val_k_neg_score # Use k=0 for eval scoring\n",
    "    }\n",
    "    # Note: evaluate_model_inductively is assumed available\n",
    "    all_scores, summary_df, anomaly_type_df = evaluate_model_inductively(\n",
    "        trained_model=trained_model_iter,\n",
    "        train_graph=train_g_dev,\n",
    "        val_graph=val_g_dev,\n",
    "        test_graph=test_g_dev,\n",
    "        gt_node_labels=gt_node_labels, # Pass full dict for this iter\n",
    "        gt_edge_labels=gt_edge_labels, # Pass full dict for this iter\n",
    "        anomaly_tracking_all=anomaly_tracking, # Pass full dict for this iter\n",
    "        device=device,\n",
    "        eval_params=eval_params,\n",
    "        target_edge_type=target_edge_type,\n",
    "        plot=False,\n",
    "        verbose=False\n",
    "    )\n",
    "\n",
    "    # 6. Clean up (Optional but good practice)\n",
    "    del trained_model_iter\n",
    "    del optimizer\n",
    "    if device.type == 'cuda': torch.cuda.empty_cache()\n",
    "    elif device.type == 'mps': torch.mps.empty_cache()\n",
    "\n",
    "    return summary_df, anomaly_type_df, all_scores"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_summary_dfs = []\n",
    "all_anomaly_type_dfs = []\n",
    "all_run_scores = {} # Store scores per iteration if needed\n",
    "\n",
    "for seed in random_seeds:\n",
    "    print(f\"\\n===== Running BGAE Iteration for Seed: {seed} =====\")\n",
    "    iteration_data = DATA_SPLITS_ITERATIONS.get(seed)\n",
    "    if iteration_data:\n",
    "        try:\n",
    "            summary_df, anomaly_type_df, scores = run_bgae_iteration(\n",
    "                iteration_data=iteration_data,\n",
    "                best_params=best_params, # Your optimal hyperparameters\n",
    "                device=device,\n",
    "                seed=seed\n",
    "            )\n",
    "            # Store results\n",
    "            summary_df['seed'] = seed\n",
    "            anomaly_type_df['seed'] = seed\n",
    "            all_summary_dfs.append(summary_df)\n",
    "            all_anomaly_type_dfs.append(anomaly_type_df)\n",
    "            all_run_scores[seed] = scores\n",
    "            print(f\"--- Finished Iteration {seed} ---\")\n",
    "        except Exception as e:\n",
    "            print(f\"--- ERROR in Iteration {seed}: {e} ---\")\n",
    "            # Store error markers if needed\n",
    "            all_summary_dfs.append(pd.DataFrame([{'seed': seed, 'error': str(e)}]))\n",
    "            all_anomaly_type_dfs.append(pd.DataFrame([{'seed': seed, 'error': str(e)}]))\n",
    "    else:\n",
    "        print(f\"Warning: Data for seed {seed} not found in DATA_SPLITS_ITERATIONS.\")\n",
    "\n",
    "# --- Aggregate results after the loop ---\n",
    "if all_summary_dfs:\n",
    "    final_summary_df = pd.concat(all_summary_dfs, ignore_index=True)\n",
    "    print(\"\\n--- Aggregated Overall Results ---\")\n",
    "if all_anomaly_type_dfs:\n",
    "     final_anomaly_type_df = pd.concat(all_anomaly_type_dfs, ignore_index=True)\n",
    "     print(\"\\n--- Aggregated Anomaly Type Results ---\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_run_scores.get(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Aggregation & Final Reporting ---\n",
    "print(\"\\n--- Aggregating Results Across All Iterations ---\")\n",
    "\n",
    "# --- Aggregate Overall Summary ---\n",
    "if all_summary_dfs:\n",
    "    full_summary_results_df = pd.concat(all_summary_dfs, ignore_index=True)\n",
    "    metrics_to_check_agg = ['AUROC', 'AP', 'Best F1'] # Core metrics to check for validity\n",
    "    cols_exist = all(m in full_summary_results_df.columns for m in metrics_to_check_agg)\n",
    "\n",
    "    if cols_exist:\n",
    "        # Filter out rows where core metrics might be NaN (indicating a failed run segment)\n",
    "        successful_summary_df = full_summary_results_df.dropna(subset=metrics_to_check_agg, how='any')\n",
    "    else:\n",
    "        print(\"Warning: Core metric columns (AUROC, AP, Best F1) missing in overall summary. Aggregation might be inaccurate.\")\n",
    "        successful_summary_df = full_summary_results_df # Proceed cautiously\n",
    "\n",
    "    if not successful_summary_df.empty:\n",
    "        print(f\"Aggregating results from {successful_summary_df['seed'].nunique()} successful runs (overall summary)...\")\n",
    "        # Define metrics to aggregate\n",
    "        metrics_to_aggregate = metrics_to_check_agg + \\\n",
    "                               [f'{p}@{k}' for k in node_k_list_eval for p in ['Precision', 'Recall'] if f'{p}@{k}' in successful_summary_df.columns]\n",
    "\n",
    "        # Calculate mean and std dev\n",
    "        aggregated_summary = successful_summary_df.groupby(['Split', 'Element'])[metrics_to_aggregate].agg(['mean', 'std'])\n",
    "        run_counts_summary = successful_summary_df.groupby(['Split', 'Element'])['seed'].nunique().rename('n_runs')\n",
    "        aggregated_summary = pd.concat([run_counts_summary, aggregated_summary], axis=1)\n",
    "\n",
    "        print(\"\\n--- Aggregated Performance Summary (Mean ± Std Dev) ---\")\n",
    "        # Format for printing\n",
    "        aggregated_summary_print = aggregated_summary.copy()\n",
    "        # Flatten MultiIndex (handle potential errors if agg resulted in single level)\n",
    "        if isinstance(aggregated_summary_print.columns, pd.MultiIndex):\n",
    "             aggregated_summary_print.columns = ['_'.join(map(str, col)).strip('_') for col in aggregated_summary_print.columns.values]\n",
    "        else:\n",
    "            aggregated_summary_print.columns = aggregated_summary_print.columns # Keep as is if already flat\n",
    "\n",
    "        for col in metrics_to_aggregate:\n",
    "            mean_col = f'{col}_mean'\n",
    "            std_col = f'{col}_std'\n",
    "            # Check if columns exist after potential flattening/aggregation quirks\n",
    "            if mean_col in aggregated_summary_print.columns and std_col in aggregated_summary_print.columns:\n",
    "                 aggregated_summary_print[f'{col}'] = aggregated_summary_print[mean_col].map('{:.4f}'.format) + \\\n",
    "                                                      ' ± ' + \\\n",
    "                                                      aggregated_summary_print[std_col].map('{:.4f}'.format)\n",
    "                 # Drop only if both exist to avoid errors\n",
    "                 aggregated_summary_print = aggregated_summary_print.drop(columns=[mean_col, std_col], errors='ignore')\n",
    "            elif mean_col in aggregated_summary_print.columns:\n",
    "                 aggregated_summary_print[f'{col}'] = aggregated_summary_print[mean_col].map('{:.4f}'.format) + ' ± nan'\n",
    "                 aggregated_summary_print = aggregated_summary_print.drop(columns=[mean_col], errors='ignore')\n",
    "\n",
    "        print(aggregated_summary_print.to_string())\n",
    "        final_summary_df = aggregated_summary # Store numerical version\n",
    "\n",
    "    else:\n",
    "        print(\"No successful runs found in overall summary results to aggregate.\")\n",
    "else:\n",
    "    print(\"No overall summary DataFrames found in all_summary_dfs to aggregate.\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# --- Aggregate Anomaly Type Metrics ---\n",
    "\n",
    "if all_anomaly_type_dfs:\n",
    "    full_anomaly_type_results_df = pd.concat(all_anomaly_type_dfs, ignore_index=True)\n",
    "    print(\"\\n--- Debug: Columns in full_anomaly_type_results_df ---\")\n",
    "    print(full_anomaly_type_results_df.columns)\n",
    "    # print(full_anomaly_type_results_df.head()) # Uncomment to see first few rows\n",
    "\n",
    "    # Filter out failed runs based on a key metric column existence or specific error marker\n",
    "    metrics_to_check_atype = ['AUROC', 'AP', 'Best F1']\n",
    "    # Check which core metrics *actually exist* in the concatenated DataFrame\n",
    "    metrics_present_check = [m for m in metrics_to_check_atype if m in full_anomaly_type_results_df.columns]\n",
    "\n",
    "    if not metrics_present_check:\n",
    "         print(\"Warning: Core metric columns (AUROC, AP, Best F1) missing entirely from anomaly type results. Cannot filter or aggregate metrics.\")\n",
    "         successful_anomaly_df = full_anomaly_type_results_df # Proceed with caution, only stats/counts will be aggregated\n",
    "    else:\n",
    "        # Filter rows where *at least one* of the essential metrics is valid (not NaN)\n",
    "        successful_anomaly_df = full_anomaly_type_results_df.dropna(subset=metrics_present_check, how='all').copy() # Use .copy() to avoid SettingWithCopyWarning\n",
    "        # Alternatively, use 'any' if you want rows where *all* essential metrics are non-NaN:\n",
    "        # successful_anomaly_df = full_anomaly_type_results_df.dropna(subset=metrics_present_check, how='any').copy()\n",
    "\n",
    "    if not successful_anomaly_df.empty:\n",
    "        n_runs_represented = successful_anomaly_df['seed'].nunique()\n",
    "        print(f\"\\nAggregating results from {n_runs_represented} successful runs (anomaly type summary)...\")\n",
    "\n",
    "        # Define metrics and stats columns to aggregate AGAIN based on the successful DataFrame\n",
    "        metrics_to_aggregate_atype = ['AUROC', 'AP', 'Best F1']\n",
    "        score_stats_to_aggregate = ['Mean Score', 'Median Score']\n",
    "        metrics_present_atype = [m for m in metrics_to_aggregate_atype if m in successful_anomaly_df.columns]\n",
    "        score_stats_present = [s for s in score_stats_to_aggregate if s in successful_anomaly_df.columns]\n",
    "\n",
    "        print(f\"Metrics available for aggregation: {metrics_present_atype}\")\n",
    "        print(f\"Score stats available for aggregation: {score_stats_present}\")\n",
    "\n",
    "        # Define grouping columns - ensure they exist\n",
    "        base_grouping_cols = ['Split', 'Anomaly Tag']\n",
    "        # Determine optional grouping cols based on what's actually in the successful_anomaly_df\n",
    "        optional_grouping_cols = []\n",
    "        if 'Element Type' in successful_anomaly_df.columns:\n",
    "            optional_grouping_cols.append('Element Type')\n",
    "        elif 'Node Type' in successful_anomaly_df.columns: # Fallback if Element Type isn't there but Node Type is\n",
    "             optional_grouping_cols.append('Node Type')\n",
    "        grouping_cols_atype = base_grouping_cols + optional_grouping_cols\n",
    "\n",
    "        if not all(col in successful_anomaly_df.columns for col in grouping_cols_atype):\n",
    "            print(f\"Error: Cannot aggregate. Missing one or more grouping columns {grouping_cols_atype} in filtered anomaly type results.\")\n",
    "        else:\n",
    "            # Aggregate performance metrics if any are present\n",
    "            if metrics_present_atype:\n",
    "                aggregated_metrics_atype = successful_anomaly_df.groupby(\n",
    "                    grouping_cols_atype, observed=True # Use observed=True for stability with categorical data\n",
    "                    )[metrics_present_atype].agg(['mean', 'std'])\n",
    "                # Flatten metrics multi-index\n",
    "                if isinstance(aggregated_metrics_atype.columns, pd.MultiIndex):\n",
    "                     aggregated_metrics_atype.columns = ['_'.join(map(str, col)).strip('_') for col in aggregated_metrics_atype.columns.values]\n",
    "\n",
    "            else:\n",
    "                aggregated_metrics_atype = pd.DataFrame() # Create empty df if no metrics to aggregate\n",
    "\n",
    "            # Aggregate score stats and counts separately\n",
    "            stats_count_cols_to_agg = score_stats_present + ['Count', 'Proportion (%)', 'seed']\n",
    "            stats_count_cols_present = [c for c in stats_count_cols_to_agg if c in successful_anomaly_df.columns]\n",
    "\n",
    "            if stats_count_cols_present:\n",
    "                agg_dict = {f'{col}_mean': pd.NamedAgg(column=col, aggfunc='mean') for col in stats_count_cols_present if col != 'seed'}\n",
    "                agg_dict.update({f'{col}_std': pd.NamedAgg(column=col, aggfunc='std') for col in stats_count_cols_present if col != 'seed'})\n",
    "                agg_dict['n_runs'] = pd.NamedAgg(column='seed', aggfunc='nunique')\n",
    "\n",
    "                aggregated_stats_counts = successful_anomaly_df.groupby(\n",
    "                        grouping_cols_atype, observed=True\n",
    "                        ).agg(**agg_dict) # Use dictionary unpacking for NamedAgg\n",
    "            else:\n",
    "                 aggregated_stats_counts = pd.DataFrame() # Create empty if no stats/counts columns\n",
    "\n",
    "            # Combine aggregated parts\n",
    "            # Check if both have data before concatenating\n",
    "            if not aggregated_stats_counts.empty and not aggregated_metrics_atype.empty:\n",
    "                 aggregated_anomaly_type = pd.concat([aggregated_stats_counts, aggregated_metrics_atype], axis=1)\n",
    "            elif not aggregated_stats_counts.empty:\n",
    "                 aggregated_anomaly_type = aggregated_stats_counts\n",
    "            elif not aggregated_metrics_atype.empty:\n",
    "                  aggregated_anomaly_type = aggregated_metrics_atype\n",
    "            else:\n",
    "                  aggregated_anomaly_type = pd.DataFrame() # Both were empty\n",
    "\n",
    "\n",
    "            if not aggregated_anomaly_type.empty:\n",
    "                print(\"\\n--- Aggregated Anomaly Type Performance (Mean ± Std Dev) ---\")\n",
    "                # Select and format key columns for concise printing\n",
    "                final_anomaly_type_print = aggregated_anomaly_type.copy()\n",
    "                cols_to_print_atype = [] # Build dynamically\n",
    "\n",
    "                # Add run count if available\n",
    "                if 'n_runs' in final_anomaly_type_print.columns: cols_to_print_atype.append('n_runs')\n",
    "                # Add mean count if available\n",
    "                if 'Count_mean' in final_anomaly_type_print.columns: cols_to_print_atype.append('Count_mean')\n",
    "                # Add mean proportion if available\n",
    "                prop_mean_col = 'Proportion (%)_mean'; prop_std_col = 'Proportion (%)_std'\n",
    "                if prop_mean_col in final_anomaly_type_print.columns:\n",
    "                     if prop_std_col in final_anomaly_type_print.columns:\n",
    "                         final_anomaly_type_print['Proportion (%)'] = final_anomaly_type_print[prop_mean_col].map('{:.1f}'.format) + \\\n",
    "                                                                       ' ± ' + \\\n",
    "                                                                       final_anomaly_type_print[prop_std_col].map('{:.1f}'.format)\n",
    "                     else:\n",
    "                         final_anomaly_type_print['Proportion (%)'] = final_anomaly_type_print[prop_mean_col].map('{:.1f}'.format) + ' ± nan'\n",
    "                     cols_to_print_atype.append('Proportion (%)')\n",
    "\n",
    "\n",
    "                # Format performance metrics\n",
    "                for m in metrics_present_atype:\n",
    "                    mean_col = f'{m}_mean'; std_col = f'{m}_std'\n",
    "                    col_name_formatted = f'{m}' # Display name\n",
    "                    if mean_col in final_anomaly_type_print.columns and std_col in final_anomaly_type_print.columns:\n",
    "                        final_anomaly_type_print[col_name_formatted] = final_anomaly_type_print[mean_col].map('{:.3f}'.format) + \\\n",
    "                                                            ' ± ' + \\\n",
    "                                                            final_anomaly_type_print[std_col].map('{:.3f}'.format)\n",
    "                        cols_to_print_atype.append(col_name_formatted)\n",
    "                    elif mean_col in final_anomaly_type_print.columns:\n",
    "                        final_anomaly_type_print[col_name_formatted] = final_anomaly_type_print[mean_col].map('{:.3f}'.format) + ' ± nan'\n",
    "                        cols_to_print_atype.append(col_name_formatted)\n",
    "\n",
    "                # Format score statistics (optional inclusion)\n",
    "                for s in score_stats_present:\n",
    "                        mean_col = f'{s}_mean'; std_col = f'{s}_std'\n",
    "                        col_name_formatted = f'{s}' # Display name\n",
    "                        if mean_col in final_anomaly_type_print.columns and std_col in final_anomaly_type_print.columns:\n",
    "                            final_anomaly_type_print[col_name_formatted] = final_anomaly_type_print[mean_col].map('{:.3f}'.format) + \\\n",
    "                                                                ' ± ' + \\\n",
    "                                                                final_anomaly_type_print[std_col].map('{:.3f}'.format)\n",
    "                            # cols_to_print_atype.append(col_name_formatted) # Uncomment to add to printout\n",
    "                        elif mean_col in final_anomaly_type_print.columns:\n",
    "                            final_anomaly_type_print[col_name_formatted] = final_anomaly_type_print[mean_col].map('{:.3f}'.format) + ' ± nan'\n",
    "                            # cols_to_print_atype.append(col_name_formatted) # Uncomment to add to printout\n",
    "\n",
    "                # Reorder and reset index for printing\n",
    "                final_anomaly_type_print = final_anomaly_type_print.reset_index()\n",
    "                # Ensure columns exist before selecting for final print\n",
    "                final_print_cols = grouping_cols_atype + [col for col in cols_to_print_atype if col in final_anomaly_type_print.columns]\n",
    "                print(final_anomaly_type_print[final_print_cols].round(3).to_string(max_rows=100))\n",
    "\n",
    "                final_anomaly_type_df = aggregated_anomaly_type # Store numerical version\n",
    "\n",
    "            else:\n",
    "                print(\"Aggregated anomaly type DataFrame is empty after processing.\")\n",
    "\n",
    "    else:\n",
    "        print(\"\\nNo successful anomaly type results found to aggregate.\")\n",
    "\n",
    "else:\n",
    "    print(\"No anomaly type DataFrames found in all_anomaly_type_dfs to aggregate.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 339,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.ensemble import IsolationForest\n",
    "from sklearn.preprocessing import StandardScaler # Assuming you might still want scaling\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "\n",
    "def run_if_iteration(\n",
    "    iteration_data: Dict,\n",
    "    device: torch.device, # Keep device arg for consistency, though IF runs on CPU\n",
    "    seed: int,\n",
    "    ) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Trains and evaluates the Isolation Forest model for a single iteration's data.\n",
    "\n",
    "    Args:\n",
    "        iteration_data (Dict): Dictionary containing 'train_graph', 'val_graph', 'test_graph',\n",
    "                               'gt_node_labels', 'gt_edge_labels', 'anomaly_tracking'.\n",
    "        device: The torch device (passed but likely unused by IF).\n",
    "        seed (int): The random seed used for this iteration (for IF's random_state).\n",
    "\n",
    "    Returns:\n",
    "        Optional[pd.DataFrame]: DataFrame containing evaluation metrics for IF on this iteration,\n",
    "                                or None if an error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Extract graphs and labels for this iteration\n",
    "        train_graph = iteration_data['train_graph']\n",
    "        val_graph = iteration_data['val_graph']\n",
    "        test_graph = iteration_data['test_graph']\n",
    "        gt_node_labels = iteration_data['gt_node_labels'] # Contains train, val, test labels\n",
    "\n",
    "        # 2. Augment Features (Run this for each split needed)\n",
    "        # Assumes augment_features_for_sklearn returns a dict: {'provider': np.array, 'member': np.array}\n",
    "        augmented_features_train = augment_features_for_sklearn(train_graph)\n",
    "        augmented_features_val = augment_features_for_sklearn(val_graph)\n",
    "        augmented_features_test = augment_features_for_sklearn(test_graph)\n",
    "\n",
    "        # 3. Initialize results storage for this iteration\n",
    "        eval_summary_iter = []\n",
    "        k_list_baselines = [50, 100, 200, 500] # K values for baseline metrics\n",
    "\n",
    "        # 4. Loop through Node Types\n",
    "        for node_type in ['provider', 'member']:\n",
    "            # Extract features and labels for this node type and all splits\n",
    "            X_train_nt = augmented_features_train.get(node_type)\n",
    "            y_train_nt_t = gt_node_labels[\"train\"].get(node_type)\n",
    "            X_val_nt = augmented_features_val.get(node_type)\n",
    "            y_val_nt_t = gt_node_labels[\"val\"].get(node_type)\n",
    "            X_test_nt = augmented_features_test.get(node_type)\n",
    "            y_test_nt_t = gt_node_labels[\"test\"].get(node_type)\n",
    "\n",
    "            # Basic checks\n",
    "            if X_train_nt is None or y_train_nt_t is None or X_train_nt.shape[0] == 0:\n",
    "                print(f\"  Skipping IF {node_type}: Missing or empty training data for iteration {seed}.\")\n",
    "                continue\n",
    "            # Convert labels to numpy\n",
    "            y_train_nt = y_train_nt_t.cpu().numpy()\n",
    "            y_val_nt = y_val_nt_t.cpu().numpy() if y_val_nt_t is not None else np.array([])\n",
    "            y_test_nt = y_test_nt_t.cpu().numpy() if y_test_nt_t is not None else np.array([])\n",
    "\n",
    "\n",
    "            # 5. Optional: Scale Features (IF is less sensitive than SVM, but can still help)\n",
    "            # scaler_nt = StandardScaler()\n",
    "            # X_train_scaled_nt = scaler_nt.fit_transform(X_train_nt)\n",
    "            # X_val_scaled_nt = scaler_nt.transform(X_val_nt) if X_val_nt is not None and X_val_nt.shape[0] > 0 else X_val_nt\n",
    "            # X_test_scaled_nt = scaler_nt.transform(X_test_nt) if X_test_nt is not None and X_test_nt.shape[0] > 0 else X_test_nt\n",
    "            # Using unscaled for simplicity here, add scaling if needed\n",
    "            X_train_scaled_nt = X_train_nt\n",
    "            X_val_scaled_nt = X_val_nt\n",
    "            X_test_scaled_nt = X_test_nt\n",
    "\n",
    "\n",
    "            # 6. Train Isolation Forest\n",
    "            # Estimate contamination from the training labels of this iteration\n",
    "            contamination_est_nt = y_train_nt.mean()\n",
    "            # Ensure contamination is within valid range (0, 0.5] for IF\n",
    "            contamination_if = max(0.001, min(0.5, contamination_est_nt))\n",
    "            if contamination_if == 0.5: contamination_if = 0.499 # Avoid edge case = 0.5\n",
    "\n",
    "            iforest_nt = IsolationForest(contamination=contamination_if, random_state=seed)\n",
    "            iforest_nt.fit(X_train_scaled_nt)\n",
    "\n",
    "            # 7. Predict/Score Anomalies for all splits\n",
    "            split_data = {\n",
    "                'train': {'X': X_train_scaled_nt, 'y': y_train_nt},\n",
    "                'val': {'X': X_val_scaled_nt, 'y': y_val_nt},\n",
    "                'test': {'X': X_test_scaled_nt, 'y': y_test_nt}\n",
    "            }\n",
    "\n",
    "            for split_name, data in split_data.items():\n",
    "                X_split = data['X']\n",
    "                y_split = data['y']\n",
    "\n",
    "                if X_split is None or y_split is None or X_split.shape[0] == 0 or y_split.shape[0] == 0:\n",
    "                    # print(f\"  Skipping evaluation for IF {node_type} on {split_name}: No data.\")\n",
    "                    continue\n",
    "\n",
    "                if X_split.shape[0] != y_split.shape[0]:\n",
    "                     print(f\"  Warning: Shape mismatch for IF {node_type} on {split_name}. Scores: {X_split.shape[0]}, Labels: {y_split.shape[0]}. Skipping eval.\")\n",
    "                     continue\n",
    "\n",
    "                scores_split_nt = iforest_nt.decision_function(X_split)\n",
    "                # Negate scores: IF decision_function gives lower scores for anomalies,\n",
    "                # but compute_evaluation_metrics expects higher scores for anomalies.\n",
    "                scores_split_nt = -scores_split_nt\n",
    "\n",
    "                # 8. Evaluate\n",
    "                metrics = compute_evaluation_metrics(scores_split_nt, y_split, k_list=k_list_baselines)\n",
    "\n",
    "                summary_row = {\n",
    "                    'Model': 'IsolationForest', # Fixed model name\n",
    "                    'Split': split_name,\n",
    "                    'Element': f'Node ({node_type})',\n",
    "                    'Num Items': len(scores_split_nt),\n",
    "                    'Num Anomalies': int(np.sum(y_split)),\n",
    "                    '% Anomalies': (np.sum(y_split) / len(scores_split_nt) * 100) if len(scores_split_nt) > 0 else 0,\n",
    "                    'seed': seed, # Add seed info\n",
    "                    'iteration': seed # Assuming seed acts as iteration identifier\n",
    "                }\n",
    "                summary_row.update(metrics)\n",
    "                eval_summary_iter.append(summary_row)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR processing Isolation Forest for iteration {seed}: {e}\")\n",
    "        # Return None or an empty DataFrame to indicate failure for this iteration\n",
    "        return None\n",
    "\n",
    "    # Convert results for this iteration to DataFrame\n",
    "    if eval_summary_iter:\n",
    "        iter_results_df = pd.DataFrame(eval_summary_iter)\n",
    "        # Reorder columns for clarity if needed (optional)\n",
    "        ordered_cols = ['Model', 'Split', 'Element', 'seed', 'Num Items', 'Num Anomalies', '% Anomalies',\n",
    "                       'AUROC', 'AP', 'Best F1', 'Best F1 Threshold'] + \\\n",
    "                       [f'{p}@{k}' for k in k_list_baselines for p in ['Precision', 'Recall']]\n",
    "        existing_cols_ordered = [col for col in ordered_cols if col in iter_results_df.columns]\n",
    "        return iter_results_df.reindex(columns=existing_cols_ordered, fill_value=np.nan)\n",
    "    else:\n",
    "        return pd.DataFrame() # Return empty dataframe if no nodes were processed\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_if_summaries = []\n",
    "for seed, iter_data in DATA_SPLITS_ITERATIONS.items():\n",
    "    print(f\"\\n===== Running IF Iteration for Seed: {seed} =====\")\n",
    "    if_summary_df_iter = run_if_iteration(\n",
    "            iteration_data=iter_data,\n",
    "            device=device, # Pass device even if unused\n",
    "            seed=seed\n",
    "        )\n",
    "    if if_summary_df_iter is not None:\n",
    "        all_if_summaries.append(if_summary_df_iter)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "def run_oddball_iteration(\n",
    "    iteration_data: Dict,\n",
    "    seed: int, \n",
    "    oddball_anomaly_type: str = 'sc', # 'sc', 'hv', 'de'\n",
    "    oddball_use_lof: bool = False,\n",
    "    k_list_eval: List[int] = [50, 100, 200] # Use K list matching your previous code\n",
    "    ) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Evaluates the OddBall model for a single iteration's data splits using\n",
    "    the provided evaluate_oddball_inductive function.\n",
    "\n",
    "    Args:\n",
    "        iteration_data (Dict): Dictionary containing 'train_graph', 'val_graph', 'test_graph',\n",
    "                               'gt_node_labels' for one seed.\n",
    "        seed (int): The random seed for this iteration (for logging).\n",
    "        oddball_anomaly_type (str): Type parameter for OddBall.\n",
    "        oddball_use_lof (bool): LOF parameter for OddBall.\n",
    "        k_list_eval (List[int]): K values for P@K, R@K metrics passed to evaluate_oddball_inductive.\n",
    "\n",
    "    Returns:\n",
    "        Optional[pd.DataFrame]: DataFrame containing evaluation metrics for OddBall on this iteration,\n",
    "                                or None if a critical error occurs.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        graphs_dict = {\n",
    "            'train': iteration_data['train_graph'],\n",
    "            'val': iteration_data['val_graph'],\n",
    "            'test': iteration_data['test_graph']\n",
    "        }\n",
    "        gt_node_labels_all_splits = iteration_data['gt_node_labels']\n",
    "        eval_summary_iter = []\n",
    "\n",
    "        # Loop through Splits and Node Types\n",
    "        for split_name, graph_split in graphs_dict.items():\n",
    "            gt_labels_split = gt_node_labels_all_splits.get(split_name, {})\n",
    "\n",
    "            for node_type in ['provider', 'member']:\n",
    "                # Check if node type exists and has labels for this split\n",
    "                if node_type not in graph_split.node_types or \\\n",
    "                   graph_split[node_type].num_nodes == 0 or \\\n",
    "                   node_type not in gt_labels_split or \\\n",
    "                   gt_labels_split[node_type] is None:\n",
    "                    # print(f\"  Skipping Oddball {node_type} in {split_name} (no nodes or labels).\")\n",
    "                    continue\n",
    "\n",
    "                # Call your existing evaluation function\n",
    "                # Ensure evaluate_oddball_inductive is defined/imported\n",
    "                metrics = evaluate_oddball_inductive(\n",
    "                    graph_split=graph_split,\n",
    "                    gt_node_labels_split=gt_labels_split, # Pass dict for this split\n",
    "                    anomaly_type=oddball_anomaly_type,\n",
    "                    node_type_to_eval=node_type,\n",
    "                    use_lof=oddball_use_lof,\n",
    "                    k_list=k_list_eval\n",
    "                )\n",
    "\n",
    "                if metrics: # Check if evaluation returned metrics\n",
    "                    num_items = graph_split[node_type].num_nodes\n",
    "                    # Safely get anomaly count\n",
    "                    label_tensor = gt_labels_split.get(node_type, torch.tensor([]))\n",
    "                    num_anomalies = int(label_tensor.sum().item())\n",
    "                    perc = (num_anomalies / num_items * 100) if num_items > 0 else 0\n",
    "\n",
    "                    summary_row = {\n",
    "                        'Model': f'OddBall', # Simplified name\n",
    "                        'Split': split_name,\n",
    "                        'Element': f'Node ({node_type})',\n",
    "                        'Num Items': num_items,\n",
    "                        'Num Anomalies': num_anomalies,\n",
    "                        '% Anomalies': perc,\n",
    "                        'seed': seed,\n",
    "                        'iteration': seed\n",
    "                    }\n",
    "                    summary_row.update(metrics) # Add calculated metrics\n",
    "                    eval_summary_iter.append(summary_row)\n",
    "                else:\n",
    "                     print(f\"OddBall evaluation failed or returned None for {node_type} in {split_name} split.\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR processing OddBall for iteration {seed}: {e}\")\n",
    "        return None \n",
    "\n",
    "    # Convert results for this iteration to DataFrame\n",
    "    if eval_summary_iter:\n",
    "        iter_results_df = pd.DataFrame(eval_summary_iter)\n",
    "        # Optional: Reorder columns if needed for consistency\n",
    "        # ordered_cols = [...]\n",
    "        # existing_cols_ordered = [col for col in ordered_cols if col in iter_results_df.columns]\n",
    "        # return iter_results_df.reindex(columns=existing_cols_ordered, fill_value=np.nan)\n",
    "        return iter_results_df\n",
    "    else:\n",
    "        # Return empty DataFrame if no results were generated (e.g., all node types skipped)\n",
    "        return pd.DataFrame()\n",
    "\n",
    "# --- Example Usage within the main loop ---\n",
    "all_oddball_summaries = []\n",
    "for seed, iter_data in DATA_SPLITS_ITERATIONS.items():\n",
    "     print(f\"\\n===== Running OddBall Iteration for Seed: {seed} =====\")\n",
    "     oddball_summary_df_iter = run_oddball_iteration(\n",
    "         iteration_data=iter_data,\n",
    "         seed=seed,\n",
    "         oddball_anomaly_type='sc', # Or 'hv', 'de'\n",
    "         oddball_use_lof=False,\n",
    "         k_list_eval=[50, 100, 200, 500] # Use consistent K list\n",
    "     )\n",
    "     if oddball_summary_df_iter is not None:\n",
    "         all_oddball_summaries.append(oddball_summary_df_iter)\n",
    "#\n",
    "# # Aggregate OddBall results after the loop\n",
    "# if all_oddball_summaries:\n",
    "#     final_oddball_summary_df = pd.concat(all_oddball_summaries, ignore_index=True)\n",
    "#     # ... (Perform groupby/aggregation similar to BGAE results) ...\n",
    "#     print(\"\\n--- Aggregated OddBall Performance ---\")\n",
    "#     # print(aggregated_oddball_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "\n",
    "\n",
    "\n",
    "def run_dominant_iteration(\n",
    "    iteration_data: Dict,\n",
    "    device: torch.device, # Keep for consistency, PyGOD uses 'gpu' param\n",
    "    seed: int,\n",
    "    # --- DOMINANT Specific Parameters ---\n",
    "    dominant_epochs: int = 200,\n",
    "    # Add other DOMINANT params if needed (e.g., num_layers, weight_decay)\n",
    "    k_list_eval: List[int] = [50, 100, 200, 500]\n",
    "    ) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Trains and evaluates the DOMINANT model for a single iteration's data splits.\n",
    "\n",
    "    Args:\n",
    "        iteration_data (Dict): Contains graphs and labels for one seed.\n",
    "        device: Target device (PyGOD uses gpu=-1 for CPU, 0 for GPU 0, etc.).\n",
    "        seed (int): Random seed for logging/consistency (DOMINANT doesn't have random_state).\n",
    "        dominant_epochs (int): Number of training epochs for DOMINANT.\n",
    "        k_list_eval (List[int]): K values for metric calculation.\n",
    "\n",
    "    Returns:\n",
    "        Optional[pd.DataFrame]: DataFrame with evaluation metrics for DOMINANT, or None if error.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        # 1. Extract graphs and labels\n",
    "        train_graph_hetero = iteration_data['train_graph']\n",
    "        val_graph_hetero = iteration_data['val_graph']\n",
    "        test_graph_hetero = iteration_data['test_graph']\n",
    "        gt_node_labels_all_splits = iteration_data['gt_node_labels']\n",
    "\n",
    "        eval_summary_iter = []\n",
    "        pygod_device_id = 0 if device.type == 'cuda' else (-1 if device.type == 'cpu' else -1) # Map to PyGOD convention\n",
    "\n",
    "        # 2. Prepare Homogeneous Data\n",
    "        # Important: PyGOD models often expect data on CPU for fitting/predicting\n",
    "        # but can use GPU internally if specified. Let's prepare on CPU.\n",
    "        train_g_homo = prepare_homogeneous_data(train_graph_hetero, target_device=torch.device('cpu'))\n",
    "        val_g_homo = prepare_homogeneous_data(val_graph_hetero, target_device=torch.device('cpu'))\n",
    "        test_g_homo = prepare_homogeneous_data(test_graph_hetero, target_device=torch.device('cpu'))\n",
    "\n",
    "        if train_g_homo is None or not hasattr(train_g_homo, 'x') or train_g_homo.x is None:\n",
    "            print(f\"  ERROR: Failed to prepare valid homogeneous training data for DOMINANT (seed {seed}). Skipping iteration.\")\n",
    "            return None\n",
    "\n",
    "        # 3. Estimate Contamination\n",
    "        total_train_nodes = train_g_homo.num_nodes\n",
    "        total_train_anomalies = sum(gt_node_labels_all_splits['train'][nt].sum().item() for nt in gt_node_labels_all_splits['train'] if nt in train_graph_hetero.node_types)\n",
    "        contamination_est = total_train_anomalies / total_train_nodes if total_train_nodes > 0 else 0.1\n",
    "        contamination_est = max(0.001, min(0.5, contamination_est)) # Ensure in valid range\n",
    "\n",
    "        # 4. Instantiate and Train DOMINANT\n",
    "        print(f\"  Training DOMINANT for iteration {seed} (Contamination: {contamination_est:.4f})...\")\n",
    "        model_dominant = DOMINANT(\n",
    "            epoch=dominant_epochs,\n",
    "            contamination=contamination_est,\n",
    "            gpu=pygod_device_id, # Pass mapped device ID\n",
    "            verbose=0 # Keep logs clean\n",
    "        )\n",
    "        start_fit = time.time()\n",
    "        model_dominant.fit(train_g_homo)\n",
    "        end_fit = time.time()\n",
    "        print(f\"    Fit completed in {end_fit - start_fit:.2f}s.\")\n",
    "\n",
    "        # 5. Evaluate on all splits\n",
    "        print(f\"  Evaluating DOMINANT for iteration {seed}...\")\n",
    "        for split_name, hetero_graph, homo_graph in [('train', train_graph_hetero, train_g_homo),\n",
    "                                                    ('val', val_graph_hetero, val_g_homo),\n",
    "                                                    ('test', test_graph_hetero, test_g_homo)]:\n",
    "            if homo_graph is None or homo_graph.num_nodes == 0:\n",
    "                print(f\"    Skipping evaluation for {split_name}: Homogeneous graph missing or empty.\")\n",
    "                continue\n",
    "            if not hasattr(homo_graph, 'x') or homo_graph.x is None:\n",
    "                 print(f\"    Skipping evaluation for {split_name}: Homogeneous graph missing features 'x'.\")\n",
    "                 continue\n",
    "            # DOMINANT also needs edge_index, check if it exists\n",
    "            if not hasattr(homo_graph, 'edge_index') or homo_graph.edge_index is None:\n",
    "                print(f\"    Warning: Evaluating DOMINANT on {split_name} graph with no 'edge_index'. May be unreliable.\")\n",
    "                # Create empty edge_index if missing and nodes exist, needed by PyGOD\n",
    "                if homo_graph.num_nodes > 0:\n",
    "                     homo_graph.edge_index = torch.empty((2,0), dtype=torch.long, device=homo_graph.x.device)\n",
    "                else: continue # Skip if no nodes either\n",
    "\n",
    "\n",
    "            # Get scores (higher score = more anomalous in PyGOD)\n",
    "            homo_scores_split = model_dominant.decision_function(homo_graph)\n",
    "\n",
    "            # Map scores back to original node types\n",
    "            gt_labels_split = gt_node_labels_all_splits.get(split_name, {})\n",
    "            mapped_results = map_homo_results(homo_graph, homo_scores_split, hetero_graph, gt_labels_split)\n",
    "\n",
    "            # Calculate metrics per node type\n",
    "            for node_type, type_data in mapped_results.items():\n",
    "                scores_raw = type_data.get('scores')\n",
    "                labels_raw = type_data.get('labels')\n",
    "                \n",
    "                if scores_raw is not None and isinstance(scores_raw, torch.Tensor):\n",
    "                    scores_np = scores_raw.cpu().numpy()\n",
    "                elif scores_raw is not None: # Assume it's already numpy or list-like\n",
    "                    scores_np = np.asarray(scores_raw)\n",
    "                else:\n",
    "                    scores_np = np.array([])\n",
    "\n",
    "                if labels_raw is not None and isinstance(labels_raw, torch.Tensor):\n",
    "                    labels_np = labels_raw.cpu().numpy()\n",
    "                elif labels_raw is not None: # Assume it's already numpy or list-like\n",
    "                    labels_np = np.asarray(labels_raw)\n",
    "                else:\n",
    "                    labels_np = np.array([])\n",
    "\n",
    "                if scores_np is not None and labels_np is not None and \\\n",
    "                   len(scores_np) > 0 and len(labels_np) > 0 and \\\n",
    "                   len(scores_np) == len(labels_np):\n",
    "\n",
    "                    metrics = compute_evaluation_metrics(scores_np, labels_np, k_list=k_list_eval)\n",
    "\n",
    "                    summary_row = {\n",
    "                        'Model': 'DOMINANT',\n",
    "                        'Split': split_name,\n",
    "                        'Element': f'Node ({node_type})',\n",
    "                        'Num Items': len(scores_np),\n",
    "                        'Num Anomalies': int(np.sum(labels_np)),\n",
    "                        '% Anomalies': (np.sum(labels_np) / len(scores_np) * 100) if len(scores_np) > 0 else 0,\n",
    "                        'seed': seed,\n",
    "                        'iteration': seed\n",
    "                    }\n",
    "                    summary_row.update(metrics)\n",
    "                    eval_summary_iter.append(summary_row)\n",
    "                # else:\n",
    "                #     print(f\"    Skipping metric calculation for {node_type} in {split_name} (invalid scores/labels).\")\n",
    "\n",
    "    except ImportError:\n",
    "        print(\"Error: pygod library not found. Please install it (`pip install pygod`) to run DOMINANT.\")\n",
    "        return None\n",
    "    except Exception as e:\n",
    "        print(f\"  ERROR processing DOMINANT for iteration {seed}: {e}\")\n",
    "        return None # Indicate failure for this iteration\n",
    "\n",
    "    # Convert results for this iteration to DataFrame\n",
    "    if eval_summary_iter:\n",
    "        iter_results_df = pd.DataFrame(eval_summary_iter)\n",
    "        return iter_results_df\n",
    "    else:\n",
    "        return pd.DataFrame() # Return empty if no results\n",
    "\n",
    "\n",
    "all_dominant_summaries = []\n",
    "for seed, iter_data in DATA_SPLITS_ITERATIONS.items():\n",
    "    print(f\"\\n===== Running DOMINANT Iteration for Seed: {seed} =====\")\n",
    "    dominant_summary_df_iter = run_dominant_iteration(\n",
    "         iteration_data=iter_data,\n",
    "         device=device, # Pass target device\n",
    "         seed=seed,\n",
    "         dominant_epochs=50, # Example epoch count\n",
    "         k_list_eval=[50, 100, 200, 500]\n",
    "     )\n",
    "    if dominant_summary_df_iter is not None:\n",
    "        all_dominant_summaries.append(dominant_summary_df_iter)\n",
    "#\n",
    "# # Aggregate DOMINANT results after the loop\n",
    "# if all_dominant_summaries:\n",
    "#     final_dominant_summary_df = pd.concat(all_dominant_summaries, ignore_index=True)\n",
    "#     # ... (Perform groupby/aggregation similar to BGAE/IF results) ...\n",
    "#     print(\"\\n--- Aggregated DOMINANT Performance ---\")\n",
    "#     # print(aggregated_dominant_print)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_dominant_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "\n",
    "\n",
    "def aggregate_and_compare_model_results(\n",
    "    bgae_results_list: List[pd.DataFrame],\n",
    "    if_results_list: List[pd.DataFrame],\n",
    "    oddball_results_list: List[pd.DataFrame],\n",
    "    dominant_results_list: List[pd.DataFrame],\n",
    "    metrics_to_compare: List[str] = ['AUROC', 'AP', 'Best F1'],\n",
    "    k_list_eval: List[int] = [50, 100, 200, 500], # K values used during evaluation\n",
    "    splits_to_compare: List[str] = ['test'] # Focus on test set by default\n",
    "    ) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Aggregates results from multiple runs for different models and creates\n",
    "    a comparison DataFrame showing mean ± std dev for specified metrics.\n",
    "\n",
    "    Args:\n",
    "        bgae_results_list: List of summary DataFrames from BGAE runs.\n",
    "        if_results_list: List of summary DataFrames from Isolation Forest runs.\n",
    "        oddball_results_list: List of summary DataFrames from OddBall runs.\n",
    "        dominant_results_list: List of summary DataFrames from DOMINANT runs.\n",
    "        metrics_to_compare: List of primary performance metrics to show in the table.\n",
    "        k_list_eval: List of K values used, to include relevant P@K/R@K metrics.\n",
    "        splits_to_compare: List of data splits to include in the comparison (e.g., ['test'], ['val', 'test']).\n",
    "\n",
    "    Returns:\n",
    "        Optional[pd.DataFrame]: A DataFrame comparing models, indexed by Split and Element,\n",
    "                                 with columns for each model showing 'mean ± std' metrics,\n",
    "                                 or None if aggregation fails.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Aggregating and Comparing Model Performances ---\")\n",
    "\n",
    "    all_results_list = []\n",
    "    model_data_map = {\n",
    "        \"BGAE\": bgae_results_list,\n",
    "        \"IsolationForest\": if_results_list,\n",
    "        \"OddBall\": oddball_results_list,\n",
    "        \"DOMINANT\": dominant_results_list\n",
    "    }\n",
    "\n",
    "    # --- 1. Concatenate and Preprocess Results for Each Model ---\n",
    "    processed_model_dfs = {}\n",
    "    metrics_to_check = ['AUROC', 'AP', 'Best F1'] # Ensure these core metrics exist\n",
    "\n",
    "    for model_name, results_list in model_data_map.items():\n",
    "        if not results_list:\n",
    "            print(f\"Warning: No results found for model '{model_name}'. Skipping.\")\n",
    "            continue\n",
    "        try:\n",
    "            full_df = pd.concat(results_list, ignore_index=True)\n",
    "            # Filter for relevant splits\n",
    "            filtered_df = full_df[full_df['Split'].isin(splits_to_compare)].copy()\n",
    "            # Drop rows where essential metrics are NaN (likely failed runs)\n",
    "            cols_exist = all(m in filtered_df.columns for m in metrics_to_check)\n",
    "            if cols_exist:\n",
    "                 successful_df = filtered_df.dropna(subset=metrics_to_check, how='any')\n",
    "            else:\n",
    "                 print(f\"Warning: Core metrics missing for {model_name}. Proceeding with available data.\")\n",
    "                 successful_df = filtered_df\n",
    "\n",
    "            if successful_df.empty:\n",
    "                 print(f\"Warning: No successful runs found for model '{model_name}' in specified splits. Skipping.\")\n",
    "                 continue\n",
    "\n",
    "            processed_model_dfs[model_name] = successful_df\n",
    "            print(f\"Processed {model_name}: Found {successful_df['seed'].nunique()} successful runs in specified splits.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing results for model '{model_name}': {e}\")\n",
    "\n",
    "    if not processed_model_dfs:\n",
    "        print(\"Error: No processed results available for any model. Cannot create comparison.\")\n",
    "        return None\n",
    "\n",
    "    # --- 2. Aggregate Metrics for Each Model ---\n",
    "    aggregated_data_list = []\n",
    "    full_metric_list = metrics_to_compare + \\\n",
    "                       [f'{p}@{k}' for k in k_list_eval for p in ['Precision', 'Recall']]\n",
    "\n",
    "    for model_name, df in processed_model_dfs.items():\n",
    "        # Ensure metrics to aggregate actually exist in this df\n",
    "        metrics_present = [m for m in full_metric_list if m in df.columns]\n",
    "        if not metrics_present:\n",
    "            print(f\"Warning: No specified metrics found for model '{model_name}'. Skipping aggregation.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Group by Split and Element, calculate mean and std\n",
    "            agg_df = df.groupby(['Split', 'Element'])[metrics_present].agg(['mean', 'std'])\n",
    "            # Add run count\n",
    "            run_counts = df.groupby(['Split', 'Element'])['seed'].nunique().rename('n_runs')\n",
    "            agg_df = pd.concat([run_counts, agg_df], axis=1)\n",
    "            agg_df['Model'] = model_name # Add model identifier\n",
    "            aggregated_data_list.append(agg_df.reset_index()) # Reset index for easier merging later\n",
    "        except Exception as e:\n",
    "            print(f\"Error aggregating data for model '{model_name}': {e}\")\n",
    "\n",
    "    if not aggregated_data_list:\n",
    "        print(\"Error: Aggregation failed for all models.\")\n",
    "        return None\n",
    "\n",
    "    # --- 3. Combine and Format the Comparison Table ---\n",
    "    comparison_df = pd.concat(aggregated_data_list, ignore_index=True)\n",
    "\n",
    "    # Pivot table for comparison view\n",
    "    try:\n",
    "        pivot_df = comparison_df.pivot_table(\n",
    "            index=['Split', 'Element'],\n",
    "            columns='Model'\n",
    "            # Values will be selected below\n",
    "        )\n",
    "    except Exception as e:\n",
    "         print(f\"Error pivoting comparison table: {e}\")\n",
    "         return None # Cannot proceed if pivot fails\n",
    "\n",
    "\n",
    "    # Format columns as 'Metric (Mean ± Std)'\n",
    "    final_comparison_cols = {} # Store formatted columns keyed by original metric name\n",
    "    n_runs_cols = {} # Store n_runs separately\n",
    "\n",
    "    # Ensure metrics_present considers columns available across *all* processed models if needed,\n",
    "    # or handle missing columns per model during formatting. Let's handle per model.\n",
    "    all_metrics_in_pivot = set(lvl0 for lvl0, lvl1 in pivot_df.columns if lvl1=='mean') & set(metrics_to_compare + [f'{p}@{k}' for k in k_list_eval for p in ['Precision', 'Recall']])\n",
    "\n",
    "\n",
    "    for metric in all_metrics_in_pivot:\n",
    "        metric_mean_cols = [col for col in pivot_df.columns if col[0] == metric and col[1] == 'mean']\n",
    "        metric_std_cols = [col for col in pivot_df.columns if col[0] == metric and col[1] == 'std']\n",
    "\n",
    "        # Create the formatted column for this metric\n",
    "        formatted_series_list = []\n",
    "        models_in_pivot = pivot_df.columns.get_level_values('Model').unique() # Models actually present\n",
    "\n",
    "        for model_name in models_in_pivot:\n",
    "             mean_col = (metric, 'mean', model_name)\n",
    "             std_col = (metric, 'std', model_name)\n",
    "\n",
    "             if mean_col in pivot_df.columns and std_col in pivot_df.columns:\n",
    "                 # Format only if both mean and std are present and not NaN\n",
    "                 formatted = pivot_df[mean_col].map('{:.4f}'.format).str.cat(\n",
    "                               pivot_df[std_col].map('{:.4f}'.format), sep=' ± '\n",
    "                           ).where(pivot_df[mean_col].notna() & pivot_df[std_col].notna(), \"N/A\") # Handle NaN after agg\n",
    "             elif mean_col in pivot_df.columns:\n",
    "                  # Format mean only if std is missing/NaN\n",
    "                  formatted = pivot_df[mean_col].map('{:.4f}'.format).where(pivot_df[mean_col].notna(), \"N/A\") + ' ± nan'\n",
    "             else:\n",
    "                  formatted = pd.Series(\"N/A\", index=pivot_df.index) # Metric not found for this model\n",
    "\n",
    "             formatted.name = (metric, model_name) # Assign multi-level name\n",
    "             formatted_series_list.append(formatted)\n",
    "\n",
    "        # Combine formatted series for this metric across all models\n",
    "        if formatted_series_list:\n",
    "             final_comparison_cols[metric] = pd.concat(formatted_series_list, axis=1)\n",
    "\n",
    "\n",
    "    # Get n_runs separately\n",
    "    n_runs_cols_present = [col for col in pivot_df.columns if col[0] == 'n_runs' and col[1] == ''] # n_runs has empty second level\n",
    "    if n_runs_cols_present:\n",
    "        n_runs_df = pivot_df[n_runs_cols_present]\n",
    "        n_runs_df.columns = n_runs_df.columns.droplevel(1) # Drop the empty level ''\n",
    "        n_runs_cols = {'n_runs': n_runs_df}\n",
    "\n",
    "\n",
    "    # Assemble the final table\n",
    "    if not final_comparison_cols:\n",
    "        print(\"Error: No metric columns could be formatted.\")\n",
    "        return None\n",
    "\n",
    "    # Create MultiIndex columns for the final DataFrame\n",
    "    metric_order = metrics_to_compare + sorted([m for m in all_metrics_in_pivot if m not in metrics_to_compare]) # Order metrics\n",
    "    models_order = sorted(processed_model_dfs.keys()) # Sort model names alphabetically\n",
    "\n",
    "    final_df_cols = pd.MultiIndex.from_product([metric_order, models_order], names=['Metric', 'Model'])\n",
    "    final_comparison_df = pd.DataFrame(index=pivot_df.index, columns=final_df_cols)\n",
    "\n",
    "    # Fill the DataFrame with formatted values\n",
    "    for metric in metric_order:\n",
    "         if metric in final_comparison_cols:\n",
    "             metric_data = final_comparison_cols[metric]\n",
    "             # Ensure columns align - reindex metric_data if necessary\n",
    "             final_comparison_df[metric] = metric_data.reindex(columns=final_df_cols.get_level_values('Model').unique(), level='Model')\n",
    "\n",
    "    # Add n_runs as the first level\n",
    "    if n_runs_cols:\n",
    "        final_comparison_df = pd.concat(n_runs_cols, axis=1, keys=['Info']).join(final_comparison_df)\n",
    "        # Rename 'n_runs' column under 'Info'\n",
    "        final_comparison_df.rename(columns={'n_runs': 'Runs'}, level=1, inplace=True)\n",
    "\n",
    "\n",
    "    print(\"\\n--- Final Model Comparison (Mean ± Std Dev) ---\")\n",
    "    print(final_comparison_df.to_string())\n",
    "\n",
    "    return final_comparison_df\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "\n",
    "# Assume compute_evaluation_metrics is defined elsewhere or imported\n",
    "\n",
    "def aggregate_and_compare_model_results(\n",
    "    bgae_results_list: List[pd.DataFrame],\n",
    "    if_results_list: List[pd.DataFrame],\n",
    "    oddball_results_list: List[pd.DataFrame],\n",
    "    dominant_results_list: List[pd.DataFrame],\n",
    "    metrics_to_compare: List[str] = ['AUROC', 'AP', 'Best F1'],\n",
    "    k_list_eval: List[int] = [50, 100, 200, 500], # K values used during evaluation\n",
    "    splits_to_compare: List[str] = ['test'], # Focus on test set by default\n",
    "    sort_elements_by: Optional[List[str]] = None # Optional order for elements in index\n",
    "    ) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Aggregates results from multiple runs for different models and creates\n",
    "    a comparison DataFrame showing mean ± std dev for specified metrics.\n",
    "\n",
    "    Args:\n",
    "        bgae_results_list: List of summary DataFrames from BGAE runs.\n",
    "        if_results_list: List of summary DataFrames from Isolation Forest runs.\n",
    "        oddball_results_list: List of summary DataFrames from OddBall runs.\n",
    "        dominant_results_list: List of summary DataFrames from DOMINANT runs.\n",
    "        metrics_to_compare: List of primary performance metrics to show in the table.\n",
    "        k_list_eval: List of K values used, to include relevant P@K/R@K metrics.\n",
    "        splits_to_compare: List of data splits to include in the comparison (e.g., ['test'], ['val', 'test']).\n",
    "        sort_elements_by (Optional[List[str]]): Specific order for rows based on 'Element' column.\n",
    "\n",
    "    Returns:\n",
    "        Optional[pd.DataFrame]: A DataFrame comparing models, indexed by Split and Element,\n",
    "                                 with columns for each model showing 'mean ± std' metrics,\n",
    "                                 or None if aggregation fails.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Aggregating and Comparing Model Performances ---\")\n",
    "\n",
    "    model_data_map = {\n",
    "        \"BGAE\": bgae_results_list,\n",
    "        \"IsolationForest\": if_results_list,\n",
    "        \"OddBall\": oddball_results_list,\n",
    "        \"DOMINANT\": dominant_results_list\n",
    "    }\n",
    "\n",
    "    # --- 1. Combine all results into a single DataFrame ---\n",
    "    all_results_list = []\n",
    "    for model_name, results_list in model_data_map.items():\n",
    "        if results_list:\n",
    "            try:\n",
    "                model_df = pd.concat(results_list, ignore_index=True)\n",
    "                # Ensure essential columns exist, add 'Model' column\n",
    "                if not model_df.empty:\n",
    "                    model_df['Model'] = model_name\n",
    "                    all_results_list.append(model_df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error concatenating results for {model_name}: {e}\")\n",
    "\n",
    "    if not all_results_list:\n",
    "        print(\"Error: No valid result lists provided or concatenation failed.\")\n",
    "        return None\n",
    "\n",
    "    combined_df = pd.concat(all_results_list, ignore_index=True)\n",
    "\n",
    "    # --- 2. Filter and Clean ---\n",
    "    filtered_df = combined_df[combined_df['Split'].isin(splits_to_compare)].copy()\n",
    "\n",
    "    metrics_to_check = ['AUROC', 'AP', 'Best F1'] # Core metrics for filtering failed runs\n",
    "    cols_exist = all(m in filtered_df.columns for m in metrics_to_check)\n",
    "    if cols_exist:\n",
    "        successful_df = filtered_df.dropna(subset=metrics_to_check, how='any')\n",
    "    else:\n",
    "        print(\"Warning: Core metric columns missing. Filtering might be incomplete.\")\n",
    "        successful_df = filtered_df\n",
    "\n",
    "    if successful_df.empty:\n",
    "        print(f\"Warning: No successful runs found for models in specified splits: {splits_to_compare}.\")\n",
    "        return pd.DataFrame() # Return empty DataFrame\n",
    "\n",
    "    print(f\"Aggregating results from {successful_df['seed'].nunique()} successful runs across models...\")\n",
    "\n",
    "    # --- 3. Calculate Aggregates (Mean, Std, Count) ---\n",
    "    # Define all metrics we might want to aggregate\n",
    "    all_metrics_possible = metrics_to_compare + \\\n",
    "                           [f'{p}@{k}' for k in k_list_eval for p in ['Precision', 'Recall']]\n",
    "    # Filter list to only those metrics actually present in the successful data\n",
    "    metrics_present = [m for m in all_metrics_possible if m in successful_df.columns]\n",
    "\n",
    "    if not metrics_present:\n",
    "         print(\"Error: None of the specified metrics_to_compare or P@K/R@K metrics found in the data.\")\n",
    "         return None\n",
    "\n",
    "    try:\n",
    "        # Group by Split, Element, and Model then aggregate\n",
    "        grouped = successful_df.groupby(['Split', 'Element', 'Model'])\n",
    "        aggregated_means = grouped[metrics_present].mean()\n",
    "        aggregated_stds = grouped[metrics_present].std()\n",
    "        aggregated_counts = grouped['seed'].nunique().rename('n_runs') # Count distinct runs per group\n",
    "\n",
    "        # Combine mean, std, and counts\n",
    "        aggregated_df = aggregated_means.join(aggregated_stds, lsuffix='_mean', rsuffix='_std')\n",
    "        aggregated_df = aggregated_df.join(aggregated_counts)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during aggregation: {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- 4. Format the Comparison Table ---\n",
    "    final_df_list = []\n",
    "    # Use index from aggregated_df to handle multi-level index easily\n",
    "    for idx, row in aggregated_df.iterrows():\n",
    "        split, element, model = idx # Unpack the index levels\n",
    "        formatted_row = {'Split': split, 'Element': element, 'Model': model, 'Runs': int(row['n_runs'])}\n",
    "        for metric in metrics_present:\n",
    "            mean_val = row.get(f'{metric}_mean', np.nan)\n",
    "            std_val = row.get(f'{metric}_std', np.nan)\n",
    "\n",
    "            if pd.notna(mean_val) and pd.notna(std_val):\n",
    "                formatted_row[metric] = f\"{mean_val:.4f} ± {std_val:.4f}\"\n",
    "            elif pd.notna(mean_val):\n",
    "                formatted_row[metric] = f\"{mean_val:.4f} ± nan\"\n",
    "            else:\n",
    "                formatted_row[metric] = \"N/A\" # Or np.nan if preferred\n",
    "        final_df_list.append(formatted_row)\n",
    "\n",
    "    if not final_df_list:\n",
    "        print(\"Error: No data after formatting.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    final_comparison_df_long = pd.DataFrame(final_df_list)\n",
    "\n",
    "    # Pivot to get models as columns\n",
    "    try:\n",
    "        # Define the order of metrics for columns\n",
    "        column_order = ['Runs'] + metrics_present\n",
    "        pivot_comparison_df = final_comparison_df_long.pivot_table(\n",
    "            index=['Split', 'Element'],\n",
    "            columns='Model',\n",
    "            values=column_order # Specify columns to pivot as values\n",
    "        )\n",
    "        # Reorder model columns alphabetically for consistency\n",
    "        pivot_comparison_df = pivot_comparison_df.reindex(columns=sorted(pivot_comparison_df.columns.levels[1]), level='Model')\n",
    "        # Reorder metric level for readability\n",
    "        pivot_comparison_df = pivot_comparison_df.reindex(columns=column_order, level=0)\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error pivoting final table: {e}\")\n",
    "        print(\"Returning table in long format instead.\")\n",
    "        # Set index for long format return\n",
    "        final_comparison_df_long = final_comparison_df_long.set_index(['Split', 'Element', 'Model'])\n",
    "        return final_comparison_df_long # Return long format as fallback\n",
    "\n",
    "\n",
    "    # --- Optional: Sort index by custom element order ---\n",
    "    if sort_elements_by and isinstance(pivot_comparison_df.index, pd.MultiIndex):\n",
    "         try:\n",
    "             # Sort by Split first, then by custom Element order\n",
    "             pivot_comparison_df = pivot_comparison_df.sort_index(\n",
    "                 level='Element',\n",
    "                 key=lambda index: index.map({elem: i for i, elem in enumerate(sort_elements_by)}),\n",
    "                 sort_remaining=True # Sorts by Split automatically after sorting by Element key\n",
    "             )\n",
    "         except Exception as e:\n",
    "              print(f\"Warning: Could not sort by custom element order: {e}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Final Model Comparison (Mean ± Std Dev) ---\")\n",
    "    # Configure pandas display options for better table printing\n",
    "    pd.set_option('display.max_rows', 500)\n",
    "    pd.set_option('display.max_columns', len(pivot_comparison_df.columns) + 1) # Adjust based on columns\n",
    "    pd.set_option('display.width', 200) # Adjust width as needed\n",
    "    pd.set_option('display.precision', 4) # Set default float precision\n",
    "\n",
    "    print(pivot_comparison_df.to_string())\n",
    "\n",
    "    # Reset display options if desired\n",
    "    # pd.reset_option('all')\n",
    "\n",
    "    return pivot_comparison_df\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Assume all_summary_dfs, all_if_summaries, all_oddball_summaries, all_dominant_summaries\n",
    "# are lists of DataFrames populated from the previous iteration loops.\n",
    "\n",
    "# Example element order (adjust based on your actual 'Element' column values)\n",
    "# element_order = [\n",
    "#     'Node (provider)',\n",
    "#     'Node (member)',\n",
    "#     \"Edge ('provider', 'to', 'member')\" # Example edge element name\n",
    "# ]\n",
    "\n",
    "# comparison_table = aggregate_and_compare_model_results(\n",
    "#     bgae_results_list = all_summary_dfs,\n",
    "#     if_results_list = all_if_summaries,\n",
    "#     oddball_results_list = all_oddball_summaries,\n",
    "#     dominant_results_list = all_dominant_summaries,\n",
    "#     metrics_to_compare = ['AUROC', 'AP', 'Best F1'], # Primary metrics\n",
    "#     k_list_eval = [50, 100, 200, 500], # K values used\n",
    "#     splits_to_compare = ['test'], # Focus on test set\n",
    "#     sort_elements_by = element_order # Optional sorting\n",
    "# )\n",
    "\n",
    "# if comparison_table is not None:\n",
    "#      print(\"\\nComparison Table Created:\")\n",
    "#      # print(comparison_table.to_markdown()) # For easy viewing if needed\n",
    "\n",
    "#      # Optional: Save to CSV\n",
    "#      # save_dir = \"final_evaluation_results_iter\"\n",
    "#      # if save_dir:\n",
    "#      #     os.makedirs(save_dir, exist_ok=True)\n",
    "#      #     comparison_table.to_csv(os.path.join(save_dir, \"model_comparison_summary.csv\"))\n",
    "\n",
    "comparison_table = aggregate_and_compare_model_results(\n",
    "     bgae_results_list = all_summary_dfs,\n",
    "     if_results_list = all_if_summaries,\n",
    "     oddball_results_list = all_oddball_summaries,\n",
    "     dominant_results_list = all_dominant_summaries,\n",
    "     metrics_to_compare = ['AUROC', 'AP', 'Best F1'], # Choose primary metrics\n",
    "     k_list_eval = [50, 100, 200, 500], # K values used\n",
    "     splits_to_compare = ['test'] # Focus on test set\n",
    " )\n",
    "\n",
    "# if comparison_table is not None:\n",
    "#      print(\"\\nComparison Table Created:\")\n",
    "#      print(comparison_table.to_markdown()) # Print markdown for easy viewing\n",
    "\n",
    "#      # Optional: Save to CSV\n",
    "#      # save_dir = \"final_evaluation_results_iter\"\n",
    "#      # if save_dir:\n",
    "#      #     os.makedirs(save_dir, exist_ok=True)\n",
    "#      #     comparison_table.to_csv(os.path.join(save_dir, \"model_comparison_summary.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "comparison_table"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import os\n",
    "import pickle\n",
    "from typing import Dict, List, Optional\n",
    "\n",
    "def save_iteration_results(\n",
    "    save_dir: str,\n",
    "    bgae_results_list: List[pd.DataFrame],\n",
    "    if_results_list: List[pd.DataFrame],\n",
    "    oddball_results_list: List[pd.DataFrame],\n",
    "    dominant_results_list: List[pd.DataFrame],\n",
    "    aggregated_comparison_df: Optional[pd.DataFrame] = None, # Optional final aggregated table\n",
    "    aggregated_bgae_summary: Optional[pd.DataFrame] = None, # Optional aggregated specific model tables\n",
    "    aggregated_if_summary: Optional[pd.DataFrame] = None,\n",
    "    aggregated_oddball_summary: Optional[pd.DataFrame] = None,\n",
    "    aggregated_dominant_summary: Optional[pd.DataFrame] = None,\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Saves the raw iteration results and optionally aggregated results as pickle files.\n",
    "\n",
    "    Args:\n",
    "        save_dir (str): The directory path where the pickle files will be saved.\n",
    "        bgae_results_list: List of summary DataFrames from BGAE runs.\n",
    "        if_results_list: List of summary DataFrames from Isolation Forest runs.\n",
    "        oddball_results_list: List of summary DataFrames from OddBall runs.\n",
    "        dominant_results_list: List of summary DataFrames from DOMINANT runs.\n",
    "        aggregated_comparison_df (Optional[pd.DataFrame]): The final comparison table.\n",
    "        aggregated_bgae_summary (Optional[pd.DataFrame]): Aggregated results for BGAE.\n",
    "        aggregated_if_summary (Optional[pd.DataFrame]): Aggregated results for IF.\n",
    "        aggregated_oddball_summary (Optional[pd.DataFrame]): Aggregated results for OddBall.\n",
    "        aggregated_dominant_summary (Optional[pd.DataFrame]): Aggregated results for DOMINANT.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Saving Iteration Results to Directory: {save_dir} ---\")\n",
    "\n",
    "    # --- Ensure Save Directory Exists ---\n",
    "    try:\n",
    "        os.makedirs(save_dir, exist_ok=True)\n",
    "    except OSError as e:\n",
    "        print(f\"Error: Could not create save directory '{save_dir}'. Cannot save results. Error: {e}\")\n",
    "        return\n",
    "\n",
    "    # --- Data to Save (Map model name to list of DataFrames) ---\n",
    "    data_to_save = {\n",
    "        \"bgae_iterations\": bgae_results_list,\n",
    "        \"if_iterations\": if_results_list,\n",
    "        \"oddball_iterations\": oddball_results_list,\n",
    "        \"dominant_iterations\": dominant_results_list,\n",
    "        \"aggregated_comparison\": aggregated_comparison_df, # Can be None\n",
    "        \"aggregated_bgae\": aggregated_bgae_summary,         # Can be None\n",
    "        \"aggregated_if\": aggregated_if_summary,           # Can be None\n",
    "        \"aggregated_oddball\": aggregated_oddball_summary,   # Can be None\n",
    "        \"aggregated_dominant\": aggregated_dominant_summary # Can be None\n",
    "    }\n",
    "\n",
    "    # --- Save Each Item as a Pickle File ---\n",
    "    for filename_base, data_object in data_to_save.items():\n",
    "        if data_object is None or (isinstance(data_object, list) and not data_object):\n",
    "            # print(f\"Skipping saving '{filename_base}.pkl': No data provided.\")\n",
    "            continue # Skip if data is None or an empty list\n",
    "\n",
    "        file_path = os.path.join(save_dir, f\"{filename_base}.pkl\")\n",
    "        try:\n",
    "            with open(file_path, 'wb') as f:\n",
    "                pickle.dump(data_object, f)\n",
    "            print(f\"Successfully saved: {file_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving '{file_path}': {e}\")\n",
    "\n",
    "    print(\"--- Results Saving Attempt Finished ---\")\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "\n",
    "# Assume you have run the iterations and aggregation, resulting in:\n",
    "# all_summary_dfs (list of BGAE DFs)\n",
    "# all_if_summaries (list of IF DFs)\n",
    "# all_oddball_summaries (list of OddBall DFs)\n",
    "# all_dominant_summaries (list of DOMINANT DFs)\n",
    "# comparison_table (final aggregated comparison DF from previous function)\n",
    "# aggregated_summary (example: aggregated results for BGAE only, if calculated separately)\n",
    "\n",
    "# Example data (replace with your actual variables)\n",
    "# all_summary_dfs = [pd.DataFrame({'A': [1,2]}), pd.DataFrame({'A': [3,4]})]\n",
    "# all_if_summaries = [pd.DataFrame({'B': [5,6]})]\n",
    "# all_oddball_summaries = [] # Example empty list\n",
    "# all_dominant_summaries = [pd.DataFrame({'D': [7,8]})]\n",
    "# comparison_table = pd.DataFrame({'Model': ['BGAE', 'IF'], 'Metric': [0.8, 0.7]}) # Example aggregated\n",
    "# aggregated_bgae_summary = pd.DataFrame({'Metric_mean': [0.8], 'Metric_std': [0.05]}) # Example\n",
    "\n",
    "#Define the directory to save files\n",
    "save_directory = \"model_evaluation_results_pickle\"\n",
    "\n",
    "#Call the save function\n",
    "save_iteration_results(\n",
    "    save_dir=save_directory,\n",
    "    bgae_results_list=all_summary_dfs,\n",
    "    if_results_list=all_if_summaries,\n",
    "    oddball_results_list=all_oddball_summaries,\n",
    "    dominant_results_list=all_dominant_summaries,\n",
    "    aggregated_comparison_df=comparison_table,\n",
    "    aggregated_bgae_summary=all_anomaly_type_dfs # Pass other aggregated DFs if you have them\n",
    "    # Pass None for aggregated DFs you haven't calculated or don't need to save separately\n",
    ")\n",
    "\n",
    "save_directory = \"model_evaluation_results_pickle\"\n",
    "file_to_load = os.path.join(save_directory, \"bgae_iterations.pkl\")\n",
    "if os.path.exists(file_to_load):\n",
    "    with open(file_to_load, 'rb') as f:\n",
    "        loaded_bgae_iterations = pickle.load(f)\n",
    "    print(f\"\\nLoaded {len(loaded_bgae_iterations)} BGAE iteration results.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "loaded_bgae_iterations"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "import os\n",
    "import math # For ceiling function\n",
    "\n",
    "# Set a suitable style for report plots\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"deep\", font_scale=1.1)\n",
    "\n",
    "def plot_metric_comparison_with_std(\n",
    "    aggregated_summary_df: pd.DataFrame, # DataFrame with MultiIndex columns ('Metric', 'Stat', 'Model') or flattened ('Metric_Stat')\n",
    "    models_to_plot: List[str], # List of models to include (e.g., ['BGAE', 'IsolationForest'])\n",
    "    metrics_to_plot: List[str] = ['AUROC', 'AP', 'Best F1'],\n",
    "    splits_to_plot: List[str] = ['test', 'train', 'val'], # Order matters for bars\n",
    "    elements_to_plot: Optional[List[str]] = None, # Specific elements/order for x-axis\n",
    "    plot_figsize: Tuple[int, int] = (20, 5.5), # Adjusted width for 3 plots\n",
    "    save_path: Optional[str] = None\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Generates a grouped bar plot comparing specified metrics across models, splits,\n",
    "    and element types, including standard deviation as error bars.\n",
    "    Replicates the structure of the user-provided plot example.\n",
    "\n",
    "    Args:\n",
    "        aggregated_summary_df (pd.DataFrame): DataFrame containing aggregated mean/std metrics.\n",
    "                                              Expects multi-index ('Split', 'Element') and columns\n",
    "                                              structured like ('Metric', 'Stat', 'Model') or flattened ('Metric_Stat').\n",
    "        models_to_plot (List[str]): List of model names (column level 'Model') to include.\n",
    "        metrics_to_plot (List[str]): List of metrics (column level 'Metric') to create subplots for.\n",
    "        splits_to_plot (List[str]): List of splits to group bars by (order determines bar order).\n",
    "        elements_to_plot (Optional[List[str]]): Ordered list of elements for the x-axis.\n",
    "                                                 If None, uses unique elements found in the data.\n",
    "        plot_figsize (Tuple[int, int]): Overall figure size.\n",
    "        save_path (Optional[str]): If provided, saves the figure to this path.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Generating Metric Comparison Plot with Std Dev ---\")\n",
    "\n",
    "    if aggregated_summary_df is None or aggregated_summary_df.empty:\n",
    "        print(\"Error: Aggregated summary DataFrame is empty. Cannot generate plot.\")\n",
    "        return\n",
    "\n",
    "    # --- Data Preparation ---\n",
    "    # Ensure index is ['Split', 'Element'] if not already\n",
    "    if not isinstance(aggregated_summary_df.index, pd.MultiIndex) or \\\n",
    "       aggregated_summary_df.index.names != ['Split', 'Element']:\n",
    "        try:\n",
    "            aggregated_summary_df = aggregated_summary_df.set_index(['Split', 'Element'])\n",
    "        except KeyError:\n",
    "            print(\"Error: DataFrame must contain 'Split' and 'Element' columns to set as index.\")\n",
    "            return\n",
    "\n",
    "    # Define element order for x-axis\n",
    "    if elements_to_plot is None:\n",
    "        elements_order = sorted(aggregated_summary_df.index.get_level_values('Element').unique())\n",
    "    else:\n",
    "        # Filter data to only include specified elements and maintain order\n",
    "        elements_order = [e for e in elements_to_plot if e in aggregated_summary_df.index.get_level_values('Element')]\n",
    "        if not elements_order:\n",
    "            print(f\"Error: None of the specified elements_to_plot found in the data.\")\n",
    "            return\n",
    "        # Keep only rows matching the desired elements\n",
    "        aggregated_summary_df = aggregated_summary_df[aggregated_summary_df.index.get_level_values('Element').isin(elements_order)]\n",
    "        # Reindex to enforce the desired order\n",
    "        current_splits = aggregated_summary_df.index.get_level_values('Split').unique()\n",
    "        new_index = pd.MultiIndex.from_product([current_splits, elements_order], names=['Split', 'Element'])\n",
    "        aggregated_summary_df = aggregated_summary_df.reindex(new_index)\n",
    "\n",
    "\n",
    "    # Define split order for bars within groups\n",
    "    splits_order = [s for s in splits_to_plot if s in aggregated_summary_df.index.get_level_values('Split')]\n",
    "    if not splits_order:\n",
    "        print(f\"Error: None of the specified splits_to_plot found in the data.\")\n",
    "        return\n",
    "\n",
    "\n",
    "    # --- Plotting Setup ---\n",
    "    n_metrics = len(metrics_to_plot)\n",
    "    n_splits = len(splits_order)\n",
    "    n_elements = len(elements_order)\n",
    "\n",
    "    fig, axes = plt.subplots(1, n_metrics, figsize=plot_figsize, sharey=True)\n",
    "    if n_metrics == 1: axes = [axes] # Ensure iterable\n",
    "\n",
    "    bar_width = 0.8 / n_splits # Adjust bar width based on number of splits\n",
    "    group_positions = np.arange(n_elements) # Center positions for each element group\n",
    "\n",
    "    # Use a predefined palette or generate one\n",
    "    palette = sns.color_palette(\"deep\", n_splits)\n",
    "    split_color_map = dict(zip(splits_order, palette))\n",
    "\n",
    "    # --- Create Plots ---\n",
    "    for i, metric in enumerate(metrics_to_plot):\n",
    "        ax = axes[i]\n",
    "        ax.set_title(metric)\n",
    "        ax.set_ylabel(\"Score\" if i == 0 else \"\") # Label only first y-axis\n",
    "        ax.set_xticks(group_positions)\n",
    "        ax.set_xticklabels(elements_order, rotation=0) # Use the determined element order\n",
    "        ax.grid(True, axis='y', linestyle=':', alpha=0.7)\n",
    "        ax.axhline(0, color='grey', linewidth=0.8) # Line at y=0\n",
    "\n",
    "        max_y = 0 # Track max y for ylim adjustment\n",
    "\n",
    "        # Loop through splits to plot grouped bars\n",
    "        for j, split in enumerate(splits_order):\n",
    "            # Calculate offset for this split's bar within the group\n",
    "            offset = (j - (n_splits - 1) / 2) * bar_width\n",
    "\n",
    "            # Extract mean and std for this metric, split, and model(s)\n",
    "            means = []\n",
    "            stds = []\n",
    "            bar_positions = group_positions + offset\n",
    "\n",
    "            for element in elements_order:\n",
    "                 mean_val = np.nan\n",
    "                 std_val = np.nan\n",
    "                 # Check if the specific index exists\n",
    "                 if (split, element) in aggregated_summary_df.index:\n",
    "                      # Iterate through models to find the metric\n",
    "                      for model_name in models_to_plot:\n",
    "                           mean_col = (metric, 'mean', model_name) if isinstance(aggregated_summary_df.columns, pd.MultiIndex) else f'{metric}_mean'\n",
    "                           std_col = (metric, 'std', model_name) if isinstance(aggregated_summary_df.columns, pd.MultiIndex) else f'{metric}_std'\n",
    "\n",
    "                           # Need to handle cases where stats might be missing for a specific model\n",
    "                           # This assumes we plot the *first* model found in models_to_plot that has data\n",
    "                           # A better approach might be needed if comparing multiple models side-by-side on the *same* plot\n",
    "                           if mean_col in aggregated_summary_df.columns:\n",
    "                                mean_val = aggregated_summary_df.loc[(split, element), mean_col]\n",
    "                                if std_col in aggregated_summary_df.columns:\n",
    "                                     std_val = aggregated_summary_df.loc[(split, element), std_col]\n",
    "                                else: std_val = 0 # Default std if missing\n",
    "                                break # Found data for this metric/element/split for *a* model\n",
    "\n",
    "                 means.append(mean_val if pd.notna(mean_val) else 0) # Plot 0 if NaN\n",
    "                 stds.append(std_val if pd.notna(std_val) else 0)   # Plot 0 error if NaN\n",
    "\n",
    "            # Plot bars and error bars for this split\n",
    "            ax.bar(bar_positions, means, bar_width, label=split, color=split_color_map[split],\n",
    "                   edgecolor='grey', linewidth=0.5)\n",
    "            ax.errorbar(bar_positions, means, yerr=stds, fmt='none', ecolor='black',\n",
    "                        capsize=3, elinewidth=1, capthick=1)\n",
    "\n",
    "            # Update max y value encountered\n",
    "            current_max = np.nanmax(np.array(means) + np.array(stds))\n",
    "            if pd.notna(current_max) and current_max > max_y:\n",
    "                max_y = current_max\n",
    "\n",
    "        ax.legend(title=\"Split\")\n",
    "\n",
    "    # --- Final Touches ---\n",
    "    # Adjust ylim for all subplots consistently\n",
    "    common_ylim_top = math.ceil(max_y * 11) / 10.0 # Add 10% buffer and ceil\n",
    "    for ax in axes:\n",
    "        ax.set_ylim(bottom=-0.05, top=min(1.05, common_ylim_top)) # Cap at 1.05 for typical metrics\n",
    "\n",
    "    fig.suptitle(\"Metric Comparison Across Splits\", fontsize=16, y=1.05) # Adjust title position\n",
    "    plt.tight_layout(rect=[0, 0, 1, 1]) # Adjust layout\n",
    "\n",
    "    # Save plot if directory specified\n",
    "    if save_path:\n",
    "        try:\n",
    "            save_dir = os.path.dirname(save_path)\n",
    "            if save_dir: os.makedirs(save_dir, exist_ok=True)\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Comparison plot saved to {save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving comparison plot: {e}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Assume 'aggregated_summary' is the DataFrame returned by the previous aggregation code\n",
    "# It should have ('Split', 'Element') as index and MultiIndex columns like ('AUROC','mean','BGAE'), ('AUROC','std','BGAE'), etc.\n",
    "\n",
    "# Example DataFrame structure (replace with your actual aggregated_summary)\n",
    "# idx = pd.MultiIndex.from_product([['test', 'train', 'val'],\n",
    "#                                   [\"Edge ('provider', 'to', 'member')\", \"Node (member)\", \"Node (provider)\"]],\n",
    "#                                  names=['Split', 'Element'])\n",
    "# cols = pd.MultiIndex.from_product([['AUROC', 'AP', 'Best F1'], ['mean', 'std'], ['BGAE']], names=['Metric', 'Stat', 'Model'])\n",
    "# aggregated_summary_example = pd.DataFrame(np.random.rand(9, 6) * 0.6 + 0.2, index=idx, columns=cols)\n",
    "# aggregated_summary_example[('n_runs', '', 'BGAE')] = 3 # Add n_runs column\n",
    "\n",
    "# Define the exact element names as they appear in your DataFrame index\n",
    "element_order_example = [\n",
    "     \"Edge ('provider', 'to', 'member')\", # Match exact string from your data\n",
    "     \"Node (member)\",\n",
    "     \"Node (provider)\"\n",
    "]\n",
    "\n",
    "plot_metric_comparison_with_std(\n",
    "     aggregated_summary_df = final_summary_df, # Use your actual aggregated DataFrame\n",
    "     models_to_plot = ['BGAE'], # Plot only BGAE as in the example figure\n",
    "     metrics_to_plot = ['AUROC', 'AP', 'Best F1'],\n",
    "     splits_to_plot = ['test', 'train', 'val'], # Order bars: test, train, val\n",
    "     elements_to_plot = element_order_example, # Use the defined order for x-axis\n",
    "     save_path = \"report_plots/metric_comparison_splits_with_std.png\" # Example save path\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "import os\n",
    "import math\n",
    "\n",
    "# Set a suitable style for report plots\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"deep\", font_scale=1.1)\n",
    "\n",
    "def plot_metric_comparison_with_std(\n",
    "    aggregated_summary_df: pd.DataFrame, # DataFrame with MultiIndex columns ('Metric', 'Stat') or flattened ('Metric_Stat')\n",
    "    metrics_to_plot: List[str] = ['AUROC', 'AP', 'Best F1'],\n",
    "    splits_to_plot: List[str] = ['test', 'train', 'val'], # Order matters for bars\n",
    "    elements_to_plot: Optional[List[str]] = None, # Specific elements/order for x-axis\n",
    "    plot_figsize: Tuple[int, int] = (20, 5.5),\n",
    "    save_path: Optional[str] = None\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Generates a grouped bar plot comparing specified metrics across splits\n",
    "    and element types, including standard deviation as error bars.\n",
    "    Assumes input DataFrame index=['Split', 'Element'] and columns are MultiIndex\n",
    "    like ('Metric', 'Stat') or flattened like 'Metric_Stat'.\n",
    "\n",
    "    Args:\n",
    "        aggregated_summary_df (pd.DataFrame): Aggregated mean/std metrics.\n",
    "        metrics_to_plot (List[str]): Metrics to create subplots for.\n",
    "        splits_to_plot (List[str]): Splits to group bars by (order determines bar order).\n",
    "        elements_to_plot (Optional[List[str]]): Ordered list of elements for the x-axis.\n",
    "        plot_figsize (Tuple[int, int]): Overall figure size.\n",
    "        save_path (Optional[str]): If provided, saves the figure to this path.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Generating Metric Comparison Plot with Std Dev ---\")\n",
    "\n",
    "    if aggregated_summary_df is None or aggregated_summary_df.empty:\n",
    "        print(\"Error: Aggregated summary DataFrame is empty. Cannot generate plot.\")\n",
    "        return\n",
    "\n",
    "    # --- Data Preparation ---\n",
    "    # Ensure index is ['Split', 'Element']\n",
    "    if not isinstance(aggregated_summary_df.index, pd.MultiIndex) or \\\n",
    "       aggregated_summary_df.index.names != ['Split', 'Element']:\n",
    "        try:\n",
    "            # Check if 'Split' and 'Element' exist as columns first\n",
    "            if 'Split' in aggregated_summary_df.columns and 'Element' in aggregated_summary_df.columns:\n",
    "                 aggregated_summary_df = aggregated_summary_df.set_index(['Split', 'Element'])\n",
    "            else:\n",
    "                 # Try resetting index if they might already be the index but unnamed\n",
    "                 temp_df = aggregated_summary_df.reset_index()\n",
    "                 if 'Split' in temp_df.columns and 'Element' in temp_df.columns:\n",
    "                      aggregated_summary_df = temp_df.set_index(['Split', 'Element'])\n",
    "                 else:\n",
    "                      raise ValueError(\"DataFrame must have 'Split' and 'Element' as index or columns.\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error setting index: {e}\")\n",
    "            return\n",
    "\n",
    "    # Define element order for x-axis\n",
    "    available_elements = aggregated_summary_df.index.get_level_values('Element').unique()\n",
    "    if elements_to_plot is None:\n",
    "        elements_order = sorted(available_elements)\n",
    "    else:\n",
    "        elements_order = [e for e in elements_to_plot if e in available_elements]\n",
    "        if not elements_order:\n",
    "            print(f\"Error: None of the specified elements_to_plot found in the data: {available_elements}\")\n",
    "            return\n",
    "        # Filter and reindex to enforce order\n",
    "        current_splits = aggregated_summary_df.index.get_level_values('Split').unique()\n",
    "        new_index = pd.MultiIndex.from_product([current_splits, elements_order], names=['Split', 'Element'])\n",
    "        aggregated_summary_df = aggregated_summary_df.reindex(new_index)\n",
    "\n",
    "    # Define split order for bars\n",
    "    available_splits = aggregated_summary_df.index.get_level_values('Split').unique()\n",
    "    splits_order = [s for s in splits_to_plot if s in available_splits]\n",
    "    if not splits_order:\n",
    "        print(f\"Error: None of the specified splits_to_plot found in the data: {available_splits}\")\n",
    "        return\n",
    "\n",
    "    # --- Plotting Setup ---\n",
    "    n_metrics = len(metrics_to_plot)\n",
    "    n_splits = len(splits_order)\n",
    "    n_elements = len(elements_order)\n",
    "\n",
    "    fig, axes = plt.subplots(1, n_metrics, figsize=plot_figsize, sharey=True)\n",
    "    if n_metrics == 1: axes = [axes] # Ensure iterable\n",
    "\n",
    "    bar_width = 0.8 / n_splits\n",
    "    group_positions = np.arange(n_elements)\n",
    "    palette = sns.color_palette(\"deep\", n_splits)\n",
    "    split_color_map = dict(zip(splits_order, palette))\n",
    "\n",
    "    # --- Create Plots ---\n",
    "    max_y_overall = 0 # Track max y across all plots for consistent ylim\n",
    "\n",
    "    for i, metric in enumerate(metrics_to_plot):\n",
    "        ax = axes[i]\n",
    "        ax.set_title(metric)\n",
    "        ax.set_ylabel(\"Score\" if i == 0 else \"\")\n",
    "        ax.set_xticks(group_positions)\n",
    "        ax.set_xticklabels(elements_order, rotation=0, ha='center') # Center align x-labels\n",
    "        ax.grid(True, axis='y', linestyle=':', alpha=0.7)\n",
    "        ax.axhline(0, color='grey', linewidth=0.8)\n",
    "\n",
    "        plot_max_y = 0 # Track max y for this subplot\n",
    "\n",
    "        # Check if metric columns exist (either flattened or MultiIndex)\n",
    "        mean_col_flat = f'{metric}_mean'\n",
    "        std_col_flat = f'{metric}_std'\n",
    "        mean_col_multi = (metric, 'mean')\n",
    "        std_col_multi = (metric, 'std')\n",
    "\n",
    "        has_flat_cols = mean_col_flat in aggregated_summary_df.columns\n",
    "        has_multi_cols = mean_col_multi in aggregated_summary_df.columns\n",
    "\n",
    "        if not has_flat_cols and not has_multi_cols:\n",
    "             print(f\"Warning: Metric '{metric}' mean column not found. Skipping subplot.\")\n",
    "             ax.text(0.5, 0.5, f\"Data for\\n'{metric}'\\nnot found\", ha='center', va='center', transform=ax.transAxes)\n",
    "             continue\n",
    "\n",
    "\n",
    "        for j, split in enumerate(splits_order):\n",
    "            offset = (j - (n_splits - 1) / 2) * bar_width\n",
    "            means = []\n",
    "            stds = []\n",
    "            bar_positions = group_positions + offset\n",
    "\n",
    "            for element in elements_order:\n",
    "                mean_val = np.nan\n",
    "                std_val = 0.0 # Default std dev to 0 if not found or NaN\n",
    "\n",
    "                idx = (split, element)\n",
    "                if idx in aggregated_summary_df.index:\n",
    "                    row = aggregated_summary_df.loc[idx]\n",
    "                    # Get mean value\n",
    "                    if has_multi_cols and mean_col_multi in row.index:\n",
    "                        mean_val = row[mean_col_multi]\n",
    "                    elif has_flat_cols and mean_col_flat in row.index:\n",
    "                        mean_val = row[mean_col_flat]\n",
    "\n",
    "                    # Get std value\n",
    "                    if has_multi_cols and std_col_multi in row.index:\n",
    "                        std_val_raw = row[std_col_multi]\n",
    "                        if pd.notna(std_val_raw): std_val = std_val_raw\n",
    "                    elif has_flat_cols and std_col_flat in row.index:\n",
    "                         std_val_raw = row[std_col_flat]\n",
    "                         if pd.notna(std_val_raw): std_val = std_val_raw\n",
    "\n",
    "                means.append(mean_val if pd.notna(mean_val) else 0)\n",
    "                stds.append(std_val) # Already defaulted to 0\n",
    "\n",
    "\n",
    "            # Plot bars and error bars\n",
    "            valid_means = np.array(means)\n",
    "            valid_stds = np.array(stds)\n",
    "            ax.bar(bar_positions, valid_means, bar_width, label=split, color=split_color_map[split],\n",
    "                   edgecolor='grey', linewidth=0.5)\n",
    "            ax.errorbar(bar_positions, valid_means, yerr=valid_stds, fmt='none', ecolor='black',\n",
    "                        capsize=3, elinewidth=1, capthick=1)\n",
    "\n",
    "            # Update max y for this subplot\n",
    "            current_max = np.nanmax(valid_means + valid_stds)\n",
    "            if pd.notna(current_max) and current_max > plot_max_y:\n",
    "                plot_max_y = current_max\n",
    "\n",
    "        ax.legend(title=\"Split\", loc='best') # Adjust legend location\n",
    "        if plot_max_y > max_y_overall: # Update overall max\n",
    "             max_y_overall = plot_max_y\n",
    "\n",
    "    # --- Final Touches ---\n",
    "    common_ylim_top = math.ceil(max_y_overall * 11) / 10.0 # Add 10% buffer and ceil\n",
    "    for ax in axes:\n",
    "        ax.set_ylim(bottom=min(-0.05, ax.get_ylim()[0]), # Keep existing bottom if lower than -0.05\n",
    "                    top=min(1.05, common_ylim_top)) # Cap at 1.05\n",
    "\n",
    "    fig.suptitle(\"Metric Comparison Across Splits\", fontsize=16, y=1.03) # Adjust title y position\n",
    "    plt.tight_layout(rect=[0, 0, 1, 1])\n",
    "\n",
    "    # Save plot\n",
    "    if save_path:\n",
    "        try:\n",
    "            save_dir = os.path.dirname(save_path)\n",
    "            if save_dir: os.makedirs(save_dir, exist_ok=True)\n",
    "            plt.savefig(save_path, dpi=300, bbox_inches='tight')\n",
    "            print(f\"Comparison plot saved to {save_path}\")\n",
    "        except Exception as e:\n",
    "            print(f\"Error saving comparison plot: {e}\")\n",
    "\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# Or create a dummy DataFrame with flattened columns\n",
    "# aggregated_summary_flat = aggregated_summary_example.copy()\n",
    "# aggregated_summary_flat.columns = ['_'.join(map(str, col)).strip('_') for col in aggregated_summary_flat.columns.values]\n",
    "# aggregated_summary_flat = aggregated_summary_flat.reset_index()\n",
    "\n",
    "# Define the exact element names as they appear in your DataFrame index\n",
    "element_order_example = [\n",
    "     \"Edge\", # Match exact string\n",
    "     \"Node (member)\",\n",
    "     \"Node (provider)\"\n",
    "]\n",
    "\n",
    "plot_metric_comparison_with_std(\n",
    "     aggregated_summary_df = df_split, # Use your actual aggregated DataFrame\n",
    "     #models_to_plot=['BGAE'], # We removed model level from columns now\n",
    "     metrics_to_plot = ['AUROC', 'AP', 'Best F1'],\n",
    "     splits_to_plot = ['test', 'train', 'val'],\n",
    "     elements_to_plot = element_order_example,\n",
    "     save_path = \"report_plots/metric_comparison_splits_with_std_v2.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 372,
   "metadata": {},
   "outputs": [],
   "source": [
    "element_rename = {\n",
    "     \"Edge ('provider', 'to', 'member')\": \"Edge\"\n",
    "}\n",
    "df_split = final_summary_df.rename(index=element_rename)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_split\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "\n",
    "def calculate_average_metric_across_elements(\n",
    "    results_list: List[pd.DataFrame],\n",
    "    metric: str = 'AP',\n",
    "    elements_to_average: List[str] = ['Node (member)', 'Node (provider)', 'Edge'], # Specify elements to average\n",
    "    split: str = 'test'\n",
    "    ) -> Tuple[Optional[float], Optional[float]]:\n",
    "    \"\"\"\n",
    "    Calculates the mean and standard deviation of a metric averaged across\n",
    "    specified element types over multiple iteration runs.\n",
    "\n",
    "    Args:\n",
    "        results_list (List[pd.DataFrame]): List where each DataFrame contains the results\n",
    "                                            (including 'Split', 'Element', metric column, 'seed'/'iteration')\n",
    "                                            from one iteration run for a specific model.\n",
    "        metric (str): The performance metric column to average (e.g., 'AP', 'AUROC').\n",
    "        elements_to_average (List[str]): A list of the 'Element' names to include in the average.\n",
    "                                         Exact strings must match those in the DataFrame.\n",
    "        split (str): The data split ('train', 'val', 'test') to focus on.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Optional[float], Optional[float]]:\n",
    "            - Mean of the average metric across runs (or None if calculation fails).\n",
    "            - Standard deviation of the average metric across runs (or None if calculation fails).\n",
    "    \"\"\"\n",
    "    if not results_list:\n",
    "        print(\"Warning: Input results_list is empty.\")\n",
    "        return None, None\n",
    "\n",
    "    all_iterations_df = pd.concat(results_list, ignore_index=True)\n",
    "\n",
    "    # --- Data Validation ---\n",
    "    required_cols = ['Split', 'Element', metric, 'seed'] # Assume 'seed' identifies runs\n",
    "    if not all(col in all_iterations_df.columns for col in required_cols):\n",
    "        print(f\"Error: Input DataFrames missing one or more required columns: {required_cols}\")\n",
    "        return None, None\n",
    "\n",
    "    # Filter for the specified split and elements\n",
    "    filtered_df = all_iterations_df[\n",
    "        (all_iterations_df['Split'] == split) &\n",
    "        (all_iterations_df['Element'].isin(elements_to_average))\n",
    "    ].copy()\n",
    "\n",
    "    if filtered_df.empty:\n",
    "        print(f\"Warning: No data found for split '{split}' and elements '{elements_to_average}'.\")\n",
    "        return None, None\n",
    "\n",
    "    # Check if all desired elements were actually found for each seed\n",
    "    elements_found = filtered_df['Element'].unique()\n",
    "    if not all(elem in elements_found for elem in elements_to_average):\n",
    "         print(f\"Warning: Not all requested elements ({elements_to_average}) were found in the filtered data for split '{split}'. Found: {elements_found}\")\n",
    "         # Proceeding with available elements\n",
    "\n",
    "    # Check if the metric column has valid data\n",
    "    if filtered_df[metric].isnull().all():\n",
    "        print(f\"Warning: Metric column '{metric}' contains only NaNs for the selected split/elements.\")\n",
    "        return None, None\n",
    "\n",
    "    # --- Calculation ---\n",
    "    try:\n",
    "        # Calculate the average metric *per iteration* across the specified elements\n",
    "        # Group by 'seed' (or 'iteration'), then calculate the mean of the metric for that seed's elements\n",
    "        average_metric_per_iteration = filtered_df.groupby('seed')[metric].mean()\n",
    "\n",
    "        if average_metric_per_iteration.empty:\n",
    "             print(\"Warning: Could not calculate average metric per iteration (maybe grouping failed?).\")\n",
    "             return None, None\n",
    "\n",
    "        # Calculate the overall mean and std dev of these *per-iteration averages*\n",
    "        final_mean = average_metric_per_iteration.mean()\n",
    "        # Calculate std dev only if more than one iteration's average was computed\n",
    "        final_std = average_metric_per_iteration.std() if len(average_metric_per_iteration) > 1 else 0.0\n",
    "\n",
    "        return float(final_mean), float(final_std)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during calculation of average metric across elements: {e}\")\n",
    "        return None, None\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Assume:\n",
    "# all_summary_dfs = [...] # List of BGAE summary DFs from each run\n",
    "# all_if_summaries = [...] # List of IF summary DFs from each run\n",
    "# all_oddball_summaries = [...]\n",
    "# all_dominant_summaries = [...]\n",
    "\n",
    "# Define elements to include in the average (use exact names from your 'Element' column)\n",
    "# elements = ['Node (member)', 'Node (provider)', \"Edge ('provider', 'to', 'member')\"] # Example\n",
    "# If 'Edge' is simply 'Edge' in your df, use that:\n",
    "elements = [\"Edge ('provider', 'to', 'member')\"]\n",
    "\n",
    "\n",
    "print(\"\\n--- Calculating Average AP across Elements (Test Set) ---\")\n",
    "\n",
    "# For BGAE\n",
    "mean_avg_ap_bgae, std_avg_ap_bgae = calculate_average_metric_across_elements(\n",
    "    results_list=all_summary_dfs,\n",
    "    metric='AP',\n",
    "    elements_to_average=elements,\n",
    "    split='test'\n",
    ")\n",
    "if mean_avg_ap_bgae is not None:\n",
    "    print(f\"BGAE Avg AP (Test): {mean_avg_ap_bgae:.4f} ± {std_avg_ap_bgae:.4f}\")\n",
    "\n",
    "# For Isolation Forest\n",
    "mean_avg_ap_if, std_avg_ap_if = calculate_average_metric_across_elements(\n",
    "    results_list=all_if_summaries,\n",
    "    metric='AP',\n",
    "    elements_to_average=elements,\n",
    "    split='test'\n",
    ")\n",
    "if mean_avg_ap_if is not None:\n",
    "    print(f\"IF Avg AP (Test):   {mean_avg_ap_if:.4f} ± {std_avg_ap_if:.4f}\")\n",
    "\n",
    "# For OddBall\n",
    "mean_avg_ap_oddball, std_avg_ap_oddball = calculate_average_metric_across_elements(\n",
    "    results_list=all_oddball_summaries,\n",
    "    metric='AP',\n",
    "    elements_to_average=elements,\n",
    "    split='test'\n",
    ")\n",
    "if mean_avg_ap_oddball is not None:\n",
    "    print(f\"OddBall Avg AP (Test): {mean_avg_ap_oddball:.4f} ± {std_avg_ap_oddball:.4f}\")\n",
    "\n",
    "# For DOMINANT\n",
    "mean_avg_ap_dominant, std_avg_ap_dominant = calculate_average_metric_across_elements(\n",
    "    results_list=all_dominant_summaries,\n",
    "    metric='AP',\n",
    "    elements_to_average=elements,\n",
    "    split='test'\n",
    ")\n",
    "if mean_avg_ap_dominant is not None:\n",
    "    print(f\"DOMINANT Avg AP (Test): {mean_avg_ap_dominant:.4f} ± {std_avg_ap_dominant:.4f}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_if_summaries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "import os\n",
    "import math\n",
    "from collections import defaultdict # Added\n",
    "\n",
    "# Set a suitable style for report plots\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"deep\", font_scale=1.1)\n",
    "\n",
    "def plot_pk_rk_comparison_grid(\n",
    "    results_dict: Dict[str, List[pd.DataFrame]], # Dict: {'ModelName': [df_run1, df_run2,...]}\n",
    "    k_list_to_plot: List[int] = [50, 100, 200], # K values to plot on x-axis\n",
    "    split_to_plot: str = 'test',\n",
    "    figsize: Tuple[int, int] = (14, 8), # Adjusted for 2x2 grid\n",
    "    save_path: Optional[str] = None,\n",
    "    model_name_map: Optional[Dict[str, str]] = None, # Optional: {'BGAE': 'Our Model (BGAE)'}\n",
    "    model_palette: Optional[Dict[str, str]] = None # Optional: {'BGAE': 'blue', 'IsolationForest': 'green'}\n",
    "    ) -> None:\n",
    "    \"\"\"\n",
    "    Generates a 2x2 grid of plots showing Precision@K and Recall@K separately\n",
    "    for Member and Provider nodes across different models, including std dev error bars.\n",
    "\n",
    "    Args:\n",
    "        results_dict: Dictionary where keys are model names and values are lists\n",
    "                      of summary DataFrames from multiple runs for that model.\n",
    "        k_list_to_plot: The K values to show on the x-axis.\n",
    "        split_to_plot: The data split ('test', 'val', etc.) to plot results for.\n",
    "        figsize: Overall figure size.\n",
    "        save_path: Optional path to save the figure.\n",
    "        model_name_map: Dictionary to map internal model names to legend labels.\n",
    "        model_palette: Dictionary to map model names to specific plot colors.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Generating 2x2 Precision/Recall@K Comparison Plot ({split_to_plot} Set) ---\")\n",
    "\n",
    "    # --- 1. Aggregate Mean/Std for P@K and R@K for each model ---\n",
    "    aggregated_data = defaultdict(lambda: defaultdict(dict))\n",
    "    models_found = list(results_dict.keys())\n",
    "\n",
    "    for model_name, results_list in results_dict.items():\n",
    "        if not results_list: continue\n",
    "        full_df = pd.concat(results_list, ignore_index=True)\n",
    "        split_df = full_df[full_df['Split'] == split_to_plot].copy()\n",
    "        if split_df.empty: continue\n",
    "\n",
    "        metrics_to_agg = [f'{p}@{k}' for k in k_list_to_plot for p in ['Precision', 'Recall']]\n",
    "        metrics_present = [m for m in metrics_to_agg if m in split_df.columns]\n",
    "        if not metrics_present: continue\n",
    "\n",
    "        try:\n",
    "            grouped = split_df.groupby('Element')[metrics_present]\n",
    "            means = grouped.mean()\n",
    "            stds = grouped.std()\n",
    "\n",
    "            for element in means.index: # Elements like 'Node (member)', 'Node (provider)'\n",
    "                 aggregated_data[model_name][element] = {}\n",
    "                 for metric in metrics_present:\n",
    "                      mean_val = means.loc[element, metric]\n",
    "                      std_val = stds.loc[element, metric]\n",
    "                      aggregated_data[model_name][element][metric] = (\n",
    "                          mean_val if pd.notna(mean_val) else 0,\n",
    "                          std_val if pd.notna(std_val) else 0\n",
    "                      )\n",
    "        except Exception as e:\n",
    "             print(f\"Warning: Could not aggregate P@K/R@K metrics for model '{model_name}': {e}\")\n",
    "\n",
    "    if not aggregated_data:\n",
    "        print(\"Error: No aggregated P@K/R@K data available to plot.\")\n",
    "        return\n",
    "\n",
    "    # --- 2. Plotting Setup ---\n",
    "    fig, axes = plt.subplots(2, 2, figsize=figsize, sharex=True, sharey=False)\n",
    "    fig.suptitle(f'Precision and Recall at Top-K Node Anomalies on {split_to_plot.capitalize()} Set', fontsize=16, y=1.03)\n",
    "\n",
    "    element_map = {'Node (member)': 0, 'Node (provider)': 1}\n",
    "    metric_map = {'Precision': 0, 'Recall': 1}\n",
    "\n",
    "    # --- **FIXED COLOR ASSIGNMENT** ---\n",
    "    # Assign colors definitively *before* the loop\n",
    "    default_colors = sns.color_palette(\"deep\", len(models_found))\n",
    "    color_assignment = {}\n",
    "    if model_palette is None:\n",
    "        model_palette = {} # Ensure it's a dict\n",
    "\n",
    "    # Assign colors: Use provided palette first, then defaults\n",
    "    color_idx = 0\n",
    "    for model_name in models_found:\n",
    "        if model_name in model_palette:\n",
    "            color_assignment[model_name] = model_palette[model_name]\n",
    "        elif color_idx < len(default_colors):\n",
    "            color_assignment[model_name] = default_colors[color_idx]\n",
    "            color_idx += 1\n",
    "        else:\n",
    "            color_assignment[model_name] = 'grey' # Fallback grey if more models than default colors\n",
    "\n",
    "    if model_name_map is None:\n",
    "        model_name_map = {model: model for model in models_found} # Default to original names\n",
    "    # --- **END FIXED COLOR ASSIGNMENT** ---\n",
    "\n",
    "    line_width = 2.0\n",
    "    grid_style = {'linestyle': ':', 'alpha': 0.6}\n",
    "\n",
    "    # --- 3. Create Subplots ---\n",
    "    plotted_models = set() # Track models plotted for legend\n",
    "\n",
    "    for model_name in models_found: # Iterate through models expected based on input dict\n",
    "        if model_name not in aggregated_data: continue # Skip if no data aggregated for this model\n",
    "\n",
    "        # --- **Get Assigned Color** ---\n",
    "        plot_color = color_assignment.get(model_name, 'grey') # Get pre-assigned color\n",
    "        legend_label = model_name_map.get(model_name, model_name)\n",
    "        first_plot_for_model = True # Flag to add model label only once\n",
    "\n",
    "        for element_name, col_idx in element_map.items():\n",
    "            if element_name not in aggregated_data.get(model_name, {}): continue # Skip if no data for this element\n",
    "\n",
    "            element_data = aggregated_data[model_name][element_name]\n",
    "\n",
    "            for metric_type, row_idx in metric_map.items():\n",
    "                ax = axes[row_idx, col_idx]\n",
    "\n",
    "                # Extract means and stds\n",
    "                means = [element_data.get(f'{metric_type}@{k}', (0, 0))[0] for k in k_list_to_plot]\n",
    "                stds = [element_data.get(f'{metric_type}@{k}', (0, 0))[1] for k in k_list_to_plot]\n",
    "\n",
    "                # Plot line with error bars\n",
    "                current_label = legend_label if first_plot_for_model else '_nolegend_'\n",
    "                ax.errorbar(k_list_to_plot, means, yerr=stds, label=current_label,\n",
    "                            color=plot_color, linestyle='-', # Use solid lines now\n",
    "                            linewidth=line_width, marker='.', markersize=0,\n",
    "                            capsize=4, elinewidth=1, capthick=1)\n",
    "                first_plot_for_model = False\n",
    "                plotted_models.add(model_name)\n",
    "\n",
    "\n",
    "    # --- 4. Final Touches for Each Axes ---\n",
    "    max_y_all = 0\n",
    "    for r in range(2):\n",
    "        for c in range(2):\n",
    "            ax = axes[r, c]\n",
    "            element_name = list(element_map.keys())[list(element_map.values()).index(c)] # Get element name back\n",
    "            metric_type = list(metric_map.keys())[list(metric_map.values()).index(r)] # Get metric name back\n",
    "\n",
    "            ax.set_title(f\"{metric_type} - {element_name} Anomalies\")\n",
    "            ax.grid(True, **grid_style)\n",
    "            ax.set_xticks(k_list_to_plot)\n",
    "\n",
    "            # Set labels only on outer axes\n",
    "            if c == 0: ax.set_ylabel(f\"{metric_type} Score\")\n",
    "            if r == 1: ax.set_xlabel(\"Number of Top Anomalies (k)\")\n",
    "\n",
    "            # Adjust Y limits (find max across all plots for consistency, or per plot)\n",
    "            current_ylim = ax.get_ylim()\n",
    "            max_y_all = max(max_y_all, current_ylim[1])\n",
    "            ax.set_ylim(bottom=-0.05) # Set bottom limit\n",
    "\n",
    "    # Apply consistent top y-limit\n",
    "    common_ylim_top = min(1.05, math.ceil(max_y_all * 11) / 10.0) # Add buffer, cap at 1.05\n",
    "    for r in range(2):\n",
    "        for c in range(2):\n",
    "            axes[r, c].set_ylim(top=common_ylim_top)\n",
    "    # --- 5. Create Shared Legend ---\n",
    "    from matplotlib.lines import Line2D\n",
    "    # --- **Use color_assignment for Legend** ---\n",
    "    legend_handles = [Line2D([0], [0], color=color_assignment.get(model, 'grey'), lw=line_width,\n",
    "                            label=model_name_map.get(model, model))\n",
    "                    for model in models_found if model in plotted_models] # Only include models actually plotted\n",
    "\n",
    "    # Place legend outside the top-right plot\n",
    "    axes[0, 1].legend(handles=legend_handles, title=\"Model\",\n",
    "                    bbox_to_anchor=(1.05, 1), loc='upper left', borderaxespad=0.)\n",
    "\n",
    "\n",
    "    # --- 6. Layout & Save ---\n",
    "    # (Keep this section as it was)\n",
    "    plt.tight_layout(rect=[0, 0, 0.88, 0.95])\n",
    "    # ... (saving logic) ...\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "# --- Example Usage ---\n",
    "#Assume results_dict is populated as before:\n",
    "results_for_plot = {\n",
    "    \"Our Model (BGAE)\": all_summary_dfs,\n",
    "    \"Isolation Forest\": all_if_summaries,\n",
    "    \"Oddball\": all_oddball_summaries,\n",
    "    \"DOMINANT\": all_dominant_summaries\n",
    "}\n",
    "palette_map = { ... } # Optional color map\n",
    "\n",
    "plot_pk_rk_comparison_grid(\n",
    "    results_dict=results_for_plot,\n",
    "    k_list_to_plot=[50, 100, 200, 500], # K values present in your data\n",
    "    split_to_plot='test',\n",
    "    model_palette=palette_map # Optional\n",
    "    # save_path=\"report_plots/pk_rk_comparison_grid.png\"\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "oddball_summary_dfba"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_anomaly_type_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_anomaly_type_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "from collections import defaultdict # Added for scatter plot data prep\n",
    "\n",
    "# --- Assumed: Helper function from previous steps ---\n",
    "def extract_main_category(tag):\n",
    "    \"\"\"Helper function to extract the main category from the tag.\"\"\"\n",
    "    if pd.isna(tag): return 'Unknown'\n",
    "    if isinstance(tag, str):\n",
    "        if tag.startswith(\"('\"): return 'Edge' # Classify edge types\n",
    "        if '/' in tag:\n",
    "            category = tag.split('/')[0]\n",
    "            if category.lower() == 'structural': return 'Structural'\n",
    "            if category.lower() == 'attribute': return 'Attribute'\n",
    "            return category\n",
    "        elif tag.lower() == 'combined': return 'Combined'\n",
    "        elif tag.lower() == 'unknown': return 'Unknown'\n",
    "        else:\n",
    "             if 'attribute' in tag.lower(): return 'Attribute'\n",
    "             if 'structural' in tag.lower(): return 'Structural'\n",
    "             if 'combined' in tag.lower(): return 'Combined'\n",
    "             return 'Other'\n",
    "    return 'Unknown'\n",
    "\n",
    "def format_mean_std(mean_val, std_val, precision=3):\n",
    "    \"\"\"Formats mean and std dev into 'mean ± std' string.\"\"\"\n",
    "    mean_val_num = pd.to_numeric(mean_val, errors='coerce')\n",
    "    std_val_num = pd.to_numeric(std_val, errors='coerce')\n",
    "    if pd.notna(mean_val_num) and pd.notna(std_val_num):\n",
    "        return f\"{mean_val_num:.{precision}f} ± {std_val_num:.{precision}f}\"\n",
    "    elif pd.notna(mean_val_num):\n",
    "        return f\"{mean_val_num:.{precision}f} ± nan\"\n",
    "    else: return \"N/A\"\n",
    "\n",
    "# --- Main Analysis Function ---\n",
    "\n",
    "def generate_anomaly_type_report_tables(\n",
    "    full_anomaly_type_results_df: pd.DataFrame, # DataFrame with results from ALL runs\n",
    "    split_name: str = 'test',\n",
    "    sort_metric: str = 'AP', # Metric for ranking detailed tags\n",
    "    metrics_to_show: List[str] = ['AUROC', 'AP', 'Best F1'], # Metrics in tables\n",
    "    n_top_bottom: int = 3 # Number of top/bottom tags to show per element\n",
    "    ) -> Tuple[Optional[pd.DataFrame], Optional[pd.DataFrame], Optional[pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Generates DataFrames for report tables summarizing performance by anomaly type.\n",
    "\n",
    "    Args:\n",
    "        full_anomaly_type_results_df (pd.DataFrame): DataFrame with per-tag metrics from ALL iterations\n",
    "                                                     (must include 'Split', 'Element Type', 'Node Type',\n",
    "                                                     'Anomaly Tag', 'Count', 'Mean Score', 'Median Score',\n",
    "                                                     'AUROC', 'AP', 'Best F1', 'seed').\n",
    "        split_name (str): The split to analyze ('train', 'val', or 'test').\n",
    "        sort_metric (str): Metric used to rank specific tags ('AP', 'AUROC', 'Best F1').\n",
    "        metrics_to_show (List[str]): List of performance metric columns for summaries.\n",
    "        n_top_bottom (int): Number of top and bottom performing tags to show per element type.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Optional[pd.DataFrame], Optional[pd.DataFrame], Optional[pd.DataFrame]]:\n",
    "            - df_category_summary: Aggregated metrics (Mean ± Std) by Main Category and Element Type.\n",
    "            - df_top_bottom_tags: Detailed metrics (Mean ± Std) for top/bottom specific tags.\n",
    "            - df_for_scatter: Data prepared for the Score vs Performance scatter plot.\n",
    "            (Returns None, None, None if analysis fails)\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Generating Anomaly Type Report Tables for Split: '{split_name}' ---\")\n",
    "\n",
    "    if full_anomaly_type_results_df is None or full_anomaly_type_results_df.empty:\n",
    "        print(\"Input DataFrame is empty.\")\n",
    "        return None, None, None\n",
    "\n",
    "    # --- 1. Preprocessing & Filtering ---\n",
    "    df = full_anomaly_type_results_df[full_anomaly_type_results_df['Split'] == split_name].copy()\n",
    "    if df.empty:\n",
    "        print(f\"No data found for split '{split_name}'.\")\n",
    "        return None, None, None\n",
    "\n",
    "    required_cols = ['Node Type', 'Anomaly Tag', 'Count', 'seed'] + metrics_to_show\n",
    "    missing_cols = [col for col in required_cols if col not in df.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Error: Missing required columns in DataFrame: {missing_cols}\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Add Main Category\n",
    "    df['Main Category'] = df['Anomaly Tag'].apply(extract_main_category)\n",
    "\n",
    "    # --- 2. Aggregate Mean & Std per Tag/Element/Node Type ---\n",
    "    # Group by everything except the run seed/iteration to get per-tag aggregates\n",
    "    grouping_cols = ['Split', 'Node Type', 'Anomaly Tag', 'Main Category']\n",
    "    # Define columns to aggregate (metrics + stats needed later)\n",
    "    cols_to_agg = metrics_to_show + ['Mean Score', 'Median Score', 'Count', 'Proportion (%)']\n",
    "    # Ensure columns exist before aggregating\n",
    "    cols_present = [c for c in cols_to_agg if c in df.columns]\n",
    "\n",
    "    try:\n",
    "        agg_funcs = {col: ['mean', 'std'] for col in cols_present}\n",
    "        agg_funcs['seed'] = ['nunique'] # Count runs\n",
    "\n",
    "        df_agg = df.groupby(grouping_cols, observed=True).agg(agg_funcs)\n",
    "\n",
    "        # Flatten MultiIndex columns\n",
    "        df_agg.columns = ['_'.join(map(str, col)).strip('_') for col in df_agg.columns.values]\n",
    "        df_agg = df_agg.rename(columns={'seed_nunique': 'n_runs',\n",
    "                                        'Count_mean': 'Avg Count',\n",
    "                                        'Proportion (%)_mean': 'Avg Proportion (%)'})\n",
    "        df_agg = df_agg.reset_index() # Make grouping cols regular columns\n",
    "\n",
    "    except Exception as e:\n",
    "         print(f\"Error during initial aggregation: {e}\")\n",
    "         return None, None, None\n",
    "\n",
    "    # --- 3. Table 1: Summary by Main Category & Element Type ---\n",
    "    print(\"\\n--- Generating Table 1: Summary by Main Category & Element ---\")\n",
    "    df_category_summary = None\n",
    "    try:\n",
    "        # Group the already aggregated data further by Main Category and Element Type\n",
    "        category_group = df_agg.groupby(['Main Category', 'Node Type'], observed=True)\n",
    "\n",
    "        # Sum counts and proportions\n",
    "        summary_counts = category_group[['Avg Count', 'Avg Proportion (%)']].sum()\n",
    "        summary_counts = summary_counts.rename(columns={'Avg Count': 'Total Anomalies',\n",
    "                                                         'Avg Proportion (%)': 'Category Proportion (%)'})\n",
    "\n",
    "        # Average the performance metrics (mean values)\n",
    "        metric_mean_cols = [f'{m}_mean' for m in metrics_to_show if f'{m}_mean' in df_agg.columns]\n",
    "        summary_metrics_mean = category_group[metric_mean_cols].mean()\n",
    "\n",
    "        # Combine counts and averaged means\n",
    "        df_category_summary = summary_counts.join(summary_metrics_mean)\n",
    "\n",
    "        # Format metrics as Mean ± Std for display table\n",
    "        df_category_summary_display = df_category_summary.copy()\n",
    "        for metric in metrics_to_show:\n",
    "             mean_col = f'{metric}_mean'\n",
    "             std_col = f'{metric}_std'\n",
    "             # Calculate the mean of the std devs within the category group for an *indicative* variability\n",
    "             std_mean = category_group[std_col].mean() if std_col in category_group.mean().columns else pd.Series(index=df_category_summary.index, dtype=float)\n",
    "             # Create formatted string\n",
    "             df_category_summary_display[metric] = [\n",
    "                 format_mean_std(df_category_summary_display.loc[idx, mean_col], std_mean.get(idx, np.nan))\n",
    "                 for idx in df_category_summary_display.index\n",
    "             ]\n",
    "             # Drop original mean col after formatting\n",
    "             if mean_col in df_category_summary_display.columns:\n",
    "                  df_category_summary_display = df_category_summary_display.drop(columns=[mean_col])\n",
    "\n",
    "\n",
    "        # Reorder and select columns for display table\n",
    "        final_cols_cat = ['Total Anomalies', 'Category Proportion (%)'] + metrics_to_show\n",
    "        df_category_summary_display = df_category_summary_display.reindex(columns=final_cols_cat, fill_value='N/A').round(3)\n",
    "        print(df_category_summary_display.to_string())\n",
    "\n",
    "        # Keep numerical version for potential later use\n",
    "        df_category_summary = df_category_summary.round(4)\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating category summary table: {e}\")\n",
    "\n",
    "\n",
    "    # --- 4. Table 2: Top & Bottom Performing Specific Tags ---\n",
    "    print(f\"\\n--- Generating Table 2: Top/Bottom {n_top_bottom} Tags by {sort_metric} ---\")\n",
    "    df_top_bottom_tags = None\n",
    "    try:\n",
    "        if sort_metric not in metrics_to_show:\n",
    "             print(f\"Warning: Sort metric '{sort_metric}' not in metrics_to_show list. Cannot sort detailed table.\")\n",
    "        else:\n",
    "            sort_col_mean = f'{sort_metric}_mean'\n",
    "            if sort_col_mean not in df_agg.columns:\n",
    "                 print(f\"Warning: Mean column for sort metric '{sort_metric}' ({sort_col_mean}) not found.\")\n",
    "            else:\n",
    "                top_bottom_list = []\n",
    "                # Get top/bottom for each element type separately\n",
    "                for element in df_agg['Element Type'].unique():\n",
    "                    df_element = df_agg[df_agg['Element Type'] == element].copy()\n",
    "                    df_element_sorted = df_element.sort_values(by=sort_col_mean, ascending=False)\n",
    "                    top_n = df_element_sorted.head(n_top_bottom)\n",
    "                    bottom_n = df_element_sorted.tail(n_top_bottom)\n",
    "                    top_bottom_list.extend([top_n, bottom_n])\n",
    "\n",
    "                if top_bottom_list:\n",
    "                    df_top_bottom_tags_agg = pd.concat(top_bottom_list).drop_duplicates()\n",
    "\n",
    "                    # Format metrics as Mean ± Std for display\n",
    "                    df_top_bottom_tags_display = df_top_bottom_tags_agg.copy()\n",
    "                    cols_detailed_display = ['Element Type', 'Main Category', 'Anomaly Tag', 'Avg Count', 'Avg Proportion (%)']\n",
    "                    for metric in metrics_to_show:\n",
    "                         mean_col = f'{metric}_mean'; std_col = f'{metric}_std'\n",
    "                         if mean_col in df_top_bottom_tags_display.columns:\n",
    "                              df_top_bottom_tags_display[metric] = df_top_bottom_tags_display.apply(\n",
    "                                   lambda row: format_mean_std(row.get(mean_col), row.get(std_col)), axis=1\n",
    "                              )\n",
    "                              cols_detailed_display.append(metric)\n",
    "                              # Drop original mean/std columns after formatting\n",
    "                              df_top_bottom_tags_display = df_top_bottom_tags_display.drop(columns=[mean_col, f'{metric}_std'], errors='ignore')\n",
    "                         else:\n",
    "                              df_top_bottom_tags_display[metric] = \"N/A\" # Add metric column even if mean was missing\n",
    "                              cols_detailed_display.append(metric)\n",
    "\n",
    "\n",
    "                    df_top_bottom_tags_display = df_top_bottom_tags_display[cols_detailed_display].round(3)\n",
    "                    print(df_top_bottom_tags_display.to_string(index=False))\n",
    "                    # Store numerical version\n",
    "                    df_top_bottom_tags = df_top_bottom_tags_agg\n",
    "\n",
    "                else:\n",
    "                     print(\"No data found for top/bottom tags.\")\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating top/bottom tags table: {e}\")\n",
    "\n",
    "\n",
    "    # --- 5. Prepare Data for Scatter Plot (Optional: Figure 2) ---\n",
    "    print(\"\\n--- Preparing Data for Score vs Performance Scatter Plot ---\")\n",
    "    df_for_scatter = None\n",
    "    try:\n",
    "        # Select necessary columns from the aggregated-per-tag DataFrame\n",
    "        scatter_cols = ['Element Type', 'Main Category', 'Anomaly Tag', 'Avg Count', 'Avg Proportion (%)']\n",
    "        metrics_for_scatter = ['AP', 'Best F1', 'Mean Score', 'Median Score'] # Choose metrics/stats for axes\n",
    "        scatter_metrics_present = []\n",
    "\n",
    "        for metric in metrics_for_scatter:\n",
    "             mean_col = f'{metric}_mean'\n",
    "             if mean_col in df_agg.columns:\n",
    "                  scatter_cols.append(mean_col)\n",
    "                  scatter_metrics_present.append(metric)\n",
    "\n",
    "        # Ensure all needed columns exist before selecting\n",
    "        if all(c in df_agg.columns for c in scatter_cols):\n",
    "            df_for_scatter = df_agg[scatter_cols].copy()\n",
    "            # Rename columns for clarity in plot\n",
    "            rename_map = {f'{m}_mean': m for m in scatter_metrics_present}\n",
    "            df_for_scatter = df_for_scatter.rename(columns=rename_map)\n",
    "            print(f\"Scatter plot data prepared with {len(df_for_scatter)} unique tags.\")\n",
    "            # print(df_for_scatter.head()) # Print head for verification\n",
    "        else:\n",
    "            missing = [c for c in scatter_cols if c not in df_agg.columns]\n",
    "            print(f\"Could not prepare scatter plot data: Missing columns {missing}\")\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error preparing data for scatter plot: {e}\")\n",
    "\n",
    "\n",
    "    return df_category_summary, df_top_bottom_tags, df_for_scatter\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Assume 'final_anomaly_type_df' is the DataFrame returned by concatenating\n",
    "# results from all iterations (output of previous aggregation step).\n",
    "\n",
    "if 'final_anomaly_type_df' in locals() and not final_anomaly_type_df.empty:\n",
    "    # Specify metrics to include in the tables\n",
    "    metrics = ['AUROC', 'AP', 'Best F1'] # Primary performance metrics\n",
    "    # Specify metric to sort detailed table by and to plot in category comparison\n",
    "    sort_display_metric = 'AP'\n",
    "\n",
    "    cat_summary, top_bottom_summary, scatter_data = generate_anomaly_type_report_tables(\n",
    "        full_anomaly_type_results_df=final_anomaly_type_df,\n",
    "        split_name='test',\n",
    "        sort_metric=sort_display_metric,\n",
    "        metrics_to_show=metrics,\n",
    "        n_top_bottom=3\n",
    "    )\n",
    "\n",
    "    # Now you can use cat_summary, top_bottom_summary for LaTeX tables\n",
    "    # and scatter_data for the optional scatter plot.\n",
    "else:\n",
    "     print(\"Input DataFrame `final_anomaly_type_df` is missing or empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 422,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "from collections import defaultdict\n",
    "import os\n",
    "import math\n",
    "\n",
    "# Assume sns.set_theme() is called outside\n",
    "\n",
    "def extract_main_category(tag):\n",
    "    if pd.isna(tag): return 'Unknown'\n",
    "    if isinstance(tag, str):\n",
    "        # No need to check for edge tuples here if no 'Element Type' exists\n",
    "        if '/' in tag:\n",
    "            category = tag.split('/')[0]\n",
    "            if category.lower() == 'structural': return 'Structural'\n",
    "            if category.lower() == 'attribute': return 'Attribute'\n",
    "            return category\n",
    "        elif tag.lower() == 'combined': return 'Combined'\n",
    "        elif tag.lower() == 'unknown': return 'Unknown'\n",
    "        else:\n",
    "             if 'attribute' in tag.lower(): return 'Attribute'\n",
    "             if 'structural' in tag.lower(): return 'Structural'\n",
    "             if 'combined' in tag.lower(): return 'Combined'\n",
    "             return 'Other'\n",
    "    return 'Unknown'\n",
    "\n",
    "def format_mean_std(mean_val, std_val, precision=3):\n",
    "    mean_val_num = pd.to_numeric(mean_val, errors='coerce')\n",
    "    std_val_num = pd.to_numeric(std_val, errors='coerce')\n",
    "    if pd.notna(mean_val_num) and pd.notna(std_val_num):\n",
    "        return f\"{mean_val_num:.{precision}f} ± {std_val_num:.{precision}f}\"\n",
    "    elif pd.notna(mean_val_num):\n",
    "        return f\"{mean_val_num:.{precision}f} ± nan\"\n",
    "    else: return \"N/A\"\n",
    "\n",
    "def generate_anomaly_type_report_tables(\n",
    "    full_anomaly_type_results_df: pd.DataFrame,\n",
    "    split_name: str = 'test',\n",
    "    sort_metric: str = 'AP',\n",
    "    metrics_to_show: List[str] = ['AUROC', 'AP', 'Best F1'],\n",
    "    n_top_bottom: int = 3\n",
    "    ) -> Tuple[Optional[pd.DataFrame], Optional[pd.DataFrame], Optional[pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Generates DataFrames for report tables summarizing performance by anomaly type,\n",
    "    assuming input df uses 'Node Type' for element identification.\n",
    "\n",
    "    Args:\n",
    "        full_anomaly_type_results_df (pd.DataFrame): DataFrame with per-tag metrics from ALL iterations\n",
    "                                                     (must include 'Split', 'Node Type', 'Anomaly Tag',\n",
    "                                                      'Count', 'Mean Score', 'Median Score', 'AUROC',\n",
    "                                                      'AP', 'Best F1', 'seed').\n",
    "        split_name (str): The split to analyze.\n",
    "        sort_metric (str): Metric used to rank specific tags.\n",
    "        metrics_to_show (List[str]): Performance metrics for summaries.\n",
    "        n_top_bottom (int): Number of top/bottom tags to show per node type.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Optional[pd.DataFrame], Optional[pd.DataFrame], Optional[pd.DataFrame]]:\n",
    "            - df_category_summary: Aggregated metrics (Mean ± Std) by Main Category and Node Type.\n",
    "            - df_top_bottom_tags: Detailed metrics (Mean ± Std) for top/bottom specific tags.\n",
    "            - df_for_scatter: Data prepared for the Score vs Performance scatter plot.\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Generating Anomaly Type Report Tables for Split: '{split_name}' ---\")\n",
    "\n",
    "    if full_anomaly_type_results_df is None or full_anomaly_type_results_df.empty:\n",
    "        print(\"Input DataFrame is empty.\")\n",
    "        return None, None, None\n",
    "\n",
    "    # --- 1. Preprocessing & Filtering ---\n",
    "    df_split = full_anomaly_type_results_df[full_anomaly_type_results_df['Split'] == split_name].copy()\n",
    "    if df_split.empty:\n",
    "        print(f\"No data found for split '{split_name}'.\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Check required columns (using Node Type instead of Element Type)\n",
    "    required_cols = ['Node Type', 'Anomaly Tag', 'Count', 'seed'] + metrics_to_show\n",
    "    score_stat_cols = ['Mean Score', 'Median Score'] # Check optional cols\n",
    "    required_cols.extend([c for c in score_stat_cols if c in df_split.columns]) # Add score stats if present\n",
    "    missing_cols = [col for col in required_cols if col not in df_split.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Error: Missing required columns in DataFrame: {missing_cols}\")\n",
    "        return None, None, None\n",
    "\n",
    "    # Add Main Category\n",
    "    df_split['Main Category'] = df_split['Anomaly Tag'].apply(extract_main_category)\n",
    "\n",
    "    # Calculate Total Anomalies per Node Type for proportion calculation\n",
    "    total_anomalies_per_node_type = df_split.groupby('Node Type')['Count'].sum()\n",
    "\n",
    "    def get_proportion(row):\n",
    "        total = total_anomalies_per_node_type.get(row['Node Type'], 0)\n",
    "        return (row['Count'] / total * 100) if total > 0 else 0\n",
    "    df_split['Proportion (%)'] = df_split.apply(get_proportion, axis=1)\n",
    "\n",
    "    # --- 2. Aggregate Mean & Std per Tag/Node Type ---\n",
    "    grouping_cols = ['Split', 'Node Type', 'Anomaly Tag', 'Main Category']\n",
    "    # Define columns to aggregate\n",
    "    cols_to_agg = metrics_to_show + [c for c in score_stat_cols if c in df_split.columns] + ['Count', 'Proportion (%)']\n",
    "    cols_present_agg = [c for c in cols_to_agg if c in df_split.columns]\n",
    "\n",
    "    try:\n",
    "        agg_funcs = {col: ['mean', 'std'] for col in cols_present_agg}\n",
    "        agg_funcs['seed'] = ['nunique']\n",
    "        df_agg = df_split.groupby(grouping_cols, observed=True).agg(agg_funcs)\n",
    "        df_agg.columns = ['_'.join(map(str, col)).strip('_') for col in df_agg.columns.values]\n",
    "        df_agg = df_agg.rename(columns={'seed_nunique': 'n_runs',\n",
    "                                        'Count_mean': 'Avg Count',\n",
    "                                        'Proportion (%)_mean': 'Avg Proportion (%)'})\n",
    "        df_agg = df_agg.reset_index()\n",
    "    except Exception as e:\n",
    "         print(f\"Error during initial aggregation: {e}\")\n",
    "         return None, None, None\n",
    "\n",
    "    # --- 3. Table 1: Summary by Main Category & Node Type ---\n",
    "    print(\"\\n--- Generating Table 1: Summary by Main Category & Node Type ---\")\n",
    "    df_category_summary = None # Numerical version\n",
    "    df_category_summary_display = pd.DataFrame() # Initialize display version\n",
    "\n",
    "    # Ensure df_agg exists and is not empty from step 2\n",
    "    if df_agg is None or df_agg.empty:\n",
    "        print(\"Skipping Table 1: Initial aggregated DataFrame (df_agg) is missing or empty.\")\n",
    "    else:\n",
    "        try:\n",
    "            # Define numerical columns needed for averaging means\n",
    "            metric_mean_cols_to_avg = [f'{m}_mean' for m in metrics_to_show if f'{m}_mean' in df_agg.columns]\n",
    "            # Define numerical columns needed for averaging stds (for display formatting)\n",
    "            metric_std_cols_to_avg = [f'{m}_std' for m in metrics_to_show if f'{m}_std' in df_agg.columns]\n",
    "            # Define numerical columns needed for summing counts/proportions\n",
    "            count_prop_cols_to_sum = [c for c in ['Avg Count', 'Avg Proportion (%)'] if c in df_agg.columns]\n",
    "\n",
    "            # --- Data Conversion and Cleaning before Grouping ---\n",
    "            df_agg_numeric = df_agg.copy()\n",
    "            cols_to_convert = metric_mean_cols_to_avg + metric_std_cols_to_avg + count_prop_cols_to_sum\n",
    "\n",
    "            for col in cols_to_convert:\n",
    "                if col in df_agg_numeric.columns:\n",
    "                    # Force conversion to numeric, making errors NaN\n",
    "                    df_agg_numeric[col] = pd.to_numeric(df_agg_numeric[col], errors='coerce')\n",
    "                #else: # Debugging line, remove later\n",
    "                #    print(f\"Debug: Column {col} not found for numeric conversion in df_agg_numeric\")\n",
    "\n",
    "\n",
    "            # --- Perform Aggregation on Cleaned Numeric Data ---\n",
    "            category_group = df_agg_numeric.groupby(['Main Category', 'Node Type'], observed=True)\n",
    "\n",
    "            # Sum counts and proportions (safe now due to numeric conversion)\n",
    "            if not count_prop_cols_to_sum: # Check if columns exist\n",
    "                summary_counts = pd.DataFrame(index=category_group.groups.keys()) # Create empty df with correct index\n",
    "                summary_counts['Total Anomalies'] = 0\n",
    "                summary_counts['Category Proportion (%)'] = 0\n",
    "            else:\n",
    "                summary_counts = category_group[count_prop_cols_to_sum].sum()\n",
    "                summary_counts = summary_counts.rename(columns={'Avg Count': 'Total Anomalies',\n",
    "                                                                'Avg Proportion (%)': 'Category Proportion (%)'})\n",
    "\n",
    "\n",
    "            # Average the performance metric means\n",
    "            if not metric_mean_cols_to_avg: # Check if columns exist\n",
    "                summary_metrics_mean = pd.DataFrame(index=category_group.groups.keys())\n",
    "            else:\n",
    "                summary_metrics_mean = category_group[metric_mean_cols_to_avg].mean() # This should work now\n",
    "\n",
    "\n",
    "            # Average the standard deviations (needed for formatting)\n",
    "            if not metric_std_cols_to_avg: # Check if columns exist\n",
    "                summary_metrics_std_mean = pd.DataFrame(index=category_group.groups.keys())\n",
    "            else:\n",
    "                summary_metrics_std_mean = category_group[metric_std_cols_to_avg].mean() # Mean of the stds\n",
    "\n",
    "\n",
    "            # Combine counts and averaged metric means\n",
    "            df_category_summary_num = summary_counts.join(summary_metrics_mean) # Numerical summary base\n",
    "\n",
    "            # --- Format for Display ---\n",
    "            df_category_summary_display = df_category_summary_num.copy()\n",
    "            for metric in metrics_to_show:\n",
    "                mean_col = f'{metric}_mean'\n",
    "                std_mean_col = f'{metric}_std' # Column name in summary_metrics_std_mean\n",
    "\n",
    "                # Get the series of averaged std deviations for this metric\n",
    "                # Use .get() on the DataFrame which returns None if column doesn't exist\n",
    "                std_mean_series = summary_metrics_std_mean.get(std_mean_col)\n",
    "\n",
    "                if mean_col in df_category_summary_display.columns:\n",
    "                    # Apply formatting using the numerical mean from summary and the averaged std dev series\n",
    "                    df_category_summary_display[metric] = [\n",
    "                        format_mean_std(\n",
    "                            df_category_summary_display.loc[idx, mean_col],\n",
    "                            std_mean_series.get(idx, np.nan) if std_mean_series is not None else np.nan # Safely get std for this index\n",
    "                        )\n",
    "                        for idx in df_category_summary_display.index\n",
    "                    ]\n",
    "                    df_category_summary_display = df_category_summary_display.drop(columns=[mean_col], errors='ignore')\n",
    "                else:\n",
    "                    # Only add metric column if it wasn't calculated (e.g. mean col missing)\n",
    "                    if metric not in df_category_summary_display.columns:\n",
    "                        df_category_summary_display[metric] = \"N/A\"\n",
    "\n",
    "            # Reorder and select columns for display table\n",
    "            final_cols_cat = ['Total Anomalies', 'Category Proportion (%)'] + metrics_to_show\n",
    "            # Ensure all desired columns exist before reindexing\n",
    "            final_cols_cat_present = [col for col in final_cols_cat if col in df_category_summary_display.columns]\n",
    "            df_category_summary_display = df_category_summary_display.reindex(columns=final_cols_cat_present, fill_value='N/A').round(3) # Use present columns\n",
    "            print(df_category_summary_display.to_string())\n",
    "\n",
    "            # Store numerical version if needed later\n",
    "            df_category_summary = df_category_summary_num.round(4)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error creating category summary table: {e}\")\n",
    "            # import traceback; traceback.print_exc() # Uncomment for full traceback\n",
    "\n",
    "\n",
    "    # --- 4. Table 2: Top & Bottom Performing Specific Tags ---\n",
    "    print(f\"\\n--- Generating Table 2: Top/Bottom {n_top_bottom} Tags by {sort_metric} ---\")\n",
    "    df_top_bottom_tags = None\n",
    "    df_top_bottom_tags_display = pd.DataFrame() # Initialize empty display df\n",
    "    try:\n",
    "        sort_col_mean = f'{sort_metric}_mean'\n",
    "        if sort_col_mean not in df_agg.columns:\n",
    "            print(f\"Warning: Mean column for sort metric '{sort_metric}' ({sort_col_mean}) not found.\")\n",
    "        else:\n",
    "            top_bottom_list = []\n",
    "            # Get top/bottom for each NODE TYPE separately\n",
    "            for node_type in df_agg['Node Type'].unique(): # Iterate through actual node types found\n",
    "                df_element = df_agg[df_agg['Node Type'] == node_type].copy()\n",
    "                df_element_sorted = df_element.sort_values(by=sort_col_mean, ascending=False)\n",
    "                top_n = df_element_sorted.head(n_top_bottom)\n",
    "                bottom_n = df_element_sorted.tail(n_top_bottom)\n",
    "                top_bottom_list.extend([top_n, bottom_n])\n",
    "\n",
    "            if top_bottom_list:\n",
    "                df_top_bottom_tags_agg = pd.concat(top_bottom_list).drop_duplicates().sort_values(\n",
    "                    by=['Node Type', sort_col_mean], ascending=[True, False] # Sort final table\n",
    "                )\n",
    "\n",
    "                # Format metrics as Mean ± Std for display\n",
    "                df_top_bottom_tags_display = df_top_bottom_tags_agg.copy()\n",
    "                cols_detailed_display = ['Node Type', 'Main Category', 'Anomaly Tag', 'Avg Count', 'Avg Proportion (%)']\n",
    "                for metric in metrics_to_show:\n",
    "                     mean_col = f'{metric}_mean'; std_col = f'{metric}_std'\n",
    "                     if mean_col in df_top_bottom_tags_display.columns:\n",
    "                          df_top_bottom_tags_display[metric] = df_top_bottom_tags_display.apply(\n",
    "                               lambda row: format_mean_std(row.get(mean_col), row.get(std_col)), axis=1\n",
    "                          )\n",
    "                          cols_detailed_display.append(metric)\n",
    "                          df_top_bottom_tags_display = df_top_bottom_tags_display.drop(columns=[mean_col, std_col], errors='ignore')\n",
    "                     else: df_top_bottom_tags_display[metric] = \"N/A\"; cols_detailed_display.append(metric)\n",
    "\n",
    "\n",
    "                df_top_bottom_tags_display = df_top_bottom_tags_display[cols_detailed_display].round(3)\n",
    "                print(df_top_bottom_tags_display.to_string(index=False))\n",
    "                # Store numerical version\n",
    "                df_top_bottom_tags = df_top_bottom_tags_agg\n",
    "            else: print(\"No data found for top/bottom tags.\")\n",
    "\n",
    "    except Exception as e: print(f\"Error creating top/bottom tags table: {e}\")\n",
    "\n",
    "\n",
    "    # --- 5. Prepare Data for Scatter Plot ---\n",
    "    print(\"\\n--- Preparing Data for Score vs Performance Scatter Plot ---\")\n",
    "    df_for_scatter = None\n",
    "    try:\n",
    "        # Select necessary columns from the aggregated-per-tag DataFrame (df_agg)\n",
    "        scatter_cols = ['Node Type', 'Main Category', 'Anomaly Tag', 'Avg Count', 'Avg Proportion (%)']\n",
    "        metrics_for_scatter = ['AP', 'Best F1', 'Mean Score_mean', 'Median Score_mean'] # Use mean cols directly\n",
    "        scatter_metrics_present = []\n",
    "\n",
    "        for col in metrics_for_scatter:\n",
    "             if col in df_agg.columns:\n",
    "                  scatter_cols.append(col)\n",
    "                  scatter_metrics_present.append(col.replace('_mean','')) # Store base name\n",
    "\n",
    "        if all(c in df_agg.columns for c in scatter_cols):\n",
    "            df_for_scatter = df_agg[scatter_cols].copy()\n",
    "            rename_map = {f'{m}_mean': m for m in scatter_metrics_present if m+'_mean' in scatter_cols} # Rename _mean cols\n",
    "            df_for_scatter = df_for_scatter.rename(columns=rename_map)\n",
    "            df_for_scatter = df_for_scatter.rename(columns={'Avg Count': 'Count', 'Avg Proportion (%)': 'Proportion (%)'}) # Simpler names for plot\n",
    "            print(f\"Scatter plot data prepared with {len(df_for_scatter)} unique tags.\")\n",
    "        else:\n",
    "            missing = [c for c in scatter_cols if c not in df_agg.columns]\n",
    "            print(f\"Could not prepare scatter plot data: Missing columns {missing}\")\n",
    "\n",
    "    except Exception as e: print(f\"Error preparing data for scatter plot: {e}\")\n",
    "\n",
    "    return df_category_summary, df_top_bottom_tags_display, df_for_scatter # Return display version of top/bottom"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "# Assume necessary imports and function definitions are done above\n",
    "# from your_module import generate_anomaly_type_report_tables, plot_metric_comparison_bars, plot_score_distributions_split # etc.\n",
    "\n",
    "# --- Prerequisites ---\n",
    "# Assume you have run the multi-iteration process and have these populated:\n",
    "# 1. final_anomaly_type_df: A pandas DataFrame concatenated from ALL iterations,\n",
    "#    containing columns like 'Split', 'Node Type', 'Anomaly Tag', 'Count',\n",
    "#    'Mean Score', 'Median Score', 'AUROC', 'AP', 'Best F1', 'seed', etc.\n",
    "# 2. final_summary_df: The aggregated DataFrame (mean +/- std) for overall performance\n",
    "#                       (output from the aggregation step of the multi-iteration run)\n",
    "# 3. all_scores: A dictionary like {'test': {'nodes': {...}, 'edges': {...}}, ...}\n",
    "#                containing the raw scores from the LAST iteration (or you might need\n",
    "#                to decide how to handle scores from multiple iterations if needed for plots).\n",
    "#                For the score distribution plot, using scores from one representative run\n",
    "#                (like the last one or one with median performance) is usually sufficient.\n",
    "\n",
    "#  Data Initialization \n",
    "final_anomaly_type_df = full_anomaly_type_results_df.copy()\n",
    "all_scores = all_run_scores.get(0) \n",
    "# Example GT labels (needed for score distribution plot - replace with actual)\n",
    "gt_node_labels = DATA_SPLITS_ITERATIONS.get(0).get(\"gt_node_labels\")\n",
    "gt_edge_labels = DATA_SPLITS_ITERATIONS.get(0).get(\"gt_edge_labels\")\n",
    "\n",
    "# --- Analysis Parameters ---\n",
    "target_split = 'test' # Analyze the test set results\n",
    "primary_sort_metric = 'AP' # Rank detailed tags by Average Precision\n",
    "metrics_to_include = ['AP'] # Metrics to show in tables/plots\n",
    "top_n_tags = 3 # Show top/bottom 3 specific tags per node type\n",
    "save_output_dir = \"anomaly_type_analysis_results\" # Directory to save tables/plots\n",
    "\n",
    "# --- Run Analysis ---\n",
    "if 'final_anomaly_type_df' in locals() and not final_anomaly_type_df.empty:\n",
    "    category_summary_df, top_bottom_tags_df, scatter_data_df = generate_anomaly_type_report_tables(\n",
    "        full_anomaly_type_results_df=final_anomaly_type_df,\n",
    "        split_name=target_split,\n",
    "        sort_metric=primary_sort_metric,\n",
    "        metrics_to_show=metrics_to_include,\n",
    "        n_top_bottom=top_n_tags\n",
    "    )\n",
    "\n",
    "    # --- Optional: Generate Plots using other functions ---\n",
    "    # Note: These plotting functions might need the aggregated data (like final_summary_df)\n",
    "    #       or raw scores (all_scores) and labels depending on their implementation.\n",
    "\n",
    "    # 1. Plot Category Comparison (using data from generate_anomaly_type_report_tables)\n",
    "    if category_summary_df is not None:\n",
    "        try:\n",
    "            # Requires plot_metric_comparison_bars function (adapted for this input if needed)\n",
    "            # This plot shows AGGREGATED performance per category\n",
    "            print(\"\\n--- Generating Category Performance Plot ---\")\n",
    "            # Need to slightly reformat category_summary_df for the plotting function if it expects specific index/columns\n",
    "            plot_data_cat = category_summary_df.reset_index()\n",
    "            # You might need a dedicated plotting function for this summary table,\n",
    "            # the previous plot_metric_comparison_bars was designed for the overall summary.\n",
    "            # Example using seaborn directly:\n",
    "            if 'Node Type' in plot_data_cat.columns and primary_sort_metric in plot_data_cat.columns:\n",
    "                 plt.figure(figsize=(10, 6))\n",
    "                 sns.barplot(data=plot_data_cat, x='Main Category', y=primary_sort_metric, hue='Node Type', palette='viridis', edgecolor='grey')\n",
    "                 plt.title(f'Average {primary_sort_metric} by Main Anomaly Category ({target_split.capitalize()} Set)')\n",
    "                 plt.ylabel(f'Average {primary_sort_metric} (Mean across Runs)')\n",
    "                 plt.xlabel('Main Anomaly Category')\n",
    "                 plt.xticks(rotation=0)\n",
    "                 plt.legend(title='Node Type', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "                 plt.grid(True, axis='y', linestyle=':', alpha=0.7)\n",
    "                 plt.tight_layout(rect=[0, 0, 0.88, 0.96])\n",
    "                 if save_output_dir:\n",
    "                     os.makedirs(save_output_dir, exist_ok=True)\n",
    "                     plt.savefig(os.path.join(save_output_dir, f\"plot_category_compare_{primary_sort_metric}_{target_split}.png\"), dpi=300, bbox_inches='tight')\n",
    "                 plt.show()\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Could not generate category comparison plot: {e}\")\n",
    "\n",
    "\n",
    "    # 2. Plot Score Distributions (using raw scores and labels)\n",
    "    # Needs the `plot_score_distributions_split` function defined previously\n",
    "    # Requires `all_scores` and `gt_node_labels`, `gt_edge_labels`\n",
    "    if 'all_scores' in locals() and 'gt_node_labels' in locals() and 'gt_edge_labels' in locals():\n",
    "         try:\n",
    "              print(\"\\n--- Generating Score Distribution Plot ---\")\n",
    "              # Ensure plot_score_distributions_split is defined/imported\n",
    "              plot_score_distributions_split(\n",
    "                   scores_dict=all_scores,\n",
    "                   gt_node_labels_dict=gt_node_labels,\n",
    "                   gt_edge_labels_dict=gt_edge_labels, # Pass both label dicts\n",
    "                   split_name=target_split,\n",
    "                   target_edge_type = ('provider','to','member') # Specify edge type if relevant\n",
    "                   # save_path=os.path.join(save_output_dir, f\"plot_score_dist_{target_split}.png\") # Optional save\n",
    "              )\n",
    "         except NameError:\n",
    "              print(\"`plot_score_distributions_split` function not defined. Skipping plot.\")\n",
    "         except Exception as e:\n",
    "              print(f\"Could not generate score distribution plot: {e}\")\n",
    "\n",
    "    # --- Optional: Save generated tables ---\n",
    "    if save_output_dir:\n",
    "        os.makedirs(save_output_dir, exist_ok=True)\n",
    "        if category_summary_df is not None:\n",
    "            category_summary_df.to_csv(os.path.join(save_output_dir, f\"table_category_summary_{target_split}.csv\"))\n",
    "            print(f\"Category summary saved to {save_output_dir}\")\n",
    "        if top_bottom_tags_df is not None:\n",
    "            top_bottom_tags_df.to_csv(os.path.join(save_output_dir, f\"table_top_bottom_tags_{target_split}.csv\"), index=False)\n",
    "            print(f\"Top/Bottom tags summary saved to {save_output_dir}\")\n",
    "        if scatter_data_df is not None:\n",
    "             scatter_data_df.to_csv(os.path.join(save_output_dir, f\"data_for_scatter_{target_split}.csv\"), index=False)\n",
    "             print(f\"Scatter plot data saved to {save_output_dir}\")\n",
    "\n",
    "else:\n",
    "     print(\"Analysis could not be performed: `final_anomaly_type_df` is missing or empty.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_anomaly_type_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DATA_SPLITS_ITERATIONS.get(0).get(\"anomaly_tracking\").get(\"test\").get(\"node\").get(\"member\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "from typing import Tuple, Optional, List\n",
    "import os\n",
    "from collections import defaultdict\n",
    "\n",
    "import re\n",
    "import pandas as pd # Should be imported at the top of your script\n",
    "\n",
    "# Helper function to format the anomaly tag for plot labels\n",
    "# This function should be defined somewhere accessible by analyze_anomaly_types_iterations,\n",
    "# e.g., at the same level as extract_main_category, or as a nested function if preferred.\n",
    "def _format_tag_for_plot_label(original_tag_str: str, main_category_str: str) -> str:\n",
    "    \"\"\"\n",
    "    Formats an anomaly tag string for better readability in plot labels.\n",
    "    - 'Category/name_of_anomaly' -> 'Name Of Anomaly'\n",
    "    - 'Combined' (if main_category is 'Combined') -> 'Combined'\n",
    "    - Edge tags (if main_category is 'Edge') -> 'Edge'\n",
    "    - Other simple tags -> 'Original Tag Str' (processed for underscores and capitalization)\n",
    "    \"\"\"\n",
    "    if pd.isna(original_tag_str):\n",
    "        return \"Unknown\"\n",
    "\n",
    "    # Handle special main categories first\n",
    "    if main_category_str == 'Combined':\n",
    "        return 'Combined'\n",
    "    if main_category_str == 'Edge':\n",
    "        # `extract_main_category` already classified it as 'Edge'\n",
    "        # The original_tag_str for edges can be complex like \"('TypeA', 'rel', 'TypeB')\"\n",
    "        return 'Edge'\n",
    "\n",
    "    name_to_format = original_tag_str\n",
    "    # Use regex to remove the \"Category/\" prefix if one exists.\n",
    "    # This removes everything up to and including the first '/'\n",
    "    name_to_format = re.sub(r'^[^/]+/', '', name_to_format, count=1)\n",
    "\n",
    "    # Replace underscores with spaces\n",
    "    name_to_format = name_to_format.replace('_', ' ')\n",
    "    \n",
    "    # Capitalize the first letter of each word\n",
    "    formatted_name = ' '.join(word.capitalize() for word in name_to_format.split())\n",
    "\n",
    "    # Return the formatted name, or the original tag if formatting results in an empty string\n",
    "    return formatted_name if formatted_name else original_tag_str\n",
    "\n",
    "# Set a suitable style for report plots\n",
    "sns.set_theme(style=\"whitegrid\", palette=\"viridis\", font_scale=1.1) # Using viridis palette\n",
    "\n",
    "def extract_main_category(tag):\n",
    "    \"\"\"Helper function to extract the main category from the tag.\"\"\"\n",
    "    if pd.isna(tag):\n",
    "        return 'Unknown'\n",
    "    if isinstance(tag, str):\n",
    "        # Handle edge type strings explicitly if they appear in Anomaly Tag column\n",
    "        if tag.startswith(\"('\"):\n",
    "            return 'Edge' # Classify edge types simply as 'Edge' for grouping\n",
    "        if '/' in tag:\n",
    "            category = tag.split('/')[0]\n",
    "            # Standardize capitalization\n",
    "            if category.lower() == 'structural': return 'Structural'\n",
    "            if category.lower() == 'attribute': return 'Attribute'\n",
    "            return category # Keep original if not standard\n",
    "        elif tag.lower() == 'combined':\n",
    "            return 'Combined'\n",
    "        elif tag.lower() == 'unknown':\n",
    "             return 'Unknown'\n",
    "        else:\n",
    "             # Basic keyword check for tags without '/'\n",
    "             if 'attribute' in tag.lower(): return 'Attribute'\n",
    "             if 'structural' in tag.lower(): return 'Structural'\n",
    "             if 'combined' in tag.lower(): return 'Combined'\n",
    "             return 'Other' # Fallback category\n",
    "    return 'Unknown' # Default for non-string types or unparseable strings\n",
    "\n",
    "\n",
    "def analyze_anomaly_types_iterations(\n",
    "    anomaly_type_df_iterations: pd.DataFrame,\n",
    "    split_name: str = 'test',\n",
    "    sort_metric: str = 'AP',\n",
    "    metrics_to_analyze: List[str] = ['AUROC', 'AP', 'Best F1'],\n",
    "    plot_metric_comparison: str = 'AP',\n",
    "    top_k_anomalies: int = 10,\n",
    "    plot_figsize_comparison: Tuple[int, int] = (12, 7),\n",
    "    plot_figsize_top_k: Tuple[int, int] = (14, 8),\n",
    "    save_dir: Optional[str] = None\n",
    "    ) -> Tuple[Optional[pd.DataFrame], Optional[pd.DataFrame]]:\n",
    "    \"\"\"\n",
    "    Analyzes performance breakdown by anomaly type for a specific data split,\n",
    "    considering multiple experimental iterations (seeds).\n",
    "    Generates tables with mean for counts/proportions and mean & std deviation for metrics.\n",
    "    Also generates a bar plot for top-k detected anomaly types, colored by Main Category.\n",
    "\n",
    "    Args:\n",
    "        anomaly_type_df_iterations (pd.DataFrame): DataFrame with per-tag metrics vs normals\n",
    "                                        (expected columns: 'Split', 'Node Type', 'Anomaly Tag',\n",
    "                                        'Count', 'Mean Score', 'Median Score', 'AUROC', 'AP', 'Best F1', 'seed').\n",
    "        split_name (str): The split to analyze ('train', 'val', or 'test').\n",
    "        sort_metric (str): Metric used to sort the detailed tag results (e.g., 'AP').\n",
    "                           The table will be sorted by the mean of this metric.\n",
    "        metrics_to_analyze (List[str]): List of performance metric columns for summaries.\n",
    "        plot_metric_comparison (str): The metric to visualize in the category comparison bar plot (plots its mean).\n",
    "        top_k_anomalies (int): Number of top anomaly types to display in the new bar plot.\n",
    "        plot_figsize_comparison: Figure size for the metric comparison plot.\n",
    "        plot_figsize_top_k: Figure size for the top-k anomaly types plot.\n",
    "        save_dir (Optional[str]): Directory to save the output tables and plots.\n",
    "\n",
    "    Returns:\n",
    "        Tuple[Optional[pd.DataFrame], Optional[pd.DataFrame]]:\n",
    "            - df_category_summary: Aggregated metrics by Main Category and Node Type.\n",
    "            - df_detailed_sorted: Detailed metrics per Anomaly Tag, sorted.\n",
    "            (Returns None, None if analysis fails)\n",
    "    \"\"\"\n",
    "    print(f\"\\n--- Analyzing Anomaly Type Performance (with Iterations) for Split: '{split_name}' ---\")\n",
    "\n",
    "    if anomaly_type_df_iterations is None or anomaly_type_df_iterations.empty:\n",
    "        print(\"Input DataFrame `anomaly_type_df_iterations` is empty. Cannot perform analysis.\")\n",
    "        return None, None\n",
    "\n",
    "    # --- 1. Preprocessing ---\n",
    "    df_split = anomaly_type_df_iterations[anomaly_type_df_iterations['Split'] == split_name].copy()\n",
    "    if df_split.empty:\n",
    "        print(f\"No data found for split '{split_name}' in `anomaly_type_df_iterations`.\")\n",
    "        return None, None\n",
    "\n",
    "    required_cols_base = ['Node Type', 'Anomaly Tag', 'Count', 'seed']\n",
    "    metrics_for_analysis_and_plotting = list(set(metrics_to_analyze + [plot_metric_comparison, 'AP'])) # 'AP' is needed for top-k plot\n",
    "    \n",
    "    missing_cols = [col for col in required_cols_base + metrics_for_analysis_and_plotting if col not in df_split.columns]\n",
    "    if missing_cols:\n",
    "        print(f\"Error: Missing required columns in DataFrame: {missing_cols}\")\n",
    "        return None, None\n",
    "\n",
    "    df_split['Main Category'] = df_split['Anomaly Tag'].apply(extract_main_category)\n",
    "\n",
    "    if 'Node Type' not in df_split.columns or 'seed' not in df_split.columns or 'Count' not in df_split.columns:\n",
    "        print(\"Error: 'Node Type', 'seed', or 'Count' column missing, cannot calculate proportions accurately.\")\n",
    "        return None, None\n",
    "    \n",
    "    df_split['Total Anomalies in Node Type for Seed'] = df_split.groupby(['Node Type', 'seed'])['Count'].transform('sum')\n",
    "    df_split['Proportion (%)'] = (df_split['Count'] / df_split['Total Anomalies in Node Type for Seed'] * 100).fillna(0)\n",
    "\n",
    "    # --- 2. Table 1: Performance Summary by Main Category & Node Type ---\n",
    "    print(\"\\n--- Table 1: Performance Summary by Main Anomaly Category & Node Type ---\")\n",
    "    df_category_summary = None\n",
    "    try:\n",
    "        metric_aggs = {metric: ['mean', 'std'] for metric in metrics_to_analyze}\n",
    "        df_category_metrics_agg = df_split.groupby(['Main Category', 'Node Type'])[metrics_to_analyze].agg(metric_aggs)\n",
    "        df_category_metrics_agg.columns = ['_'.join(col).strip() for col in df_category_metrics_agg.columns.values]\n",
    "\n",
    "        category_counts_per_seed = df_split.groupby(['Main Category', 'Node Type', 'seed'])['Count'].sum().reset_index()\n",
    "        category_total_anomalies_agg = category_counts_per_seed.groupby(['Main Category', 'Node Type']).agg(\n",
    "            Total_Anomalies=('Count', 'mean')\n",
    "        )\n",
    "\n",
    "        category_proportions_per_seed = df_split.groupby(['Main Category', 'Node Type', 'seed'])['Proportion (%)'].sum().reset_index()\n",
    "        category_proportion_agg = category_proportions_per_seed.groupby(['Main Category', 'Node Type']).agg(\n",
    "            Category_Proportion_Pct=('Proportion (%)', 'mean')\n",
    "        )\n",
    "        \n",
    "        df_category_summary = pd.concat([category_total_anomalies_agg, category_proportion_agg, df_category_metrics_agg], axis=1).reset_index()\n",
    "        \n",
    "        metric_cols_ordered = []\n",
    "        for m in metrics_to_analyze:\n",
    "            metric_cols_ordered.extend([f'{m}_mean', f'{m}_std'])\n",
    "        \n",
    "        cols_order = ['Main Category', 'Node Type', 'Total_Anomalies', 'Category_Proportion_Pct'] + metric_cols_ordered\n",
    "        df_category_summary = df_category_summary.reindex(columns=cols_order).round(3)\n",
    "        \n",
    "        for col in df_category_summary.columns:\n",
    "            if col.endswith(\"_std\"):\n",
    "                df_category_summary[col] = df_category_summary[col].fillna(0)\n",
    "        \n",
    "        df_category_summary.rename(columns={'Total_Anomalies': 'Total Anomalies (Avg)', \n",
    "                                            'Category_Proportion_Pct': 'Category Proportion (%) (Avg)'}, inplace=True)\n",
    "\n",
    "        print(df_category_summary.to_string(index=False))\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating category summary table: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "\n",
    "    # --- 3. Table 2: Detailed Performance by Specific Anomaly Tag (Sorted) ---\n",
    "    sort_metric_mean_col_name = f'{sort_metric}_mean'\n",
    "\n",
    "    print(f\"\\n--- Table 2: Detailed Performance by Specific Anomaly Tag (Sorted by {sort_metric_mean_col_name}) ---\")\n",
    "    df_detailed_sorted = None\n",
    "    try:\n",
    "        score_stats = [col for col in ['Mean Score', 'Median Score'] if col in df_split.columns]\n",
    "        \n",
    "        detailed_aggs = {}\n",
    "        for m in metrics_to_analyze + score_stats:\n",
    "            detailed_aggs[f\"{m}_mean\"] = (m, 'mean')\n",
    "            detailed_aggs[f\"{m}_std\"] = (m, 'std')\n",
    "        detailed_aggs['Count'] = ('Count', 'mean')\n",
    "        detailed_aggs['Proportion (%)'] = ('Proportion (%)', 'mean')\n",
    "\n",
    "        df_detailed_agg = df_split.groupby(['Node Type', 'Main Category', 'Anomaly Tag']).agg(**detailed_aggs).reset_index()\n",
    "        \n",
    "        for col in df_detailed_agg.columns:\n",
    "            if col.endswith(\"_std\"):\n",
    "                df_detailed_agg[col] = df_detailed_agg[col].fillna(0)\n",
    "\n",
    "        cols_detailed_ordered = ['Node Type', 'Main Category', 'Anomaly Tag', 'Count', 'Proportion (%)']\n",
    "        for m in metrics_to_analyze + score_stats:\n",
    "            cols_detailed_ordered.extend([f'{m}_mean', f'{m}_std'])\n",
    "        \n",
    "        cols_detailed_ordered = [col for col in cols_detailed_ordered if col in df_detailed_agg.columns]\n",
    "        df_detailed_sorted = df_detailed_agg[cols_detailed_ordered]\n",
    "\n",
    "        if sort_metric_mean_col_name not in df_detailed_sorted.columns:\n",
    "            print(f\"Error: Sort metric '{sort_metric_mean_col_name}' not found in detailed aggregated DataFrame. Sorting by Node Type, Main Category.\")\n",
    "            df_detailed_sorted = df_detailed_sorted.sort_values(by=['Node Type', 'Main Category'], ascending=[True, True])\n",
    "        else:\n",
    "            df_detailed_sorted = df_detailed_sorted.sort_values(\n",
    "                by=['Node Type', 'Main Category', sort_metric_mean_col_name],\n",
    "                ascending=[True, True, False]\n",
    "            )\n",
    "        \n",
    "        df_detailed_sorted = df_detailed_sorted.round(3)\n",
    "        print(df_detailed_sorted.to_string(index=False, max_rows=50))\n",
    "    except KeyError as e:\n",
    "         print(f\"KeyError during detailed table creation or sorting (possibly related to '{sort_metric_mean_col_name}'): {e}\")\n",
    "    except Exception as e:\n",
    "        print(f\"Error creating detailed sorted table: {e}\")\n",
    "        import traceback\n",
    "        traceback.print_exc()\n",
    "\n",
    "    # --- 4. Plot 1: Metric Comparison by Main Category (with Error Bars for Std Dev) ---\n",
    "    print(f\"\\n--- Plot 1: Comparison of Average {plot_metric_comparison} by Main Category (with Std Dev over Iterations) ---\")\n",
    "    if df_category_summary is not None and f'{plot_metric_comparison}_mean' in df_category_summary.columns:\n",
    "        try:\n",
    "            plt.figure(figsize=plot_figsize_comparison)\n",
    "            ax = sns.barplot(\n",
    "                data=df_split, \n",
    "                x='Main Category',\n",
    "                y=plot_metric_comparison,\n",
    "                hue='Node Type',\n",
    "                palette='viridis',\n",
    "                edgecolor='grey',\n",
    "                linewidth=0.75,\n",
    "                estimator=np.mean, \n",
    "                errorbar='sd'\n",
    "            )\n",
    "            plt.title(f'{plot_metric_comparison} by Main Anomaly Category ({split_name.capitalize()} Set)')\n",
    "            plt.xlabel('Main Anomaly Category')\n",
    "            plt.ylabel(f' Average Precision ({plot_metric_comparison})')\n",
    "            plt.xticks(rotation=0)\n",
    "            plt.legend(title='Node Type', bbox_to_anchor=(1.02, 1), loc='upper left')\n",
    "            ax.grid(True, axis='y', linestyle=':', alpha=0.7)\n",
    "            \n",
    "            for container in ax.containers:\n",
    "                 ax.bar_label(container, fmt='%.2f', label_type='edge', padding=2, fontsize=9)\n",
    "\n",
    "            plt.ylim(bottom=0)\n",
    "            plt.tight_layout(rect=[0, 0, 0.88, 0.97])\n",
    "\n",
    "            if save_dir:\n",
    "                if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
    "                plot_path_comp = os.path.join(save_dir, f'plot_category_comparison_iters_{plot_metric_comparison}_{split_name}.png')\n",
    "                try:\n",
    "                    plt.savefig(plot_path_comp, dpi=300, bbox_inches='tight')\n",
    "                    print(f\"Comparison plot saved to {plot_path_comp}\")\n",
    "                except Exception as e: print(f\"Error saving comparison plot: {e}\")\n",
    "            plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating comparison plot: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    else:\n",
    "        metric_mean_col = f'{plot_metric_comparison}_mean'\n",
    "        print(f\"Cannot generate comparison plot: Summary data missing or plot metric '{metric_mean_col}' not found.\")\n",
    "\n",
    "    # --- 5. Plot 2: Top-k Best Detected Anomaly Types (by AP_mean, colored by Main Category) ---\n",
    "    print(f\"\\n--- Plot 2: Top {top_k_anomalies} Best Detected Anomaly Tags (by AP mean, colored by Main Category) ---\")\n",
    "    if df_detailed_sorted is not None and 'AP_mean' in df_detailed_sorted.columns and top_k_anomalies > 0:\n",
    "        try:\n",
    "            top_k_df_for_plot_info = df_detailed_sorted.sort_values(by='AP_mean', ascending=False).head(top_k_anomalies)\n",
    "            \n",
    "            if top_k_df_for_plot_info.empty:\n",
    "                print(f\"No data available for top {top_k_anomalies} plot.\")\n",
    "            else:\n",
    "                top_k_tags_identifiers_tuples = list(zip(top_k_df_for_plot_info['Node Type'], top_k_df_for_plot_info['Anomaly Tag']))\n",
    "                \n",
    "                plot_data_top_k = df_split[\n",
    "                    df_split.set_index(['Node Type', 'Anomaly Tag']).index.isin(top_k_tags_identifiers_tuples)\n",
    "                ].copy()\n",
    "\n",
    "                #plot_data_top_k['Tag Identifier'] = plot_data_top_k['Node Type'] + ' | ' + plot_data_top_k['Anomaly Tag']\n",
    "                #ordered_tag_identifiers_for_plot = (top_k_df_for_plot_info['Node Type'] + ' | ' + top_k_df_for_plot_info['Anomaly Tag']).tolist()\n",
    "\n",
    "                # 1. Modification for plot_data_top_k:\n",
    "                # Apply the formatting to the 'Anomaly Tag' using its 'Main Category'\n",
    "                plot_data_top_k['Formatted Anomaly Name'] = plot_data_top_k.apply(\n",
    "                    lambda row: _format_tag_for_plot_label(row['Anomaly Tag'], row['Main Category']), axis=1\n",
    "                )\n",
    "                plot_data_top_k['Tag Identifier'] = plot_data_top_k['Node Type'] + ' | ' + plot_data_top_k['Formatted Anomaly Name']\n",
    "\n",
    "                # 2. Modification for ordered_tag_identifiers_for_plot:\n",
    "                # Apply the formatting similarly to get the ordered list of new identifiers\n",
    "                top_k_df_for_plot_info['Formatted Anomaly Name'] = top_k_df_for_plot_info.apply(\n",
    "                    lambda row: _format_tag_for_plot_label(row['Anomaly Tag'], row['Main Category']), axis=1\n",
    "                )\n",
    "                ordered_tag_identifiers_for_plot = (top_k_df_for_plot_info['Node Type'] + ' | ' + top_k_df_for_plot_info['Formatted Anomaly Name']).tolist()\n",
    "\n",
    "                # Define a color palette for Main Categories.\n",
    "                # If you have more than a few categories, you might want a more dynamic palette.\n",
    "                # Using seaborn's default categorical palette if not specified, or you can set one:\n",
    "                # E.g., unique_main_categories = plot_data_top_k['Main Category'].unique()\n",
    "                # category_palette = sns.color_palette(\"husl\", n_colors=len(unique_main_categories))\n",
    "                # Or provide a dictionary mapping: main_category_colors = {'Structural': 'blue', 'Attribute': 'green', 'Combined': 'red', 'Other': 'purple', 'Unknown': 'grey', 'Edge': 'orange'}\n",
    "\n",
    "                plt.figure(figsize=plot_figsize_top_k)\n",
    "                ax_top_k = sns.barplot(\n",
    "                    data=plot_data_top_k,\n",
    "                    x='Tag Identifier',\n",
    "                    y='AP', \n",
    "                    hue='Main Category', # Color bars by Main Category\n",
    "                    order=ordered_tag_identifiers_for_plot,\n",
    "                    # palette=main_category_colors, # Optional: use a predefined palette or color map\n",
    "                    palette='Set2', # Using a distinct palette\n",
    "                    edgecolor='grey',\n",
    "                    linewidth=0.75,\n",
    "                    estimator=np.mean,\n",
    "                    errorbar='sd'\n",
    "                )\n",
    "                plt.title(f'Top {top_k_anomalies} Best Detected Anomaly Tags by AP ({split_name.capitalize()} Set)')\n",
    "                plt.xlabel('Anomaly Tag (Node Type | Tag)')\n",
    "                plt.ylabel('Average Precision (AP)')\n",
    "                plt.xticks(rotation=45, ha='right')\n",
    "                plt.legend(title='Main Category', bbox_to_anchor=(1.02, 1), loc='upper left') # Add legend for Main Category\n",
    "                ax_top_k.grid(True, axis='y', linestyle=':', alpha=0.7)\n",
    "\n",
    "                for container in ax_top_k.containers:\n",
    "                    ax_top_k.bar_label(container, fmt='%.2f', label_type='edge', padding=2, fontsize=9)\n",
    "                \n",
    "                plt.ylim(bottom=0)\n",
    "                plt.tight_layout(rect=[0, 0, 0.85, 0.96]) # Adjust layout for legend\n",
    "\n",
    "                if save_dir:\n",
    "                    plot_path_top_k = os.path.join(save_dir, f'plot_top_k_anomalies_AP_colored_{split_name}.png')\n",
    "                    try:\n",
    "                        plt.savefig(plot_path_top_k, dpi=300, bbox_inches='tight')\n",
    "                        print(f\"Top-k anomalies plot saved to {plot_path_top_k}\")\n",
    "                    except Exception as e: print(f\"Error saving top-k anomalies plot: {e}\")\n",
    "                plt.show()\n",
    "        except Exception as e:\n",
    "            print(f\"Error generating top-k anomalies plot: {e}\")\n",
    "            import traceback\n",
    "            traceback.print_exc()\n",
    "    elif top_k_anomalies <= 0:\n",
    "        print(\"Skipping top-k anomalies plot as top_k_anomalies is not positive.\")\n",
    "    else:\n",
    "        print(\"Cannot generate top-k anomalies plot: Detailed summary data or 'AP_mean' column missing.\")\n",
    "\n",
    "\n",
    "    # --- Save DataFrames ---\n",
    "    if save_dir:\n",
    "        if not os.path.exists(save_dir): os.makedirs(save_dir)\n",
    "        if df_category_summary is not None:\n",
    "            cat_path = os.path.join(save_dir, f'summary_category_iters_{split_name}.csv')\n",
    "            try:\n",
    "                df_category_summary.to_csv(cat_path, index=False)\n",
    "                print(f\"Category summary table saved to {cat_path}\")\n",
    "            except Exception as e: print(f\"Error saving category summary: {e}\")\n",
    "        if df_detailed_sorted is not None:\n",
    "             det_path = os.path.join(save_dir, f'summary_detailed_tags_iters_{split_name}.csv')\n",
    "             try:\n",
    "                 df_detailed_sorted.to_csv(det_path, index=False)\n",
    "                 print(f\"Detailed summary table saved to {det_path}\")\n",
    "             except Exception as e: print(f\"Error saving detailed summary: {e}\")\n",
    "\n",
    "    return df_category_summary, df_detailed_sorted\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "example_save_dir = 'analysis_results_iterations_v2' # New save dir for this version\n",
    "\n",
    "category_summary_iters, detailed_summary_iters = analyze_anomaly_types_iterations(\n",
    "    anomaly_type_df_iterations=full_anomaly_type_results_df,\n",
    "    split_name='test',\n",
    "    sort_metric='AP', \n",
    "    metrics_to_analyze=['AUROC', 'AP', 'Best F1', 'Mean Score'], # Added Mean Score to metrics\n",
    "    plot_metric_comparison='AP',\n",
    "    top_k_anomalies=7, # Show top 7 anomalies\n",
    "    save_dir=example_save_dir\n",
    ")\n",
    "\n",
    "print(\"\\n--- Returned Category Summary (Mean for Counts/Props, Mean & Std for Metrics) ---\")\n",
    "if category_summary_iters is not None:\n",
    "    print(category_summary_iters.to_string())\n",
    "\n",
    "print(\"\\n--- Returned Detailed Summary (Mean for Counts/Props, Mean & Std for Metrics) ---\")\n",
    "if detailed_summary_iters is not None:\n",
    "    print(detailed_summary_iters.to_string(index=False, max_rows=15))\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "all_anomaly_type_dfs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "full_anomaly_type_results_df"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "\n",
    "\n",
    "def aggregate_and_compare_model_results(\n",
    "    bgae_results_list: List[pd.DataFrame],\n",
    "    if_results_list: List[pd.DataFrame],\n",
    "    oddball_results_list: List[pd.DataFrame],\n",
    "    dominant_results_list: List[pd.DataFrame],\n",
    "    metrics_to_compare: List[str] = ['AUROC', 'AP', 'Best F1'],\n",
    "    k_list_eval: List[int] = [50, 100, 200, 500], # K values used during evaluation\n",
    "    splits_to_compare: List[str] = ['test'] # Focus on test set by default\n",
    "    ) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Aggregates results from multiple runs for different models and creates\n",
    "    a comparison DataFrame showing mean ± std dev for specified metrics.\n",
    "\n",
    "    Args:\n",
    "        bgae_results_list: List of summary DataFrames from BGAE runs.\n",
    "        if_results_list: List of summary DataFrames from Isolation Forest runs.\n",
    "        oddball_results_list: List of summary DataFrames from OddBall runs.\n",
    "        dominant_results_list: List of summary DataFrames from DOMINANT runs.\n",
    "        metrics_to_compare: List of primary performance metrics to show in the table.\n",
    "        k_list_eval: List of K values used, to include relevant P@K/R@K metrics.\n",
    "        splits_to_compare: List of data splits to include in the comparison (e.g., ['test'], ['val', 'test']).\n",
    "\n",
    "    Returns:\n",
    "        Optional[pd.DataFrame]: A DataFrame comparing models, indexed by Split and Element,\n",
    "                                 with columns for each model showing 'mean ± std' metrics,\n",
    "                                 or None if aggregation fails.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Aggregating and Comparing Model Performances ---\")\n",
    "\n",
    "    all_results_list = []\n",
    "    model_data_map = {\n",
    "        \"BGAE\": bgae_results_list,\n",
    "        \"IsolationForest\": if_results_list,\n",
    "        \"OddBall\": oddball_results_list,\n",
    "        \"DOMINANT\": dominant_results_list\n",
    "    }\n",
    "\n",
    "    # --- 1. Concatenate and Preprocess Results for Each Model ---\n",
    "    processed_model_dfs = {}\n",
    "    metrics_to_check = ['AUROC', 'AP', 'Best F1'] # Ensure these core metrics exist\n",
    "\n",
    "    for model_name, results_list in model_data_map.items():\n",
    "        if not results_list:\n",
    "            print(f\"Warning: No results found for model '{model_name}'. Skipping.\")\n",
    "            continue\n",
    "        try:\n",
    "            full_df = pd.concat(results_list, ignore_index=True)\n",
    "            # Filter for relevant splits\n",
    "            filtered_df = full_df[full_df['Split'].isin(splits_to_compare)].copy()\n",
    "            # Drop rows where essential metrics are NaN (likely failed runs)\n",
    "            cols_exist = all(m in filtered_df.columns for m in metrics_to_check)\n",
    "            if cols_exist:\n",
    "                 successful_df = filtered_df.dropna(subset=metrics_to_check, how='any')\n",
    "            else:\n",
    "                 print(f\"Warning: Core metrics missing for {model_name}. Proceeding with available data.\")\n",
    "                 successful_df = filtered_df\n",
    "\n",
    "            if successful_df.empty:\n",
    "                 print(f\"Warning: No successful runs found for model '{model_name}' in specified splits. Skipping.\")\n",
    "                 continue\n",
    "\n",
    "            processed_model_dfs[model_name] = successful_df\n",
    "            print(f\"Processed {model_name}: Found {successful_df['seed'].nunique()} successful runs in specified splits.\")\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing results for model '{model_name}': {e}\")\n",
    "\n",
    "    if not processed_model_dfs:\n",
    "        print(\"Error: No processed results available for any model. Cannot create comparison.\")\n",
    "        return None\n",
    "\n",
    "    # --- 2. Aggregate Metrics for Each Model ---\n",
    "    aggregated_data_list = []\n",
    "    full_metric_list = metrics_to_compare + \\\n",
    "                       [f'{p}@{k}' for k in k_list_eval for p in ['Precision', 'Recall']]\n",
    "\n",
    "    for model_name, df in processed_model_dfs.items():\n",
    "        # Ensure metrics to aggregate actually exist in this df\n",
    "        metrics_present = [m for m in full_metric_list if m in df.columns]\n",
    "        if not metrics_present:\n",
    "            print(f\"Warning: No specified metrics found for model '{model_name}'. Skipping aggregation.\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            # Group by Split and Element, calculate mean and std\n",
    "            agg_df = df.groupby(['Split', 'Element'])[metrics_present].agg(['mean', 'std'])\n",
    "            # Add run count\n",
    "            run_counts = df.groupby(['Split', 'Element'])['seed'].nunique().rename('n_runs')\n",
    "            agg_df = pd.concat([run_counts, agg_df], axis=1)\n",
    "            agg_df['Model'] = model_name # Add model identifier\n",
    "            aggregated_data_list.append(agg_df.reset_index()) # Reset index for easier merging later\n",
    "        except Exception as e:\n",
    "            print(f\"Error aggregating data for model '{model_name}': {e}\")\n",
    "\n",
    "    if not aggregated_data_list:\n",
    "        print(\"Error: Aggregation failed for all models.\")\n",
    "        return None\n",
    "\n",
    "    # --- 3. Combine and Format the Comparison Table ---\n",
    "    comparison_df = pd.concat(aggregated_data_list, ignore_index=True)\n",
    "\n",
    "    # Pivot table for comparison view\n",
    "    try:\n",
    "        pivot_df = comparison_df.pivot_table(\n",
    "            index=['Split', 'Element'],\n",
    "            columns='Model'\n",
    "            # Values will be selected below\n",
    "        )\n",
    "    except Exception as e:\n",
    "         print(f\"Error pivoting comparison table: {e}\")\n",
    "         return None # Cannot proceed if pivot fails\n",
    "\n",
    "\n",
    "    # Format columns as 'Metric (Mean ± Std)'\n",
    "    final_comparison_cols = {} # Store formatted columns keyed by original metric name\n",
    "    n_runs_cols = {} # Store n_runs separately\n",
    "\n",
    "    # Ensure metrics_present considers columns available across *all* processed models if needed,\n",
    "    # or handle missing columns per model during formatting. Let's handle per model.\n",
    "    all_metrics_in_pivot = set(lvl0 for lvl0, lvl1 in pivot_df.columns if lvl1=='mean') & set(metrics_to_compare + [f'{p}@{k}' for k in k_list_eval for p in ['Precision', 'Recall']])\n",
    "\n",
    "\n",
    "    for metric in all_metrics_in_pivot:\n",
    "        metric_mean_cols = [col for col in pivot_df.columns if col[0] == metric and col[1] == 'mean']\n",
    "        metric_std_cols = [col for col in pivot_df.columns if col[0] == metric and col[1] == 'std']\n",
    "\n",
    "        # Create the formatted column for this metric\n",
    "        formatted_series_list = []\n",
    "        models_in_pivot = pivot_df.columns.get_level_values('Model').unique() # Models actually present\n",
    "\n",
    "        for model_name in models_in_pivot:\n",
    "             mean_col = (metric, 'mean', model_name)\n",
    "             std_col = (metric, 'std', model_name)\n",
    "\n",
    "             if mean_col in pivot_df.columns and std_col in pivot_df.columns:\n",
    "                 # Format only if both mean and std are present and not NaN\n",
    "                 formatted = pivot_df[mean_col].map('{:.4f}'.format).str.cat(\n",
    "                               pivot_df[std_col].map('{:.4f}'.format), sep=' ± '\n",
    "                           ).where(pivot_df[mean_col].notna() & pivot_df[std_col].notna(), \"N/A\") # Handle NaN after agg\n",
    "             elif mean_col in pivot_df.columns:\n",
    "                  # Format mean only if std is missing/NaN\n",
    "                  formatted = pivot_df[mean_col].map('{:.4f}'.format).where(pivot_df[mean_col].notna(), \"N/A\") + ' ± nan'\n",
    "             else:\n",
    "                  formatted = pd.Series(\"N/A\", index=pivot_df.index) # Metric not found for this model\n",
    "\n",
    "             formatted.name = (metric, model_name) # Assign multi-level name\n",
    "             formatted_series_list.append(formatted)\n",
    "\n",
    "        # Combine formatted series for this metric across all models\n",
    "        if formatted_series_list:\n",
    "             final_comparison_cols[metric] = pd.concat(formatted_series_list, axis=1)\n",
    "\n",
    "\n",
    "    # Get n_runs separately\n",
    "    n_runs_cols_present = [col for col in pivot_df.columns if col[0] == 'n_runs' and col[1] == ''] # n_runs has empty second level\n",
    "    if n_runs_cols_present:\n",
    "        n_runs_df = pivot_df[n_runs_cols_present]\n",
    "        n_runs_df.columns = n_runs_df.columns.droplevel(1) # Drop the empty level ''\n",
    "        n_runs_cols = {'n_runs': n_runs_df}\n",
    "\n",
    "\n",
    "    # Assemble the final table\n",
    "    if not final_comparison_cols:\n",
    "        print(\"Error: No metric columns could be formatted.\")\n",
    "        return None\n",
    "\n",
    "    # Create MultiIndex columns for the final DataFrame\n",
    "    metric_order = metrics_to_compare + sorted([m for m in all_metrics_in_pivot if m not in metrics_to_compare]) # Order metrics\n",
    "    models_order = sorted(processed_model_dfs.keys()) # Sort model names alphabetically\n",
    "\n",
    "    final_df_cols = pd.MultiIndex.from_product([metric_order, models_order], names=['Metric', 'Model'])\n",
    "    final_comparison_df = pd.DataFrame(index=pivot_df.index, columns=final_df_cols)\n",
    "\n",
    "    # Fill the DataFrame with formatted values\n",
    "    for metric in metric_order:\n",
    "         if metric in final_comparison_cols:\n",
    "             metric_data = final_comparison_cols[metric]\n",
    "             # Ensure columns align - reindex metric_data if necessary\n",
    "             final_comparison_df[metric] = metric_data.reindex(columns=final_df_cols.get_level_values('Model').unique(), level='Model')\n",
    "\n",
    "    # Add n_runs as the first level\n",
    "    if n_runs_cols:\n",
    "        final_comparison_df = pd.concat(n_runs_cols, axis=1, keys=['Info']).join(final_comparison_df)\n",
    "        # Rename 'n_runs' column under 'Info'\n",
    "        final_comparison_df.rename(columns={'n_runs': 'Runs'}, level=1, inplace=True)\n",
    "\n",
    "\n",
    "    print(\"\\n--- Final Model Comparison (Mean ± Std Dev) ---\")\n",
    "    print(final_comparison_df.to_string())\n",
    "\n",
    "    return final_comparison_df\n",
    "\n",
    "\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from typing import Dict, Tuple, Optional, List\n",
    "\n",
    "# Assume compute_evaluation_metrics is defined elsewhere or imported\n",
    "\n",
    "def aggregate_and_compare_model_results(\n",
    "    bgae_results_list: List[pd.DataFrame],\n",
    "    if_results_list: List[pd.DataFrame],\n",
    "    oddball_results_list: List[pd.DataFrame],\n",
    "    dominant_results_list: List[pd.DataFrame],\n",
    "    metrics_to_compare: List[str] = ['AUROC', 'AP', 'Best F1'],\n",
    "    k_list_eval: List[int] = [50, 100, 200, 500], # K values used during evaluation\n",
    "    splits_to_compare: List[str] = ['test'], # Focus on test set by default\n",
    "    sort_elements_by: Optional[List[str]] = None # Optional order for elements in index\n",
    "    ) -> Optional[pd.DataFrame]:\n",
    "    \"\"\"\n",
    "    Aggregates results from multiple runs for different models and creates\n",
    "    a comparison DataFrame showing mean ± std dev for specified metrics.\n",
    "\n",
    "    Args:\n",
    "        bgae_results_list: List of summary DataFrames from BGAE runs.\n",
    "        if_results_list: List of summary DataFrames from Isolation Forest runs.\n",
    "        oddball_results_list: List of summary DataFrames from OddBall runs.\n",
    "        dominant_results_list: List of summary DataFrames from DOMINANT runs.\n",
    "        metrics_to_compare: List of primary performance metrics to show in the table.\n",
    "        k_list_eval: List of K values used, to include relevant P@K/R@K metrics.\n",
    "        splits_to_compare: List of data splits to include in the comparison (e.g., ['test'], ['val', 'test']).\n",
    "        sort_elements_by (Optional[List[str]]): Specific order for rows based on 'Element' column.\n",
    "\n",
    "    Returns:\n",
    "        Optional[pd.DataFrame]: A DataFrame comparing models, indexed by Split and Element,\n",
    "                                 with columns for each model showing 'mean ± std' metrics,\n",
    "                                 or None if aggregation fails.\n",
    "    \"\"\"\n",
    "    print(\"\\n--- Aggregating and Comparing Model Performances ---\")\n",
    "\n",
    "    model_data_map = {\n",
    "        \"BGAE\": bgae_results_list,\n",
    "        \"IsolationForest\": if_results_list,\n",
    "        \"OddBall\": oddball_results_list,\n",
    "        \"DOMINANT\": dominant_results_list\n",
    "    }\n",
    "\n",
    "    # --- 1. Combine all results into a single DataFrame ---\n",
    "    all_results_list = []\n",
    "    for model_name, results_list in model_data_map.items():\n",
    "        if results_list:\n",
    "            try:\n",
    "                model_df = pd.concat(results_list, ignore_index=True)\n",
    "                # Ensure essential columns exist, add 'Model' column\n",
    "                if not model_df.empty:\n",
    "                    model_df['Model'] = model_name\n",
    "                    all_results_list.append(model_df)\n",
    "            except Exception as e:\n",
    "                print(f\"Error concatenating results for {model_name}: {e}\")\n",
    "\n",
    "    if not all_results_list:\n",
    "        print(\"Error: No valid result lists provided or concatenation failed.\")\n",
    "        return None\n",
    "\n",
    "    combined_df = pd.concat(all_results_list, ignore_index=True)\n",
    "\n",
    "    # --- 2. Filter and Clean ---\n",
    "    filtered_df = combined_df[combined_df['Split'].isin(splits_to_compare)].copy()\n",
    "\n",
    "    metrics_to_check = ['AUROC', 'AP', 'Best F1'] # Core metrics for filtering failed runs\n",
    "    cols_exist = all(m in filtered_df.columns for m in metrics_to_check)\n",
    "    if cols_exist:\n",
    "        successful_df = filtered_df.dropna(subset=metrics_to_check, how='any')\n",
    "    else:\n",
    "        print(\"Warning: Core metric columns missing. Filtering might be incomplete.\")\n",
    "        successful_df = filtered_df\n",
    "\n",
    "    if successful_df.empty:\n",
    "        print(f\"Warning: No successful runs found for models in specified splits: {splits_to_compare}.\")\n",
    "        return pd.DataFrame() # Return empty DataFrame\n",
    "\n",
    "    print(f\"Aggregating results from {successful_df['seed'].nunique()} successful runs across models...\")\n",
    "\n",
    "    # --- 3. Calculate Aggregates (Mean, Std, Count) ---\n",
    "    # Define all metrics we might want to aggregate\n",
    "    all_metrics_possible = metrics_to_compare + \\\n",
    "                           [f'{p}@{k}' for k in k_list_eval for p in ['Precision', 'Recall']]\n",
    "    # Filter list to only those metrics actually present in the successful data\n",
    "    metrics_present = [m for m in all_metrics_possible if m in successful_df.columns]\n",
    "\n",
    "    if not metrics_present:\n",
    "         print(\"Error: None of the specified metrics_to_compare or P@K/R@K metrics found in the data.\")\n",
    "         return None\n",
    "\n",
    "    try:\n",
    "        # Group by Split, Element, and Model then aggregate\n",
    "        grouped = successful_df.groupby(['Split', 'Element', 'Model'])\n",
    "        aggregated_means = grouped[metrics_present].mean()\n",
    "        aggregated_stds = grouped[metrics_present].std()\n",
    "        aggregated_counts = grouped['seed'].nunique().rename('n_runs') # Count distinct runs per group\n",
    "\n",
    "        # Combine mean, std, and counts\n",
    "        aggregated_df = aggregated_means.join(aggregated_stds, lsuffix='_mean', rsuffix='_std')\n",
    "        aggregated_df = aggregated_df.join(aggregated_counts)\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error during aggregation: {e}\")\n",
    "        return None\n",
    "\n",
    "    # --- 4. Format the Comparison Table ---\n",
    "    final_df_list = []\n",
    "    # Use index from aggregated_df to handle multi-level index easily\n",
    "    for idx, row in aggregated_df.iterrows():\n",
    "        split, element, model = idx # Unpack the index levels\n",
    "        formatted_row = {'Split': split, 'Element': element, 'Model': model, 'Runs': int(row['n_runs'])}\n",
    "        for metric in metrics_present:\n",
    "            mean_val = row.get(f'{metric}_mean', np.nan)\n",
    "            std_val = row.get(f'{metric}_std', np.nan)\n",
    "\n",
    "            if pd.notna(mean_val) and pd.notna(std_val):\n",
    "                formatted_row[metric] = f\"{mean_val:.4f} ± {std_val:.4f}\"\n",
    "            elif pd.notna(mean_val):\n",
    "                formatted_row[metric] = f\"{mean_val:.4f} ± nan\"\n",
    "            else:\n",
    "                formatted_row[metric] = \"N/A\" # Or np.nan if preferred\n",
    "        final_df_list.append(formatted_row)\n",
    "\n",
    "    if not final_df_list:\n",
    "        print(\"Error: No data after formatting.\")\n",
    "        return pd.DataFrame()\n",
    "\n",
    "    final_comparison_df_long = pd.DataFrame(final_df_list)\n",
    "\n",
    "    # Pivot to get models as columns\n",
    "    try:\n",
    "        # Define the order of metrics for columns\n",
    "        column_order = ['Runs'] + metrics_present\n",
    "        pivot_comparison_df = final_comparison_df_long.pivot_table(\n",
    "            index=['Split', 'Element'],\n",
    "            columns='Model',\n",
    "            values=column_order # Specify columns to pivot as values\n",
    "        )\n",
    "        # Reorder model columns alphabetically for consistency\n",
    "        pivot_comparison_df = pivot_comparison_df.reindex(columns=sorted(pivot_comparison_df.columns.levels[1]), level='Model')\n",
    "        # Reorder metric level for readability\n",
    "        pivot_comparison_df = pivot_comparison_df.reindex(columns=column_order, level=0)\n",
    "\n",
    "\n",
    "    except Exception as e:\n",
    "        print(f\"Error pivoting final table: {e}\")\n",
    "        print(\"Returning table in long format instead.\")\n",
    "        # Set index for long format return\n",
    "        final_comparison_df_long = final_comparison_df_long.set_index(['Split', 'Element', 'Model'])\n",
    "        return final_comparison_df_long # Return long format as fallback\n",
    "\n",
    "\n",
    "    # --- Optional: Sort index by custom element order ---\n",
    "    if sort_elements_by and isinstance(pivot_comparison_df.index, pd.MultiIndex):\n",
    "         try:\n",
    "             # Sort by Split first, then by custom Element order\n",
    "             pivot_comparison_df = pivot_comparison_df.sort_index(\n",
    "                 level='Element',\n",
    "                 key=lambda index: index.map({elem: i for i, elem in enumerate(sort_elements_by)}),\n",
    "                 sort_remaining=True # Sorts by Split automatically after sorting by Element key\n",
    "             )\n",
    "         except Exception as e:\n",
    "              print(f\"Warning: Could not sort by custom element order: {e}\")\n",
    "\n",
    "\n",
    "    print(\"\\n--- Final Model Comparison (Mean ± Std Dev) ---\")\n",
    "    # Configure pandas display options for better table printing\n",
    "    pd.set_option('display.max_rows', 500)\n",
    "    pd.set_option('display.max_columns', len(pivot_comparison_df.columns) + 1) # Adjust based on columns\n",
    "    pd.set_option('display.width', 200) # Adjust width as needed\n",
    "    pd.set_option('display.precision', 4) # Set default float precision\n",
    "\n",
    "    print(pivot_comparison_df.to_string())\n",
    "\n",
    "    # Reset display options if desired\n",
    "    # pd.reset_option('all')\n",
    "\n",
    "    return pivot_comparison_df\n",
    "\n",
    "# --- Example Usage ---\n",
    "# Assume all_summary_dfs, all_if_summaries, all_oddball_summaries, all_dominant_summaries\n",
    "# are lists of DataFrames populated from the previous iteration loops.\n",
    "\n",
    "# Example element order (adjust based on your actual 'Element' column values)\n",
    "# element_order = [\n",
    "#     'Node (provider)',\n",
    "#     'Node (member)',\n",
    "#     \"Edge ('provider', 'to', 'member')\" # Example edge element name\n",
    "# ]\n",
    "\n",
    "# comparison_table = aggregate_and_compare_model_results(\n",
    "#     bgae_results_list = all_summary_dfs,\n",
    "#     if_results_list = all_if_summaries,\n",
    "#     oddball_results_list = all_oddball_summaries,\n",
    "#     dominant_results_list = all_dominant_summaries,\n",
    "#     metrics_to_compare = ['AUROC', 'AP', 'Best F1'], # Primary metrics\n",
    "#     k_list_eval = [50, 100, 200, 500], # K values used\n",
    "#     splits_to_compare = ['test'], # Focus on test set\n",
    "#     sort_elements_by = element_order # Optional sorting\n",
    "# )\n",
    "\n",
    "# if comparison_table is not None:\n",
    "#      print(\"\\nComparison Table Created:\")\n",
    "#      # print(comparison_table.to_markdown()) # For easy viewing if needed\n",
    "\n",
    "#      # Optional: Save to CSV\n",
    "#      # save_dir = \"final_evaluation_results_iter\"\n",
    "#      # if save_dir:\n",
    "#      #     os.makedirs(save_dir, exist_ok=True)\n",
    "#      #     comparison_table.to_csv(os.path.join(save_dir, \"model_comparison_summary.csv\"))\n",
    "\n",
    "comparison_table = aggregate_and_compare_model_results(\n",
    "     bgae_results_list = all_summary_dfs,\n",
    "     if_results_list = all_if_summaries,\n",
    "     oddball_results_list = all_oddball_summaries,\n",
    "     dominant_results_list = all_dominant_summaries,\n",
    "     metrics_to_compare = ['AUROC', 'AP', 'Best F1'], # Choose primary metrics\n",
    "     k_list_eval = [50, 100, 200, 500], # K values used\n",
    "     splits_to_compare = ['test'] # Focus on test set\n",
    " )\n",
    "\n",
    "# if comparison_table is not None:\n",
    "#      print(\"\\nComparison Table Created:\")\n",
    "#      print(comparison_table.to_markdown()) # Print markdown for easy viewing\n",
    "\n",
    "#      # Optional: Save to CSV\n",
    "#      # save_dir = \"final_evaluation_results_iter\"\n",
    "#      # if save_dir:\n",
    "#      #     os.makedirs(save_dir, exist_ok=True)\n",
    "#      #     comparison_table.to_csv(os.path.join(save_dir, \"model_comparison_summary.csv\"))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "myenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
